{
  "github_url": "https://github.paypal.com/GADS-Consumer-ML/ql-store-recommendation-prod.git",
  "input_files": [
    "research/pipeline/00_driver.ipynb",
    "research/pipeline/01_bq_feat.ipynb",
    "research/pipeline/01_varmart_feat.ipynb",
    "research/pipeline/02_combine.ipynb",
    "research/pipeline/03_prepare_training_data.ipynb",
    "research/pipeline/04_prepare_oot_data.ipynb",
    "research/pipeline/04_training.ipynb",
    "research/pipeline/05_scoring_oot.ipynb",
    "research/pipeline/06_evaluation.ipynb"
  ],
  "repo_name": "ql-store-recommendation-prod",
  "local_repo_path": "repos/ql-store-recommendation-prod",
  "files": [
    "repos/ql-store-recommendation-prod/research/pipeline/00_driver.py",
    "repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.py",
    "repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.py",
    "repos/ql-store-recommendation-prod/research/pipeline/02_combine.py",
    "repos/ql-store-recommendation-prod/research/pipeline/03_prepare_training_data.py",
    "repos/ql-store-recommendation-prod/research/pipeline/04_prepare_oot_data.py",
    "repos/ql-store-recommendation-prod/research/pipeline/04_training.py",
    "repos/ql-store-recommendation-prod/research/pipeline/05_scoring_oot.py",
    "repos/ql-store-recommendation-prod/research/pipeline/06_evaluation.py"
  ],
  "summaries": [
    {
      "summary": "**Load YAML configuration file** (Lines 4-13):\n- Defines a function to load a YAML file.\n- Loads the configuration from a specified file path.\n\n**Set BigQuery prefix** (Lines 14-16):\n- Extracts the BigQuery project dataset prefix from the configuration.\n\n**Create live unique merchants training table** (Lines 18-52):\n- Drops the existing table if it exists.\n- Creates a new table with unique merchants, joining multiple tables to gather necessary merchant information.\n\n**Create driver_00 table** (Lines 54-81):\n- Drops the existing table if it exists.\n- Creates a new table with customer and merchant interaction data, filtering based on specific conditions.\n\n**Create driver_0 table** (Lines 84-102):\n- Drops the existing table if it exists.\n- Creates a new table by filtering `driver_00` to include customers with multiple placements or specific transaction conditions.\n\n**Create driver_1 table** (Lines 105-120):\n- Drops the existing table if it exists.\n- Creates a new table with payment transaction data, joining with the unique merchants table.\n\n**Create driver_positive_train_attributed table** (Lines 124-168):\n- Drops the existing table if it exists.\n- Creates a new table with positive training samples attributed to transactions, applying sampling ratios.\n\n**Create driver_positive_train_organic_0 table** (Lines 170-204):\n- Drops the existing table if it exists.\n- Creates a new table with organic positive training samples, applying various filters to remove biases.\n\n**Create driver_positive_train_organic table** (Lines 207-244):\n- Drops the existing table if it exists.\n- Creates a new table with sampled organic positive training data, applying merchant sampling ratios.\n\n**Create driver_positive table** (Lines 247-277):\n- Drops the existing table if it exists.\n- Combines various positive training samples into a single table, including attributed, organic, and saved transactions.\n\n**Create driver_positive_training_split_0 table** (Lines 281-307):\n- Drops the existing table if it exists.\n- Creates a new table with positive training samples, marking hard and uniform negatives.\n\n**Create driver_positive_training_split table** (Lines 310-321):\n- Drops the existing table if it exists.\n- Adds a count of daily positive samples to the training split table.\n\n**Create driver_training_hard_negative table** (Lines 324-336):\n- Drops the existing table if it exists.\n- Creates a new table with hard negative samples by joining with the `driver_0` table.\n\n**Create driver_training_hard_negative_downsample table** (Lines 339-349):\n- Drops the existing table if it exists.\n- Downsamples hard negative samples to match the count of daily positive samples.\n\n**Create driver_training_uniform_negative table** (Lines 352-364):\n- Drops the existing table if it exists.\n- Creates a new table with uniform negative samples by joining with the unique merchants table.\n\n**Create driver_training_uniform_negative_remove_window_positive table** (Lines 367-376):\n- Drops the existing table if it exists.\n- Removes uniform negative samples that overlap with positive feedback windows.\n\n**Create driver_training_uniform_negative_downsample_0 table** (Lines 379-401):\n- Drops the existing table if it exists.\n- Downsamples uniform negative samples based on merchant sampling probabilities.\n\n**Create driver_training_uniform_negative_downsample table** (Lines 404-407):\n- Drops the existing table if it exists.\n- Further downsamples uniform negative samples to match the positive-to-negative ratio.\n\n**Create driver_dev table** (Lines 410-445):\n- Drops the existing table if it exists.\n- Combines positive and negative samples into a development dataset, including both hard and uniform negatives.\n\n**Create driver_oot_uniform_negative_0 table** (Lines 449-469):\n- Drops the existing table if it exists.\n- Creates a new table with out-of-time (OOT) uniform negative samples.\n\n**Create driver_oot_uniform_negative table** (Lines 472-478):\n- Drops the existing table if it exists.\n- Removes duplicate OOT uniform negative samples.\n\n**Create driver_oot table** (Lines 481-495):\n- Drops the existing table if it exists.\n- Combines positive and negative OOT samples into a single table.\n\n**Create driver_simu table** (Lines 498-506):\n- Drops the existing table if it exists.\n- Combines development and OOT samples into a simulation dataset.\n\n**Create driver_simu_consumer table** (Lines 509-512):\n- Drops the existing table if it exists.\n- Creates a new table with distinct customer IDs and run dates from the simulation dataset."
    },
    {
      "summary": "**Load YAML configuration file (Lines 4-13):**\n- Defines a function to load a YAML file.\n- Loads a configuration file and assigns it to a variable.\n\n**Extract BigQuery project dataset prefix (Lines 14-16):**\n- Extracts the BigQuery project dataset prefix from the loaded configuration.\n\n**Create driver_simu_txn_365d table (Lines 17-40):**\n- Drops the existing table if it exists.\n- Creates a new table with transaction data joined with payment data, including various date intervals.\n\n**Create driver_simu_txn_365d_agg table (Lines 43-55):**\n- Drops the existing table if it exists.\n- Creates a new aggregated table with transaction counts and amounts over different time intervals.\n\n**Create driver_consumer_base table (Lines 58-66):**\n- Drops the existing table if it exists.\n- Creates a new table with consumer base data including various date intervals.\n\n**Create driver_consumer_base_txn_5k_merch_category table (Lines 69-93):**\n- Drops the existing table if it exists.\n- Creates a new table with consumer base data joined with transaction and merchant category data, including recency rank.\n\n**Create driver_consumer_base_last_10_txn table (Lines 96-175):**\n- Drops the existing table if it exists.\n- Creates a new table with the last 10 transactions for each consumer, including average transaction amounts.\n\n**Create driver_consumer_base_all_history_array_0 table (Lines 178-185):**\n- Drops the existing table if it exists.\n- Creates a new table with the most recent 100 transactions for each consumer.\n\n**Create driver_consumer_base_all_history_array table (Lines 188-194):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated lists of the most recent 100 merchants and categories for each consumer.\n\n**Create driver_combine_category table (Lines 198-203):**\n- Drops the existing table if it exists.\n- Creates a new table combining consumer and merchant category data.\n\n**Create driver_combine_category_agg_0 table (Lines 206-219):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated transaction data by category over different time intervals.\n\n**Create driver_combine_category_agg_1 table (Lines 222-231):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated transaction counts over different time intervals.\n\n**Create driver_combine_category_agg_2 table (Lines 234-257):**\n- Drops the existing table if it exists.\n- Creates a new table with average transaction amounts by category over different time intervals.\n\n**Create driver_combine_category_agg_3 table (Lines 260-286):**\n- Drops the existing table if it exists.\n- Creates a new table combining consumer and receiver category data with aggregated transaction data.\n\n**Create driver_combine_category_agg_4 table (Lines 289-302):**\n- Drops the existing table if it exists.\n- Creates a new table with transaction frequency ranks by category over different time intervals.\n\n**Create driver_combine_category_agg_5 table (Lines 305-464):**\n- Drops the existing table if it exists.\n- Creates a new table with the top 3 frequent merchant categories over different time intervals.\n\n**Create driver_merchant_base table (Lines 468-476):**\n- Drops the existing table if it exists.\n- Creates a new table with merchant base data including a 30-day interval.\n\n**Create driver_merchant_base_txn_30d table (Lines 479-506):**\n- Drops the existing table if it exists.\n- Creates a new table with merchant transaction data over the last 30 days.\n\n**Create driver_merchant_base_txn_30d_filter_sndr table (Lines 509-523):**\n- Drops the existing table if it exists.\n- Creates a new table filtering merchant transactions by sender account type.\n\n**Create driver_merchant_base_price_agg table (Lines 526-551):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated price data for merchants over the last 30 days.\n\n**Create driver_merchant_base_sales_agg table (Lines 554-566):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated sales data for merchants over the last 30 days.\n\n**Create driver_elig_save_365d_category table (Lines 569-616):**\n- Drops the existing table if it exists.\n- Creates a new table with consumer save event data joined with merchant category data.\n\n**Create driver_elig_save_agg_00 table (Lines 619-630):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated save event counts over different time intervals.\n\n**Create driver_elig_save_agg_0 table (Lines 633-643):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated save event counts for consumers over different time intervals.\n\n**Create driver_elig_save_agg_1 table (Lines 646-717):**\n- Drops the existing table if it exists.\n- Creates a new table with the last 5 save events for each consumer.\n\n**Create driver_elig_save_agg_2 table (Lines 720-731):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated save event counts by category over different time intervals.\n\n**Create driver_elig_save_agg_3 table (Lines 734-751):**\n- Drops the existing table if it exists.\n- Creates a new table combining consumer and receiver category data with aggregated save event counts.\n\n**Create driver_merchant_base_click_save table (Lines 754-793):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated save event counts for merchants over the last 30 days, segmented by various criteria.\n\n**Create driver_consumer_base_gender table (Lines 796-817):**\n- Drops the existing table if it exists.\n- Creates a new table with consumer gender data based on first name predictions."
    },
    {
      "summary": "**Authenticate user and set environment variable (Lines 4-9):**\n- Imports a cloud module and authenticates the user.\n- Sets an environment variable to disable a development feature.\n\n**Export data from BigQuery to Google Cloud Storage (Lines 13-22):**\n- Exports data from a BigQuery table to a specified Google Cloud Storage location in Parquet format.\n\n**Import necessary modules and set up Fetcher (Lines 23-40):**\n- Imports various modules including json, os, sys, datetime, and a Fetcher component.\n- Initializes variables for the current date, user, production status, stage, sequence number, and job name.\n- Configures the Fetcher with job details, including job name, group name, model name, model owner, description, and manager.\n\n**Configure Fetcher with GCP settings and variables (Lines 43-66):**\n- Sets GCP-related configurations such as project ID, bucket name, BigQuery project and dataset, and data locations.\n- Specifies the variables to be fetched and the split ratio for training data.\n- Optionally sets the workspace for the Fetcher.\n\n**Run the Fetcher (Line 67):**\n- Executes the Fetcher with the configured settings.\n\n**Create or replace an external table in BigQuery (Lines 68-73):**\n- Creates or replaces an external table in BigQuery using data stored in Google Cloud Storage in Parquet format."
    },
    {
      "summary": "**[Load YAML configuration file] (Lines 4-13):**\n- Defines a function to load a YAML file.\n- Attempts to read and parse the YAML file, returning its content or None if the file is not found.\n- Loads a specific configuration file into a variable.\n\n**[Extract BigQuery project dataset prefix from configuration] (Lines 14-16):**\n- Checks if the configuration is loaded.\n- Extracts the BigQuery project dataset prefix from the configuration.\n\n**[Create driver_dev_features table in BigQuery] (Lines 17-218):**\n- Constructs a SQL query to drop and create a new table named `driver_dev_features`.\n- The new table is created by selecting and joining multiple tables, with various columns being coalesced to handle null values.\n\n**[Create driver_oot_features table in BigQuery] (Lines 219-420):**\n- Constructs a SQL query to drop and create a new table named `driver_oot_features`.\n- Similar to the previous block, this table is created by selecting and joining multiple tables, with various columns being coalesced to handle null values.\n\n**[Expand driver_oot_features table with historical receiver IDs and categories] (Lines 421-625):**\n- Constructs a SQL query to drop and create a new table named `driver_oot_features_expand_seq`.\n- Adds columns to the table by splitting a comma-separated list of historical receiver IDs and categories into individual columns.\n\n**[Export driver_dev_features table to Google Cloud Storage] (Lines 630-639):**\n- Constructs a SQL query to export the `driver_dev_features` table to Google Cloud Storage in Parquet format.\n- Specifies the URI and format for the export, and sets the overwrite option to true."
    },
    {
      "summary": "**[Import necessary libraries and modules] (Lines 1-9):**\n- Import various libraries and modules required for data processing, transformation, and machine learning tasks.\n\n**[Set up model version and directories] (Lines 10-20):**\n- Define the model version based on the current date.\n- Create directories for storing model artifacts if they do not already exist.\n- Save the current model version to a file.\n\n**[Initialize dictionaries and read data files] (Lines 21-30):**\n- Initialize dictionaries for storing feature encoders and scalers.\n- Read parquet files from a specified directory and concatenate them into a single DataFrame.\n\n**[Map state names to abbreviations] (Lines 31-87):**\n- Define a dictionary to map full state names to their abbreviations.\n- Create a mapping for state names found in the data to their abbreviations.\n\n**[Clean and encode state feature] (Lines 88-104):**\n- Define a function to clean state names and apply it to the state column in the data.\n- Encode the cleaned state names using LabelEncoder and store the encoder mappings.\n\n**[Encode categorical features] (Lines 105-112):**\n- Encode several categorical features using LabelEncoder and store the encoder mappings.\n\n**[Scale numerical features] (Lines 113-134):**\n- Define a list of numerical features to be scaled.\n- Scale each numerical feature using StandardScaler and store the scaler parameters.\n\n**[Process sequence features] (Lines 135-153):**\n- Replace and fill missing values in sequence features.\n- Convert sequence features to padded sequences and tokenize them.\n- Rename and drop unnecessary columns.\n\n**[Write transformed data to parquet files] (Lines 154-166):**\n- Define a function to write DataFrame chunks to parquet files.\n- Split the data into chunks and write each chunk to a specified directory.\n\n**[Export feature transformers] (Lines 167-175):**\n- Save the trained Tokenizer, categorical feature encoders, and numerical feature scalers to files using pickle."
    },
    {
      "summary": "**[Authenticate user and set up DataProc cluster] (Lines 1-11):**\n- Imports the `aml.cloud_v1` module.\n- Authenticates the user.\n- Creates a DataProc client for a specific GCP project.\n- Defines a list of packages to install on the cluster.\n- Creates a DataProc cluster with specified configurations (name, image, packages, number of workers, memory, CPUs, and wait for completion).\n\n"
    },
    {
      "summary": "**[Import necessary libraries and configure GPU settings] (Lines 2-14):**\n- Import essential libraries for data processing, machine learning, and model training.\n- Configure TensorFlow to use GPU with memory growth enabled.\n\n**[Load configuration and model version] (Lines 15-34):**\n- Define a function to load a YAML configuration file.\n- Load the configuration file and model version.\n- Set up directories for saving model artifacts.\n\n**[Load and concatenate data] (Lines 36-43):**\n- List and read all parquet files from a specified directory.\n- Concatenate the data from these files into a single DataFrame.\n\n**[Load feature encoders and split data] (Lines 44-49):**\n- Load categorical feature encoders and tokenizer from pickle files.\n- Split the data into training and validation sets based on a 'split' column.\n\n**[Define feature names] (Lines 52-74):**\n- Define lists of numeric, categorical, and ID feature names to be used in the model.\n\n**[Function to prepare data for model input] (Lines 75-106):**\n- Define a function to create feature columns and prepare data for model input.\n- Include dense, sparse, and variable-length sparse features.\n\n**[Prepare training and validation data] (Lines 107-108):**\n- Use the defined function to prepare training and validation data.\n\n**[Define data generator function] (Lines 110-120):**\n- Define a generator function to yield batches of data for training.\n\n**[Create TensorFlow datasets] (Lines 121-137):**\n- Create TensorFlow datasets for training and validation using the data generator function.\n- Shuffle the datasets.\n\n**[Set up and compile the model] (Lines 140-155):**\n- Use TensorFlow's MirroredStrategy for distributed training.\n- Define and compile the DIN model with specified configurations.\n\n**[Train the model with early stopping] (Lines 156-170):**\n- Set up early stopping callback.\n- Train the model using the training dataset and validate using the validation dataset.\n\n**[Save the trained model] (Lines 171-175):**\n- Save the trained model in both H5 and TensorFlow SavedModel formats.\n\n**[Convert TensorFlow model to ONNX format] (Lines 176-186):**\n- Convert the TensorFlow model to ONNX format and save the ONNX specification.\n\n**[Prepare and save the production model] (Lines 196-227):**\n- Filter non-ASCII characters from categorical feature encoders.\n- Load numerical feature scalars.\n- Create a graph for the production model, including renaming and scaling nodes.\n- Save the production model.\n\n**[Prepare test data and make predictions] (Lines 230-248):**\n- Prepare test data for prediction.\n- Save test data to a JSON file.\n\n**[Convert and test ONNX model] (Lines 250-269):**\n- Convert the TensorFlow model to ONNX format.\n- Load the ONNX model and prepare test data for ONNX inference.\n- Compare predictions from TensorFlow and ONNX models to ensure consistency."
    },
    {
      "summary": "**[Set up environment and configuration] (Lines 2-24):**\n- Import necessary modules and libraries.\n- Set working directory and user-specific paths.\n- Load configuration settings and validate setup.\n- Load the current model version from a file.\n\n**[Define function for scoring out-of-time (OOT) data] (Lines 26-48):**\n- Define a function `oot_data_eval` to score OOT data using specified models.\n- Load OOT data from a specified path.\n- Copy model files from Google Cloud Storage (GCS) to a local temporary directory.\n- Initialize a model scorer and create a DataFrame with scores.\n- Optionally select specific columns to keep in the output.\n- Save the scored data to a specified path in parquet format.\n\n**[Prepare model paths and scoring parameters] (Lines 49-69):**\n- Define local and GCS paths for model files and OOT data.\n- Load model specifications and prepare lists of model paths and score outputs.\n\n**[Submit Spark job to GCP] (Lines 70-86):**\n- Initialize a GCP client for submitting Spark jobs.\n- Create and submit a Spark job to GCP to run the `oot_data_eval` function with specified parameters and configurations.\n- Retrieve and display the job ID.\n\n**[Monitor job status and save logs] (Lines 87-94):**\n- Check the status of the submitted Spark job and wait for its completion.\n- Save the job logs to a local file for future reference.\n\n**[Create external table in BigQuery] (Lines 100-104):**\n- Create or replace an external table in BigQuery to reference the scored OOT data stored in GCS."
    },
    {
      "summary": "**Load model version and create directories if not exist (Lines 4-12):**\n- Loads the current model version from a pickle file.\n- Constructs paths for model artifacts and evaluation readouts.\n- Creates the evaluation readout directory if it does not exist.\n\n**Load YAML configuration file (Lines 13-25):**\n- Defines a function to load a YAML file.\n- Loads the base configuration from a specified YAML file.\n- Extracts the BigQuery project dataset prefix from the configuration.\n\n**Create driver_oot_txn_365d table (Lines 26-44):**\n- Constructs a SQL query to create a table with transaction data joined with payment data.\n- The table includes transactions within a 365-day window.\n\n**Create driver_oot_txn_save_365d table (Lines 47-65):**\n- Constructs a SQL query to create a table with transaction data joined with save event data.\n- The table includes save events within a 365-day window.\n\n**Create mlv2_gpt_similar_map_snapshot table (Lines 68-137):**\n- Constructs a SQL query to create a table with similar merchant mappings.\n- Joins the mappings with live unique merchants.\n\n**Create mlv2_gpt_similar_map_snapshot_1 table (Lines 140-193):**\n- Constructs a SQL query to create a table with concatenated similar merchant lists.\n- Splits the concatenated list into individual similar merchant columns.\n\n**Create driver_oot_txn_save_365d_similar table (Lines 196-207):**\n- Constructs a SQL query to create a table with transaction and save data joined with similar merchants.\n- Includes a column indicating similarity based on purchase or save events.\n\n**Create driver_oot_txn_save_365d_similar_dedup table (Lines 209-246):**\n- Constructs a SQL query to create a deduplicated table of similar merchants.\n- Aggregates and deduplicates similar merchants for each customer and run date.\n\n**Create driver_oot_two_tower_similar_score table (Lines 249-336):**\n- Constructs a SQL query to create a table with similarity scores between customers and merchants.\n- Joins customer and merchant embeddings to calculate dot product similarity scores.\n\n**Create driver_oot_hueristic_model_comparison table (Lines 339-372):**\n- Constructs a SQL query to create a table comparing heuristic model scores.\n- Includes ranks based on past transactions, two-tower similarity, and transformed scores.\n\n**Calculate recall metrics for all models (Lines 375-430):**\n- Constructs a SQL query to calculate recall metrics for different models.\n- Groups results by model and calculates recall at various ranks.\n\n**Save and plot performance metrics (Lines 433-447):**\n- Saves the performance metrics to a CSV file.\n- Plots bar charts of the performance metrics using Seaborn and Matplotlib.\n\n**Calculate recall metrics for first-time users (Lines 448-513):**\n- Constructs a SQL query to calculate recall metrics for first-time users.\n- Groups results by model and calculates recall at various ranks for first-time users.\n\n**Save and plot performance metrics for first-time users (Lines 516-530):**\n- Saves the first-time user performance metrics to a CSV file.\n- Plots bar charts of the first-time user performance metrics using Seaborn and Matplotlib.\n\n**Calculate recall metrics excluding specific merchants (Lines 531-590):**\n- Constructs a SQL query to calculate recall metrics excluding specific merchants.\n- Groups results by model and calculates recall at various ranks excluding specified merchants.\n\n**Save and plot performance metrics excluding specific merchants (Lines 592-607):**\n- Saves the performance metrics excluding specific merchants to a CSV file.\n- Plots bar charts of the performance metrics excluding specific merchants using Seaborn and Matplotlib.\n\n**Calculate recall metrics for recent data (Lines 608-728):**\n- Constructs a SQL query to calculate recall metrics for recent data.\n- Groups results by model and calculates recall at various ranks for recent data.\n\n**Save and plot performance metrics for recent data (Lines 730-731):**\n- Saves the recent data performance metrics to a CSV file.\n- Plots bar charts of the recent data performance metrics using Seaborn and Matplotlib."
    }
  ],
  "all_nodes": [
    "MAJOR COMPONENTS IDENTIFIED: [Driver Creation]\n\nDETAILS FOR EACH:\n[Driver Creation]:\n    - Line Range: Lines 4-512\n    - Evidence:\n        - \"Create live unique merchants training table\" (Lines 18-52) \u2013 This section involves creating a new table with unique merchants by joining multiple tables, which is part of the initial data loading and extraction process.\n        - \"Create driver_00 table\" (Lines 54-81) \u2013 This section involves creating a new table with customer and merchant interaction data, which is a fundamental part of creating the driver dataset.\n        - \"Create driver_positive_train_attributed table\" (Lines 124-168) \u2013 This section involves creating a new table with positive training samples attributed to transactions, which is essential for the driver dataset.\n        - \"Create driver_dev table\" (Lines 410-445) \u2013 This section involves combining positive and negative samples into a development dataset, which is a crucial part of the driver dataset creation.\n        - \"Create driver_oot table\" (Lines 481-495) \u2013 This section involves combining positive and negative OOT samples into a single table, which is part of the driver dataset creation.\n        - \"Create driver_simu table\" (Lines 498-506) \u2013 This section involves combining development and OOT samples into a simulation dataset, which is part of the driver dataset creation.\n\nThe entire file is dedicated to creating various tables that form the driver dataset, which is the initial and fundamental step in the ML pipeline. Each table creation step is part of the process of loading, extracting, and transforming data to form the driver dataset, which will be used in subsequent steps of the ML workflow. Therefore, all these steps collectively fall under the \"Driver Creation\" component.",
    "MAJOR COMPONENTS IDENTIFIED: [Driver Creation]\n\nDETAILS FOR EACH:\n[Driver Creation]:\n    - Line Range: Lines 17-817\n    - Evidence:\n        - \"Create driver_simu_txn_365d table (Lines 17-40): ... Creates a new table with transaction data joined with payment data, including various date intervals.\" \u2013 This indicates the creation of a foundational dataset with transaction and payment data.\n        - \"Create driver_consumer_base table (Lines 58-66): ... Creates a new table with consumer base data including various date intervals.\" \u2013 This shows the creation of another foundational dataset with consumer base data.\n        - \"Create driver_merchant_base table (Lines 468-476): ... Creates a new table with merchant base data including a 30-day interval.\" \u2013 This indicates the creation of a foundational dataset with merchant base data.\n        - \"Create driver_elig_save_365d_category table (Lines 569-616): ... Creates a new table with consumer save event data joined with merchant category data.\" \u2013 This shows the creation of a foundational dataset with consumer save event data.\n    - Why This Is Separate: The entire file is dedicated to creating various driver tables, which are foundational datasets for the ML pipeline. These tables include transaction data, consumer base data, merchant base data, and save event data, all of which are essential for subsequent steps in the ML workflow. The creation of these tables is a distinct and substantial component of the ML pipeline, fitting the \"Driver Creation\" category. There is no overlap with other identified components' line ranges as the entire file focuses on this task.",
    "MAJOR COMPONENTS IDENTIFIED: [Data Pulling]\n\nDETAILS FOR EACH:\n[Data Pulling]:\n    - Line Range: Lines 23-67\n    - Evidence:\n        - \"Imports various modules including json, os, sys, datetime, and a Fetcher component.\" \u2013 This indicates the setup for data fetching.\n        - \"Configures the Fetcher with job details, including job name, group name, model name, model owner, description, and manager.\" \u2013 This shows the configuration of the Fetcher, which is used to pull data.\n        - \"Specifies the variables to be fetched and the split ratio for training data.\" \u2013 This confirms the fetching of specific variables for the dataset.\n        - \"Executes the Fetcher with the configured settings.\" \u2013 This is the actual execution of the data pulling process.\n\nCould not identify any other major ML components.",
    "MAJOR COMPONENTS IDENTIFIED: [Feature Consolidation]\n\nDETAILS FOR EACH:\n[Feature Consolidation]:\n    - Line Range: Lines 17-625\n    - Evidence:\n        - \"Create driver_dev_features table in BigQuery\" (Lines 17-218) \u2013 This section involves constructing a SQL query to create a new table by selecting and joining multiple tables, which is a key aspect of feature consolidation.\n        - \"Create driver_oot_features table in BigQuery\" (Lines 219-420) \u2013 Similar to the previous block, this section constructs a SQL query to create another table by selecting and joining multiple tables, further supporting the feature consolidation process.\n        - \"Expand driver_oot_features table with historical receiver IDs and categories\" (Lines 421-625) \u2013 This section involves adding columns to the table by splitting a comma-separated list into individual columns, which is part of consolidating features into a unified dataset.\n\n    - Why This Is Separate: The process of merging multiple datasets into a unified feature set for modeling is distinct from other ML components such as data preprocessing or model training. The lines involved in this process (17-625) do not overlap with other components identified in the summary, ensuring a clear separation.\n\nCould not identify any other major ML components.",
    "MAJOR COMPONENTS IDENTIFIED: [Data Preprocessing]\n\nDETAILS FOR EACH:\n[Data Preprocessing]:\n    - Line Range: Lines 21-175\n    - Evidence:\n        - \"Initialize dictionaries for storing feature encoders and scalers.\" \u2013 This indicates the setup for data transformation processes.\n        - \"Read parquet files from a specified directory and concatenate them into a single DataFrame.\" \u2013 This is part of the data loading and initial preparation.\n        - \"Define a function to clean state names and apply it to the state column in the data.\" \u2013 This is a data cleaning step.\n        - \"Encode the cleaned state names using LabelEncoder and store the encoder mappings.\" \u2013 This involves encoding categorical variables.\n        - \"Scale each numerical feature using StandardScaler and store the scaler parameters.\" \u2013 This involves scaling numerical features.\n        - \"Replace and fill missing values in sequence features.\" \u2013 This is part of handling missing data.\n        - \"Convert sequence features to padded sequences and tokenize them.\" \u2013 This is a transformation step for sequence data.\n        - \"Define a function to write DataFrame chunks to parquet files.\" \u2013 This involves saving the processed data.\n        - \"Save the trained Tokenizer, categorical feature encoders, and numerical feature scalers to files using pickle.\" \u2013 This involves saving the preprocessing models.\n\nThe entire range from lines 21 to 175 involves various steps of data preprocessing, including cleaning, encoding, scaling, and saving the transformed data and preprocessing models. These steps are all integral parts of the Data Preprocessing component and are necessary for preparing the data for subsequent modeling tasks.",
    "MAJOR COMPONENTS IDENTIFIED: Could not identify any major ML components\n\nDETAILS FOR EACH:\nNone\n\nThe provided code summary for the file `04_prepare_oot_data.ipynb` only includes steps related to user authentication and setting up a DataProc cluster. These steps are preparatory and do not fall into any of the predefined ML component categories such as Driver Creation, Feature Engineering, Data Pulling, etc. Therefore, no major ML components can be confidently identified from the given summary.",
    "MAJOR COMPONENTS IDENTIFIED: [Model Training, Model Packaging, Model Scoring]\n\nDETAILS FOR EACH:\n[Model Training]:\n    - Line Range: Lines 140-170\n    - Evidence:\n        - \"Use TensorFlow's MirroredStrategy for distributed training.\" \u2013 This indicates the setup for model training using TensorFlow.\n        - \"Train the model using the training dataset and validate using the validation dataset.\" \u2013 This confirms the actual training process of the model.\n    - Why This Is Separate: This section is focused on the training of the model, which is a distinct and critical step in the ML workflow. It involves setting up the model, compiling it, and training it with the dataset.\n\n[Model Packaging]:\n    - Line Range: Lines 171-186\n    - Evidence:\n        - \"Save the trained model in both H5 and TensorFlow SavedModel formats.\" \u2013 This indicates the process of saving the trained model in deployment-ready formats.\n        - \"Convert the TensorFlow model to ONNX format and save the ONNX specification.\" \u2013 This further confirms the packaging of the model into a different format for deployment.\n    - Why This Is Separate: This section deals with converting and saving the trained model into formats suitable for deployment, which is a distinct step from training the model.\n\n[Model Scoring]:\n    - Line Range: Lines 230-248\n    - Evidence:\n        - \"Prepare test data for prediction.\" \u2013 This indicates the preparation of data for model inference.\n        - \"Save test data to a JSON file.\" \u2013 This confirms the process of scoring the model on test data.\n    - Why This Is Separate: This section is focused on using the trained model to make predictions on test data, which is a distinct step from both training and packaging the model.\n\nFULL ML PIPELINE FILE LIST:\n['repos/ql-store-recommendation-prod/research/pipeline/00_driver.py', 'repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.py', 'repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.py', 'repos/ql-store-recommendation-prod/research/pipeline/02_combine.py', 'repos/ql-store-recommendation-prod/research/pipeline/03_prepare_training_data.py', 'repos/ql-store-recommendation-prod/research/pipeline/04_prepare_oot_data.py', 'repos/ql-store-recommendation-prod/research/pipeline/04_training.py', 'repos/ql-store-recommendation-prod/research/pipeline/05_scoring_oot.py', 'repos/ql-store-recommendation-prod/research/pipeline/06_evaluation.py']\n\nCURRENT FILE'S NAME:\n04_training.ipynb\n\nCURRENT FILE'S CODE SUMMARY:\n**[Import necessary libraries and configure GPU settings] (Lines 2-14):**\n- Import essential libraries for data processing, machine learning, and model training.\n- Configure TensorFlow to use GPU with memory growth enabled.\n\n**[Load configuration and model version] (Lines 15-34):**\n- Define a function to load a YAML configuration file.\n- Load the configuration file and model version.\n- Set up directories for saving model artifacts.\n\n**[Load and concatenate data] (Lines 36-43):**\n- List and read all parquet files from a specified directory.\n- Concatenate the data from these files into a single DataFrame.\n\n**[Load feature encoders and split data] (Lines 44-49):**\n- Load categorical feature encoders and tokenizer from pickle files.\n- Split the data into training and validation sets based on a 'split' column.\n\n**[Define feature names] (Lines 52-74):**\n- Define lists of numeric, categorical, and ID feature names to be used in the model.\n\n**[Function to prepare data for model input] (Lines 75-106):**\n- Define a function to create feature columns and prepare data for model input.\n- Include dense, sparse, and variable-length sparse features.\n\n**[Prepare training and validation data] (Lines 107-108):**\n- Use the defined function to prepare training and validation data.\n\n**[Define data generator function] (Lines 110-120):**\n- Define a generator function to yield batches of data for training.\n\n**[Create TensorFlow datasets] (Lines 121-137):**\n- Create TensorFlow datasets for training and validation using the data generator function.\n- Shuffle the datasets.\n\n**[Set up and compile the model] (Lines 140-155):**\n- Use TensorFlow's MirroredStrategy for distributed training.\n- Define and compile the DIN model with specified configurations.\n\n**[Train the model with early stopping] (Lines 156-170):**\n- Set up early stopping callback.\n- Train the model using the training dataset and validate using the validation dataset.\n\n**[Save the trained model] (Lines 171-175):**\n- Save the trained model in both H5 and TensorFlow SavedModel formats.\n\n**[Convert TensorFlow model to ONNX format] (Lines 176-186):**\n- Convert the TensorFlow model to ONNX format and save the ONNX specification.\n\n**[Prepare and save the production model] (Lines 196-227):**\n- Filter non-ASCII characters from categorical feature encoders.\n- Load numerical feature scalars.\n- Create a graph for the production model, including renaming and scaling nodes.\n- Save the production model.\n\n**[Prepare test data and make predictions] (Lines 230-248):**\n- Prepare test data for prediction.\n- Save test data to a JSON file.\n\n**[Convert and test ONNX model] (Lines 250-269):**\n- Convert the TensorFlow model to ONNX format.\n- Load the ONNX model and prepare test data for ONNX inference.\n- Compare predictions from TensorFlow and ONNX models to ensure consistency.",
    "MAJOR COMPONENTS IDENTIFIED: [Model Scoring]\n\nDETAILS FOR EACH:\n[Model Scoring]:\n    - Line Range: Lines 26-104\n    - Evidence:\n        - \"Define a function `oot_data_eval` to score OOT data using specified models.\" \u2013 This indicates the primary function of the code is to score out-of-time (OOT) data, which falls under the Model Scoring category.\n        - \"Initialize a model scorer and create a DataFrame with scores.\" \u2013 This further supports that the main task is scoring data using a model.\n        - \"Save the scored data to a specified path in parquet format.\" \u2013 This shows the output of the scoring process, which is a key part of Model Scoring.\n        - \"Submit Spark job to GCP to run the `oot_data_eval` function with specified parameters and configurations.\" \u2013 This indicates the execution of the scoring function on a distributed system, which is part of the scoring process.\n        - \"Create or replace an external table in BigQuery to reference the scored OOT data stored in GCS.\" \u2013 This final step involves making the scored data accessible for further use, completing the Model Scoring process.\n\nCould not identify any other major ML components.",
    "MAJOR COMPONENTS IDENTIFIED: [Model Evaluation]\n\nDETAILS FOR EACH:\n[Model Evaluation]:\n    - Line Range: Lines 375-731\n    - Evidence:\n        - \"Calculate recall metrics for all models (Lines 375-430)\" \u2013 This section involves calculating performance metrics, which is a key part of model evaluation.\n        - \"Save and plot performance metrics (Lines 433-447)\" \u2013 Saving and visualizing performance metrics is part of evaluating the model's effectiveness.\n        - \"Calculate recall metrics for first-time users (Lines 448-513)\" \u2013 Evaluating model performance specifically for first-time users.\n        - \"Save and plot performance metrics for first-time users (Lines 516-530)\" \u2013 Saving and visualizing performance metrics for first-time users.\n        - \"Calculate recall metrics excluding specific merchants (Lines 531-590)\" \u2013 Evaluating model performance excluding specific merchants.\n        - \"Save and plot performance metrics excluding specific merchants (Lines 592-607)\" \u2013 Saving and visualizing performance metrics excluding specific merchants.\n        - \"Calculate recall metrics for recent data (Lines 608-728)\" \u2013 Evaluating model performance on recent data.\n        - \"Save and plot performance metrics for recent data (Lines 730-731)\" \u2013 Saving and visualizing performance metrics for recent data.\n\n    - Why This Is Separate: The entire section from Lines 375-731 is dedicated to evaluating the model's performance across various scenarios and saving/plotting the results. This is a distinct and critical part of the ML workflow that focuses on assessing how well the model performs, which is separate from other components like data preprocessing or model training. There is no overlap with other identified components' line ranges, ensuring this is a unique and standalone node in the ML workflow."
  ],
  "consolidated_nodes": [],
  "edges": [],
  "dag_yaml": "",
  "verified_dag": {},
  "config": {},
  "notebooks": []
}