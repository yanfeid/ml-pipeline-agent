{
  "github_url": "https://github.paypal.com/GADS-Consumer-ML/ql-store-recommendation-prod.git",
  "input_files": [
    "research/pipeline/00_driver.ipynb",
    "research/pipeline/01_bq_feat.ipynb",
    "research/pipeline/01_varmart_feat.ipynb",
    "research/pipeline/02_combine.ipynb",
    "research/pipeline/03_prepare_training_data.ipynb",
    "research/pipeline/04_prepare_oot_data.ipynb",
    "research/pipeline/04_training.ipynb",
    "research/pipeline/05_scoring_oot.ipynb",
    "research/pipeline/06_evaluation.ipynb"
  ],
  "repo_name": "ql-store-recommendation-prod",
  "local_repo_path": "repos/ql-store-recommendation-prod",
  "files": [
    "repos/ql-store-recommendation-prod/research/pipeline/00_driver.py",
    "repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.py",
    "repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.py",
    "repos/ql-store-recommendation-prod/research/pipeline/02_combine.py",
    "repos/ql-store-recommendation-prod/research/pipeline/03_prepare_training_data.py",
    "repos/ql-store-recommendation-prod/research/pipeline/04_prepare_oot_data.py",
    "repos/ql-store-recommendation-prod/research/pipeline/04_training.py",
    "repos/ql-store-recommendation-prod/research/pipeline/05_scoring_oot.py",
    "repos/ql-store-recommendation-prod/research/pipeline/06_evaluation.py"
  ],
  "summaries": [
    {
      "summary": "**Load YAML configuration file** (Lines 4-13):\n- Defines a function to load a YAML file.\n- Loads the configuration from a specified file path.\n\n**Set BigQuery prefix** (Lines 14-16):\n- Extracts the BigQuery project dataset prefix from the configuration.\n\n**Create live unique merchants training table** (Lines 18-52):\n- Drops the existing table if it exists.\n- Creates a new table with unique merchants, joining multiple tables to gather necessary merchant information.\n\n**Create driver_00 table** (Lines 54-81):\n- Drops the existing table if it exists.\n- Creates a new table with customer and merchant interaction data, filtering based on specific conditions.\n\n**Create driver_0 table** (Lines 84-102):\n- Drops the existing table if it exists.\n- Creates a new table by filtering `driver_00` to include customers with multiple placements or specific transaction conditions.\n\n**Create driver_1 table** (Lines 105-120):\n- Drops the existing table if it exists.\n- Creates a new table with payment transaction data, joining with the unique merchants table.\n\n**Create driver_positive_train_attributed table** (Lines 124-168):\n- Drops the existing table if it exists.\n- Creates a new table with positive training samples attributed to transactions, applying sampling ratios.\n\n**Create driver_positive_train_organic_0 table** (Lines 170-204):\n- Drops the existing table if it exists.\n- Creates a new table with organic positive training samples, applying various filters to remove biases.\n\n**Create driver_positive_train_organic table** (Lines 207-244):\n- Drops the existing table if it exists.\n- Creates a new table with sampled organic positive training data, applying merchant sampling ratios.\n\n**Create driver_positive table** (Lines 247-277):\n- Drops the existing table if it exists.\n- Combines various positive training samples into a single table, including attributed, organic, and saved transactions.\n\n**Create driver_positive_training_split_0 table** (Lines 281-307):\n- Drops the existing table if it exists.\n- Creates a new table with positive training samples, marking hard and uniform negatives.\n\n**Create driver_positive_training_split table** (Lines 310-321):\n- Drops the existing table if it exists.\n- Adds a count of daily positive samples to the training split table.\n\n**Create driver_training_hard_negative table** (Lines 324-336):\n- Drops the existing table if it exists.\n- Creates a new table with hard negative samples by joining with the `driver_0` table.\n\n**Create driver_training_hard_negative_downsample table** (Lines 339-349):\n- Drops the existing table if it exists.\n- Downsamples hard negative samples to match the count of daily positive samples.\n\n**Create driver_training_uniform_negative table** (Lines 352-364):\n- Drops the existing table if it exists.\n- Creates a new table with uniform negative samples by joining with the unique merchants table.\n\n**Create driver_training_uniform_negative_remove_window_positive table** (Lines 367-376):\n- Drops the existing table if it exists.\n- Removes uniform negative samples that overlap with positive feedback windows.\n\n**Create driver_training_uniform_negative_downsample_0 table** (Lines 379-401):\n- Drops the existing table if it exists.\n- Downsamples uniform negative samples based on merchant sampling probabilities.\n\n**Create driver_training_uniform_negative_downsample table** (Lines 404-407):\n- Drops the existing table if it exists.\n- Further downsamples uniform negative samples to match the positive-to-negative ratio.\n\n**Create driver_dev table** (Lines 410-445):\n- Drops the existing table if it exists.\n- Combines positive and negative samples into a development dataset, including both hard and uniform negatives.\n\n**Create driver_oot_uniform_negative_0 table** (Lines 449-469):\n- Drops the existing table if it exists.\n- Creates a new table with out-of-time (OOT) uniform negative samples.\n\n**Create driver_oot_uniform_negative table** (Lines 472-478):\n- Drops the existing table if it exists.\n- Removes duplicate OOT uniform negative samples.\n\n**Create driver_oot table** (Lines 481-495):\n- Drops the existing table if it exists.\n- Combines positive and negative OOT samples into a single table.\n\n**Create driver_simu table** (Lines 498-506):\n- Drops the existing table if it exists.\n- Combines development and OOT samples into a simulation dataset.\n\n**Create driver_simu_consumer table** (Lines 509-512):\n- Drops the existing table if it exists.\n- Creates a new table with distinct customer IDs and run dates from the simulation dataset."
    },
    {
      "summary": "**Load YAML configuration file (Lines 4-13):**\n- Defines a function to load a YAML file.\n- Loads a configuration file and assigns it to a variable.\n\n**Extract BigQuery project dataset prefix (Lines 14-16):**\n- Extracts the BigQuery project dataset prefix from the loaded configuration.\n\n**Create driver_simu_txn_365d table (Lines 17-40):**\n- Drops the existing table if it exists.\n- Creates a new table with transaction data joined with payment data, including various date intervals.\n\n**Create driver_simu_txn_365d_agg table (Lines 43-55):**\n- Drops the existing table if it exists.\n- Creates a new aggregated table with transaction counts and amounts over different time intervals.\n\n**Create driver_consumer_base table (Lines 58-66):**\n- Drops the existing table if it exists.\n- Creates a new table with consumer base data including various date intervals.\n\n**Create driver_consumer_base_txn_5k_merch_category table (Lines 69-93):**\n- Drops the existing table if it exists.\n- Creates a new table with consumer base data joined with transaction and merchant category data, including recency rank.\n\n**Create driver_consumer_base_last_10_txn table (Lines 96-175):**\n- Drops the existing table if it exists.\n- Creates a new table with the last 10 transactions for each consumer, including average transaction amounts.\n\n**Create driver_consumer_base_all_history_array_0 table (Lines 178-185):**\n- Drops the existing table if it exists.\n- Creates a new table with the most recent 100 transactions for each consumer.\n\n**Create driver_consumer_base_all_history_array table (Lines 188-194):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated lists of the most recent 100 merchants and categories for each consumer.\n\n**Create driver_combine_category table (Lines 198-203):**\n- Drops the existing table if it exists.\n- Creates a new table combining consumer and merchant category data.\n\n**Create driver_combine_category_agg_0 table (Lines 206-219):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated transaction data by category over different time intervals.\n\n**Create driver_combine_category_agg_1 table (Lines 222-231):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated transaction counts over different time intervals.\n\n**Create driver_combine_category_agg_2 table (Lines 234-257):**\n- Drops the existing table if it exists.\n- Creates a new table with average transaction amounts by category over different time intervals.\n\n**Create driver_combine_category_agg_3 table (Lines 260-286):**\n- Drops the existing table if it exists.\n- Creates a new table combining consumer and receiver category data with aggregated transaction data.\n\n**Create driver_combine_category_agg_4 table (Lines 289-302):**\n- Drops the existing table if it exists.\n- Creates a new table with transaction frequency ranks by category over different time intervals.\n\n**Create driver_combine_category_agg_5 table (Lines 305-464):**\n- Drops the existing table if it exists.\n- Creates a new table with the top 3 frequent merchant categories over different time intervals.\n\n**Create driver_merchant_base table (Lines 468-476):**\n- Drops the existing table if it exists.\n- Creates a new table with merchant base data including a 30-day interval.\n\n**Create driver_merchant_base_txn_30d table (Lines 479-506):**\n- Drops the existing table if it exists.\n- Creates a new table with merchant transaction data over the last 30 days.\n\n**Create driver_merchant_base_txn_30d_filter_sndr table (Lines 509-523):**\n- Drops the existing table if it exists.\n- Creates a new table filtering merchant transactions by sender account type.\n\n**Create driver_merchant_base_price_agg table (Lines 526-551):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated price data for merchants over the last 30 days.\n\n**Create driver_merchant_base_sales_agg table (Lines 554-566):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated sales data for merchants over the last 30 days.\n\n**Create driver_elig_save_365d_category table (Lines 569-616):**\n- Drops the existing table if it exists.\n- Creates a new table with consumer save event data joined with merchant category data.\n\n**Create driver_elig_save_agg_00 table (Lines 619-630):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated save event counts over different time intervals.\n\n**Create driver_elig_save_agg_0 table (Lines 633-643):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated save event counts for consumers over different time intervals.\n\n**Create driver_elig_save_agg_1 table (Lines 646-717):**\n- Drops the existing table if it exists.\n- Creates a new table with the last 5 save events for each consumer.\n\n**Create driver_elig_save_agg_2 table (Lines 720-731):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated save event counts by category over different time intervals.\n\n**Create driver_elig_save_agg_3 table (Lines 734-751):**\n- Drops the existing table if it exists.\n- Creates a new table combining consumer and receiver category data with aggregated save event counts.\n\n**Create driver_merchant_base_click_save table (Lines 754-793):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated save event counts for merchants over the last 30 days, segmented by various criteria.\n\n**Create driver_consumer_base_gender table (Lines 796-817):**\n- Drops the existing table if it exists.\n- Creates a new table with consumer gender data based on first name predictions."
    },
    {
      "summary": "**Authenticate user and set environment variable (Lines 4-9):**\n- Imports a cloud module and authenticates the user.\n- Sets an environment variable to disable a development feature.\n\n**Export data from BigQuery to Google Cloud Storage (Lines 13-22):**\n- Exports data from a BigQuery table to a specified Google Cloud Storage location in Parquet format.\n\n**Import necessary modules and set up Fetcher (Lines 23-40):**\n- Imports various modules including json, os, sys, datetime, and a Fetcher component.\n- Initializes variables for the current date, user, production status, stage, sequence number, and job name.\n- Configures the Fetcher with job details, including job name, group name, model name, model owner, description, and manager.\n\n**Configure Fetcher with GCP settings and variables (Lines 43-66):**\n- Sets GCP-related configurations such as project ID, bucket name, BigQuery project and dataset, and data locations.\n- Specifies the variables to be fetched and the split ratio for training data.\n- Optionally sets the workspace for the Fetcher.\n\n**Run the Fetcher (Line 67):**\n- Executes the Fetcher with the configured settings.\n\n**Create or replace an external table in BigQuery (Lines 68-73):**\n- Creates or replaces an external table in BigQuery using data stored in Google Cloud Storage in Parquet format."
    },
    {
      "summary": "**[Load YAML configuration file] (Lines 4-13):**\n- Defines a function to load a YAML file.\n- Attempts to read and parse the YAML file, returning its content or None if the file is not found.\n- Loads a specific configuration file into a variable.\n\n**[Extract BigQuery project dataset prefix from configuration] (Lines 14-16):**\n- Checks if the configuration is loaded.\n- Extracts the BigQuery project dataset prefix from the configuration.\n\n**[Create driver_dev_features table in BigQuery] (Lines 17-218):**\n- Constructs a SQL query to drop and create a new table named `driver_dev_features`.\n- The new table is created by selecting and joining multiple tables, with various columns being coalesced to handle null values.\n\n**[Create driver_oot_features table in BigQuery] (Lines 219-420):**\n- Constructs a SQL query to drop and create a new table named `driver_oot_features`.\n- Similar to the previous block, this table is created by selecting and joining multiple tables, with various columns being coalesced to handle null values.\n\n**[Expand driver_oot_features table with historical receiver IDs and categories] (Lines 421-625):**\n- Constructs a SQL query to drop and create a new table named `driver_oot_features_expand_seq`.\n- Adds columns to the table by splitting a comma-separated list of historical receiver IDs and categories into individual columns.\n\n**[Export driver_dev_features table to Google Cloud Storage] (Lines 630-639):**\n- Constructs a SQL query to export the `driver_dev_features` table to Google Cloud Storage in Parquet format.\n- Specifies the URI and format for the export, and sets the overwrite option to true."
    },
    {
      "summary": "**[Import necessary libraries and modules] (Lines 1-9):**\n- Import various libraries and modules required for data processing, transformation, and machine learning tasks.\n\n**[Set up model version and directories] (Lines 10-20):**\n- Define the model version based on the current date.\n- Create directories for storing model artifacts if they do not already exist.\n- Save the current model version to a file.\n\n**[Initialize dictionaries and read data files] (Lines 21-30):**\n- Initialize dictionaries for storing feature encoders and scalers.\n- Read parquet files from a specified directory and concatenate them into a single DataFrame.\n\n**[Map state names to abbreviations] (Lines 31-87):**\n- Define a dictionary to map full state names to their abbreviations.\n- Create a mapping for state names found in the data to their abbreviations.\n\n**[Clean and encode state feature] (Lines 88-104):**\n- Define a function to clean state names and apply it to the state column in the data.\n- Encode the cleaned state names using LabelEncoder and store the encoder mappings.\n\n**[Encode categorical features] (Lines 105-112):**\n- Encode several categorical features using LabelEncoder and store the encoder mappings.\n\n**[Scale numerical features] (Lines 113-134):**\n- Define a list of numerical features to be scaled.\n- Scale each numerical feature using StandardScaler and store the scaler parameters.\n\n**[Process sequence features] (Lines 135-153):**\n- Replace and fill missing values in sequence features.\n- Convert sequence features to padded sequences and tokenize them.\n- Rename and drop unnecessary columns.\n\n**[Write transformed data to parquet files] (Lines 154-166):**\n- Define a function to write DataFrame chunks to parquet files.\n- Split the data into chunks and write each chunk to a specified directory.\n\n**[Export feature transformers] (Lines 167-175):**\n- Save the trained Tokenizer, categorical feature encoders, and numerical feature scalers to files using pickle."
    },
    {
      "summary": "**[Authenticate user and set up DataProc cluster] (Lines 1-11):**\n- Imports the `aml.cloud_v1` module.\n- Authenticates the user.\n- Creates a DataProc client for a specific GCP project.\n- Defines a list of packages to install on the cluster.\n- Creates a DataProc cluster with specified configurations (name, image, packages, number of workers, memory, CPUs, and wait for completion).\n\n"
    },
    {
      "summary": "**[Import necessary libraries and configure GPU settings] (Lines 2-14):**\n- Import essential libraries for data processing, machine learning, and model training.\n- Configure TensorFlow to use GPU with memory growth enabled.\n\n**[Load configuration and model version] (Lines 15-34):**\n- Define a function to load a YAML configuration file.\n- Load the configuration file and model version.\n- Set up directories for saving model artifacts.\n\n**[Load and concatenate data] (Lines 36-43):**\n- List and read all parquet files from a specified directory.\n- Concatenate the data from these files into a single DataFrame.\n\n**[Load feature encoders and split data] (Lines 44-49):**\n- Load categorical feature encoders and tokenizer from pickle files.\n- Split the data into training and validation sets based on a 'split' column.\n\n**[Define feature names] (Lines 52-74):**\n- Define lists of numeric, categorical, and ID feature names to be used in the model.\n\n**[Function to prepare data for model input] (Lines 75-106):**\n- Define a function to create feature columns and prepare data for model input.\n- Include dense, sparse, and variable-length sparse features.\n\n**[Prepare training and validation data] (Lines 107-108):**\n- Use the defined function to prepare training and validation data.\n\n**[Define data generator function] (Lines 110-120):**\n- Define a generator function to yield batches of data for training.\n\n**[Create TensorFlow datasets] (Lines 121-137):**\n- Create TensorFlow datasets for training and validation using the data generator function.\n- Shuffle the datasets.\n\n**[Set up and compile the model] (Lines 140-155):**\n- Use TensorFlow's MirroredStrategy for distributed training.\n- Define and compile the DIN model with specified configurations.\n\n**[Train the model with early stopping] (Lines 156-170):**\n- Set up early stopping callback.\n- Train the model using the training dataset and validate using the validation dataset.\n\n**[Save the trained model] (Lines 171-175):**\n- Save the trained model in both H5 and TensorFlow SavedModel formats.\n\n**[Convert TensorFlow model to ONNX format] (Lines 176-186):**\n- Convert the TensorFlow model to ONNX format and save the ONNX specification.\n\n**[Prepare and save the production model] (Lines 196-227):**\n- Filter non-ASCII characters from categorical feature encoders.\n- Load numerical feature scalars.\n- Create a graph for the production model, including renaming and scaling nodes.\n- Save the production model.\n\n**[Prepare test data and make predictions] (Lines 230-248):**\n- Prepare test data for prediction.\n- Save test data to a JSON file.\n\n**[Convert and test ONNX model] (Lines 250-269):**\n- Convert the TensorFlow model to ONNX format.\n- Load the ONNX model and prepare test data for ONNX inference.\n- Compare predictions from TensorFlow and ONNX models to ensure consistency."
    },
    {
      "summary": "**[Set up environment and configuration] (Lines 2-24):**\n- Import necessary modules and libraries.\n- Set working directory and user-specific paths.\n- Load configuration settings and validate setup.\n- Load the current model version from a file.\n\n**[Define function for scoring out-of-time (OOT) data] (Lines 26-48):**\n- Define a function `oot_data_eval` to score OOT data using specified models.\n- Load OOT data from a specified path.\n- Copy model files from Google Cloud Storage (GCS) to a local temporary directory.\n- Initialize a model scorer and create a DataFrame with scores.\n- Optionally select specific columns to keep in the output.\n- Save the scored data to a specified path in parquet format.\n\n**[Prepare model paths and scoring parameters] (Lines 49-69):**\n- Define local and GCS paths for model files and OOT data.\n- Load model specifications and prepare lists of model paths and score outputs.\n\n**[Submit Spark job to GCP] (Lines 70-86):**\n- Initialize a GCP client for submitting Spark jobs.\n- Create and submit a Spark job to GCP to run the `oot_data_eval` function with specified parameters and configurations.\n- Retrieve and display the job ID.\n\n**[Monitor job status and save logs] (Lines 87-94):**\n- Check the status of the submitted Spark job and wait for its completion.\n- Save the job logs to a local file for future reference.\n\n**[Create external table in BigQuery] (Lines 100-104):**\n- Create or replace an external table in BigQuery to reference the scored OOT data stored in GCS."
    },
    {
      "summary": "**Load model version and create directories if not exist (Lines 4-12):**\n- Loads the current model version from a pickle file.\n- Constructs paths for model artifacts and evaluation readouts.\n- Creates the evaluation readout directory if it does not exist.\n\n**Load YAML configuration file (Lines 13-25):**\n- Defines a function to load a YAML file.\n- Loads the base configuration from a specified YAML file.\n- Extracts the BigQuery project dataset prefix from the configuration.\n\n**Create driver_oot_txn_365d table (Lines 26-44):**\n- Constructs a SQL query to create a table with transaction data joined with payment data.\n- The table includes transactions within a 365-day window.\n\n**Create driver_oot_txn_save_365d table (Lines 47-65):**\n- Constructs a SQL query to create a table with transaction data joined with save event data.\n- The table includes save events within a 365-day window.\n\n**Create mlv2_gpt_similar_map_snapshot table (Lines 68-137):**\n- Constructs a SQL query to create a table with similar merchant mappings.\n- Joins the mappings with live unique merchants.\n\n**Create mlv2_gpt_similar_map_snapshot_1 table (Lines 140-193):**\n- Constructs a SQL query to create a table with concatenated similar merchant lists.\n- Splits the concatenated list into individual similar merchant columns.\n\n**Create driver_oot_txn_save_365d_similar table (Lines 196-207):**\n- Constructs a SQL query to create a table with transaction and save data joined with similar merchants.\n- Includes a column indicating similarity based on purchase or save events.\n\n**Create driver_oot_txn_save_365d_similar_dedup table (Lines 209-246):**\n- Constructs a SQL query to create a deduplicated table of similar merchants.\n- Aggregates and deduplicates similar merchants for each customer and run date.\n\n**Create driver_oot_two_tower_similar_score table (Lines 249-336):**\n- Constructs a SQL query to create a table with similarity scores between customers and merchants.\n- Joins customer and merchant embeddings to calculate dot product similarity scores.\n\n**Create driver_oot_hueristic_model_comparison table (Lines 339-372):**\n- Constructs a SQL query to create a table comparing heuristic model scores.\n- Includes ranks based on past transactions, two-tower similarity, and transformed scores.\n\n**Calculate recall metrics for all models (Lines 375-430):**\n- Constructs a SQL query to calculate recall metrics for different models.\n- Groups results by model and calculates recall at various ranks.\n\n**Save and plot performance metrics (Lines 433-447):**\n- Saves the performance metrics to a CSV file.\n- Plots bar charts of the performance metrics using Seaborn and Matplotlib.\n\n**Calculate recall metrics for first-time users (Lines 448-513):**\n- Constructs a SQL query to calculate recall metrics for first-time users.\n- Groups results by model and calculates recall at various ranks for first-time users.\n\n**Save and plot performance metrics for first-time users (Lines 516-530):**\n- Saves the first-time user performance metrics to a CSV file.\n- Plots bar charts of the first-time user performance metrics using Seaborn and Matplotlib.\n\n**Calculate recall metrics excluding specific merchants (Lines 531-590):**\n- Constructs a SQL query to calculate recall metrics excluding specific merchants.\n- Groups results by model and calculates recall at various ranks excluding specified merchants.\n\n**Save and plot performance metrics excluding specific merchants (Lines 592-607):**\n- Saves the performance metrics excluding specific merchants to a CSV file.\n- Plots bar charts of the performance metrics excluding specific merchants using Seaborn and Matplotlib.\n\n**Calculate recall metrics for recent data (Lines 608-728):**\n- Constructs a SQL query to calculate recall metrics for recent data.\n- Groups results by model and calculates recall at various ranks for recent data.\n\n**Save and plot performance metrics for recent data (Lines 730-731):**\n- Saves the recent data performance metrics to a CSV file.\n- Plots bar charts of the recent data performance metrics using Seaborn and Matplotlib."
    }
  ],
  "all_nodes": [],
  "consolidated_nodes": [],
  "edges": [],
  "dag_yaml": "",
  "verified_dag": {},
  "config": {},
  "notebooks": []
}