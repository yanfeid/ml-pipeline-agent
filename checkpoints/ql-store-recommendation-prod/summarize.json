{
  "github_url": "https://github.paypal.com/GADS-Consumer-ML/ql-store-recommendation-prod.git",
  "input_files": [
    "research/pipeline/00_driver.ipynb",
    "research/pipeline/01_bq_feat.ipynb",
    "research/pipeline/01_varmart_feat.ipynb",
    "research/pipeline/02_combine.ipynb",
    "research/pipeline/03_prepare_training_data.ipynb",
    "research/pipeline/04_training.ipynb",
    "research/pipeline/05_scoring_oot.ipynb",
    "research/pipeline/06_evaluation.ipynb"
  ],
  "repo_name": "ql-store-recommendation-prod",
  "local_repo_path": "repos/ql-store-recommendation-prod",
  "existing_config_path": null,
  "files": [
    "repos/ql-store-recommendation-prod/research/pipeline/00_driver.py",
    "repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.py",
    "repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.py",
    "repos/ql-store-recommendation-prod/research/pipeline/02_combine.py",
    "repos/ql-store-recommendation-prod/research/pipeline/03_prepare_training_data.py",
    "repos/ql-store-recommendation-prod/research/pipeline/04_training.py",
    "repos/ql-store-recommendation-prod/research/pipeline/05_scoring_oot.py",
    "repos/ql-store-recommendation-prod/research/pipeline/06_evaluation.py"
  ],
  "summaries": [
    {
      "summary": "**Load YAML configuration file (Lines 4-13):**\n- Defines a function to load a YAML file.\n- Loads the configuration from a specified file path.\n\n**Extract BigQuery project dataset prefix from configuration (Lines 14-16):**\n- Retrieves the BigQuery project dataset prefix from the loaded configuration.\n\n**Create table for eligible merchants (Lines 18-52):**\n- Drops and creates a table for live unique merchants with specific attributes.\n- Joins multiple tables to gather necessary merchant information.\n\n**Create initial driver table (Lines 54-83):**\n- Drops and creates a table with customer and merchant interaction data.\n- Filters data based on specific conditions like event date and customer ID.\n\n**Filter driver table based on placement count (Lines 84-102):**\n- Drops and creates a table to filter customers based on the count of distinct placements.\n- Ensures customers with more than one placement are included, and those with one placement are included only if they have specific interactions.\n\n**Create transaction data table (Lines 105-120):**\n- Drops and creates a table with transaction data for customers and merchants.\n- Filters transactions based on specific conditions like transaction status and date.\n\n**Create positive samples for training (Lines 124-168):**\n- Drops and creates a table for positive training samples attributed to transactions.\n- Applies sampling ratios and splits data into training and validation sets.\n\n**Create organic positive samples (Lines 170-204):**\n- Drops and creates a table for organic positive training samples.\n- Filters out highly active users and user-merchant pairs to avoid bias.\n\n**Combine positive samples (Lines 207-277):**\n- Drops and creates a table combining attributed and organic positive samples.\n- Includes both training and out-of-time (OOT) samples.\n\n**Create negative samples (Lines 281-307):**\n- Drops and creates a table for negative samples.\n- Identifies hard and uniform negative samples based on specific conditions.\n\n**Add day positive count to training split (Lines 310-321):**\n- Drops and creates a table adding the count of positive samples per day for each customer.\n\n**Generate hard negative samples (Lines 324-336):**\n- Drops and creates a table for hard negative samples.\n- Joins with the driver table to filter out specific interactions.\n\n**Downsample hard negative samples (Lines 339-349):**\n- Drops and creates a table to downsample hard negative samples.\n- Ensures the number of samples per customer and day is limited.\n\n**Generate uniform negative samples (Lines 352-364):**\n- Drops and creates a table for uniform negative samples.\n- Ensures negative samples do not overlap with positive samples.\n\n**Remove window positive samples from uniform negatives (Lines 367-376):**\n- Drops and creates a table to remove uniform negative samples that overlap with positive samples within a specific time window.\n\n**Downsample uniform negative samples (Lines 379-407):**\n- Drops and creates a table to downsample uniform negative samples.\n- Applies a sampling probability based on merchant activity.\n\n**Create development dataset (Lines 410-445):**\n- Drops and creates a table combining positive and negative samples for model development.\n- Includes both hard and uniform negative samples.\n\n**Generate OOT uniform negative samples (Lines 449-469):**\n- Drops and creates a table for OOT uniform negative samples.\n- Ensures each customer and day combination has a unique negative sample.\n\n**Filter OOT uniform negative samples (Lines 472-478):**\n- Drops and creates a table to filter out duplicate OOT uniform negative samples.\n\n**Create OOT dataset (Lines 481-495):**\n- Drops and creates a table combining positive and negative samples for OOT evaluation.\n\n**Create simulation dataset (Lines 498-506):**\n- Drops and creates a table combining development and OOT datasets for simulation.\n\n**Create simulation consumer dataset (Lines 509-512):**\n- Drops and creates a table with distinct customer and run date combinations for simulation."
    },
    {
      "summary": "**Load YAML configuration file (Lines 4-13):**\n- Defines a function to load a YAML file.\n- Loads the configuration from a specified file path.\n\n**Extract BigQuery project dataset prefix from configuration (Lines 14-16):**\n- Retrieves the BigQuery project dataset prefix from the loaded configuration.\n\n**Create driver_simu_txn_365d table (Lines 17-40):**\n- Drops the existing table if it exists.\n- Creates a new table with transaction data joined with payment data, including various date intervals.\n\n**Create driver_simu_txn_365d_agg table (Lines 43-55):**\n- Drops the existing table if it exists.\n- Aggregates transaction data over different time intervals (7, 30, 180, 365 days).\n\n**Create driver_consumer_base table (Lines 58-66):**\n- Drops the existing table if it exists.\n- Creates a base table with customer IDs and run dates, including various date intervals.\n\n**Create driver_consumer_base_txn_5k_merch_category table (Lines 69-93):**\n- Drops the existing table if it exists.\n- Joins consumer base data with transaction data and merchant category data, including recency ranking.\n\n**Create driver_consumer_base_last_10_txn table (Lines 96-175):**\n- Drops the existing table if it exists.\n- Aggregates the last 10 transactions for each customer, calculating average transaction amounts.\n\n**Create driver_consumer_base_all_history_array_0 table (Lines 178-185):**\n- Drops the existing table if it exists.\n- Selects the most recent 100 transactions for each customer.\n\n**Create driver_consumer_base_all_history_array table (Lines 188-194):**\n- Drops the existing table if it exists.\n- Aggregates the most recent 100 merchants and their categories for each customer.\n\n**Create driver_combine_category table (Lines 198-203):**\n- Drops the existing table if it exists.\n- Combines driver simulation data with merchant category data.\n\n**Create driver_combine_category_agg_0 table (Lines 206-219):**\n- Drops the existing table if it exists.\n- Aggregates transaction data by category over different time intervals (30, 180, 365 days).\n\n**Create driver_combine_category_agg_1 table (Lines 222-231):**\n- Drops the existing table if it exists.\n- Aggregates total transaction numbers over different time intervals (30, 180, 365 days).\n\n**Create driver_combine_category_agg_2 table (Lines 234-257):**\n- Drops the existing table if it exists.\n- Combines category-specific and overall transaction data, calculating average transaction amounts.\n\n**Create driver_combine_category_agg_3 table (Lines 260-286):**\n- Drops the existing table if it exists.\n- Combines customer and receiver category data, including various transaction metrics.\n\n**Create driver_combine_category_agg_4 table (Lines 289-302):**\n- Drops the existing table if it exists.\n- Ranks categories by transaction frequency over different time intervals (30, 180, 365 days).\n\n**Create driver_combine_category_agg_5 table (Lines 305-464):**\n- Drops the existing table if it exists.\n- Aggregates the top 3 frequent merchant categories over different time intervals (30, 180, 365 days).\n\n**Create driver_merchant_base table (Lines 468-476):**\n- Drops the existing table if it exists.\n- Creates a base table with receiver IDs and run dates, including a 30-day interval.\n\n**Create driver_merchant_base_txn_30d table (Lines 479-506):**\n- Drops the existing table if it exists.\n- Joins merchant base data with transaction data within a 30-day interval.\n\n**Create driver_merchant_base_txn_30d_filter_sndr table (Lines 509-523):**\n- Drops the existing table if it exists.\n- Filters transactions to include only specific customer account types.\n\n**Create driver_merchant_base_price_agg table (Lines 526-551):**\n- Drops the existing table if it exists.\n- Aggregates transaction prices, calculating average and percentile values over 30 days.\n\n**Create driver_merchant_base_sales_agg table (Lines 554-566):**\n- Drops the existing table if it exists.\n- Aggregates sales data, including transaction counts and amounts over 30 days.\n\n**Create driver_elig_save_365d_category table (Lines 569-616):**\n- Drops the existing table if it exists.\n- Joins consumer base data with save event data and merchant category data, including recency ranking.\n\n**Create driver_elig_save_agg_00 table (Lines 619-630):**\n- Drops the existing table if it exists.\n- Aggregates save event counts over different time intervals (7, 30, 180 days) for each customer and receiver.\n\n**Create driver_elig_save_agg_0 table (Lines 633-643):**\n- Drops the existing table if it exists.\n- Aggregates save event counts over different time intervals (7, 30, 180 days) for each customer.\n\n**Create driver_elig_save_agg_1 table (Lines 646-717):**\n- Drops the existing table if it exists.\n- Aggregates the last 5 save events for each customer.\n\n**Create driver_elig_save_agg_2 table (Lines 720-731):**\n- Drops the existing table if it exists.\n- Aggregates save event counts by category over different time intervals (7, 30, 180 days).\n\n**Create driver_elig_save_agg_3 table (Lines 734-751):**\n- Drops the existing table if it exists.\n- Combines customer and receiver category save event data, including various save metrics.\n\n**Create driver_merchant_base_click_save table (Lines 754-793):**\n- Drops the existing table if it exists.\n- Aggregates save event counts for merchants, including various placements and engagement segments over 30 days.\n\n**Create driver_consumer_base_gender table (Lines 796-817):**\n- Drops the existing table if it exists.\n- Joins consumer base data with gender prediction data to determine the gender of each customer."
    },
    {
      "summary": "**User authentication and environment setup (Lines 4-9):**\n- Imports necessary modules for cloud operations.\n- Authenticates the user.\n- Sets an environment variable to disable a specific development feature.\n\n**Export data to Google Cloud Storage (Lines 13-22):**\n- Exports data from a BigQuery table to Google Cloud Storage in Parquet format.\n\n**Import modules and initialize variables (Lines 23-34):**\n- Imports additional necessary modules.\n- Initializes variables for the current date, user, production status, stage, sequence number, and job name.\n\n**Configure and initialize Fetcher object (Lines 34-66):**\n- Creates and configures a Fetcher object with various settings including job name, group name, model name, owner, description, and manager.\n- Sets GCP-specific configurations such as project ID, bucket name, and data locations.\n- Specifies the variables to be fetched and the data split ratio.\n\n**Run Fetcher and create external table (Lines 67-73):**\n- Executes the Fetcher to fetch the data.\n- Creates or replaces an external table in BigQuery using the fetched data stored in Google Cloud Storage."
    },
    {
      "summary": "**Load YAML configuration file** (Lines 4-13):\n- Defines a function to load a YAML file.\n- Attempts to load a configuration file and assigns its content to a variable.\n\n**Extract BigQuery project dataset prefix** (Lines 14-16):\n- Extracts the BigQuery project dataset prefix from the loaded configuration.\n\n**Create driver_dev_features table** (Lines 17-216):\n- Constructs a SQL query to drop and create a new table `driver_dev_features`.\n- Selects various features from multiple joined tables, using COALESCE to handle null values.\n\n**Create driver_oot_features table** (Lines 219-418):\n- Constructs a SQL query to drop and create a new table `driver_oot_features`.\n- Similar to the previous block, selects features from multiple joined tables with COALESCE.\n\n**Expand sequence features in driver_oot_features_expand_seq table** (Lines 421-625):\n- Constructs a SQL query to drop and create a new table `driver_oot_features_expand_seq`.\n- Splits a comma-separated list into multiple columns for historical receiver IDs and categories.\n\n**Export data to Google Cloud Storage** (Lines 630-639):\n- Exports the `ql_store_rmr_driver_dev_features` table to Google Cloud Storage in Parquet format."
    },
    {
      "summary": "**[Import necessary libraries and set up paths] (Lines 1-20):**\n- Import various libraries for data manipulation, preprocessing, and model saving.\n- Set up paths and directories for saving model artifacts and feature transformers.\n\n**[Load and concatenate data] (Lines 21-30):**\n- Load parquet files from a specified directory.\n- Concatenate these files into a single DataFrame.\n\n**[Map state names to abbreviations] (Lines 31-94):**\n- Create a dictionary to map full state names to their abbreviations.\n- Define a function to clean state names in the data and apply this function to the relevant column.\n\n**[Encode categorical features] (Lines 95-112):**\n- Encode the cleaned state names using LabelEncoder.\n- Encode other specified categorical features and store the encoders.\n\n**[Scale numerical features] (Lines 113-134):**\n- Define a list of numerical features to be scaled.\n- Scale these features using StandardScaler and store the scalers.\n\n**[Process sequence features] (Lines 135-153):**\n- Replace and fill missing values in sequence-related columns.\n- Tokenize and pad sequences for these columns.\n- Drop the original sequence columns after processing.\n\n**[Save processed data in chunks] (Lines 154-166):**\n- Define a function to write DataFrame chunks to parquet files.\n- Split the data into chunks and save each chunk to a specified directory.\n\n**[Save feature transformers] (Lines 167-175):**\n- Save the tokenizer, categorical feature encoders, and numerical feature scalars to disk using pickle."
    },
    {
      "summary": "**[Import necessary libraries and modules] (Lines 2-11):**\n- Import various libraries and modules required for data processing, model building, and training.\n\n**[Configure GPU settings] (Lines 12-14):**\n- List available GPUs and set memory growth to avoid memory allocation issues.\n\n**[Load YAML configuration file] (Lines 15-21):**\n- Define a function to load a YAML file and handle file not found exceptions.\n\n**[Load configuration and model version] (Lines 22-34):**\n- Load the YAML configuration file and model version, create necessary directories for saving models and artifacts.\n\n**[Load and concatenate data] (Lines 36-43):**\n- Load multiple parquet files from a directory, concatenate them into a single DataFrame.\n\n**[Load feature encoders and tokenizers] (Lines 44-47):**\n- Load pre-trained categorical feature encoders and tokenizers from pickle files.\n\n**[Split data into training and validation sets] (Lines 48-49):**\n- Split the concatenated data into training and validation sets based on a 'split' column.\n\n**[Define feature names] (Lines 52-74):**\n- Define lists of numeric, categorical, and ID feature names to be used in the model.\n\n**[Function to prepare data for model input] (Lines 75-106):**\n- Define a function to prepare feature columns and data for model input, including handling sparse and dense features.\n\n**[Prepare training and validation data] (Lines 107-109):**\n- Prepare training and validation data using the defined function, extracting features and labels.\n\n**[Data generator function] (Lines 110-120):**\n- Define a generator function to yield batches of data for training and validation.\n\n**[Create TensorFlow datasets] (Lines 121-137):**\n- Create TensorFlow datasets for training and validation using the data generator function.\n\n**[Define and compile the model] (Lines 140-155):**\n- Define and compile the DIN model within a distributed strategy scope, using parameters from the configuration file.\n\n**[Early stopping callback] (Lines 156-162):**\n- Define an early stopping callback to monitor validation loss and stop training early if no improvement is seen.\n\n**[Train the model] (Lines 163-169):**\n- Train the model using the training and validation datasets, with early stopping.\n\n**[Save the trained model] (Lines 171-175):**\n- Save the trained model in both H5 and TensorFlow SavedModel formats.\n\n**[Convert TensorFlow model to ONNX] (Lines 176-185):**\n- Convert the trained TensorFlow model to ONNX format and save the ONNX specification.\n\n**[Prepare for out-of-time (OOT) scoring] (Lines 188-229):**\n- Prepare directories and files for OOT scoring, including renaming and moving files, and saving the model graph.\n\n**[Generate test data for prediction] (Lines 230-248):**\n- Generate test data for prediction, save it to a JSON file for later use.\n\n**[Convert and test ONNX model] (Lines 250-269):**\n- Convert the TensorFlow model to ONNX format, run predictions using ONNX runtime, and compare results with TensorFlow predictions to ensure consistency."
    },
    {
      "summary": "**[Setup and configuration] (Lines 2-24):**\n- Imports necessary modules and libraries.\n- Sets the working directory and user environment.\n- Loads configuration parameters and model version information.\n\n**[Define scoring function] (Lines 26-48):**\n- Defines a function `oot_data_eval` to evaluate out-of-time (OOT) data.\n- Loads OOT data and model specifications.\n- Scores the OOT data using the loaded models.\n- Saves the scored data to a specified path.\n\n**[Prepare model paths and scoring parameters] (Lines 49-69):**\n- Sets local and GCP paths for model files and data.\n- Loads model specifications and prepares a list of model paths and scoring outputs.\n\n**[Submit Spark job to GCP] (Lines 70-86):**\n- Configures and submits a Spark job to Google Cloud Platform (GCP) to run the `oot_data_eval` function.\n- Specifies necessary packages and parameters for the Spark job.\n\n**[Check job status and save logs] (Lines 87-94):**\n- Monitors the status of the submitted Spark job.\n- Saves the job logs to a specified file.\n\n**[Create external table in BigQuery] (Lines 100-104):**\n- Creates or replaces an external table in BigQuery using the scored data stored in GCP."
    },
    {
      "summary": "**[Load model version and create directories] (Lines 4-12):**\n- Imports necessary libraries.\n- Loads the current model version from a file.\n- Constructs paths for model artifacts and evaluation readouts.\n- Creates the evaluation readout directory if it does not exist.\n\n**[Load configuration file] (Lines 13-25):**\n- Defines a function to load a YAML configuration file.\n- Loads the configuration file and extracts the BigQuery project dataset prefix.\n\n**[Create driver_oot_txn_365d table] (Lines 26-44):**\n- Constructs a SQL query to create a table `driver_oot_txn_365d` by joining `driver_oot` with transaction data from the past 365 days.\n\n**[Create driver_oot_txn_save_365d table] (Lines 47-65):**\n- Constructs a SQL query to create a table `driver_oot_txn_save_365d` by joining `driver_oot_txn_365d` with offer save events from the past 365 days.\n\n**[Create mlv2_gpt_similar_map_snapshot table] (Lines 68-137):**\n- Constructs a SQL query to create a snapshot table of similar merchants based on the latest data from a specific source table.\n\n**[Create mlv2_gpt_similar_map_snapshot_1 table] (Lines 140-193):**\n- Constructs a SQL query to create a table with concatenated similar merchant IDs, splitting them into individual columns.\n\n**[Create driver_oot_txn_save_365d_similar table] (Lines 196-207):**\n- Constructs a SQL query to create a table `driver_oot_txn_save_365d_similar` by joining `driver_oot_txn_save_365d` with the similar merchants snapshot.\n\n**[Create driver_oot_txn_save_365d_similar_dedup table] (Lines 209-247):**\n- Constructs a SQL query to create a deduplicated table of similar merchants for each customer and run date.\n\n**[Create driver_oot_two_tower_similar_score table] (Lines 249-336):**\n- Constructs a SQL query to calculate similarity scores between customers and merchants using embeddings from two towers.\n\n**[Create driver_oot_hueristic_model_comparison table] (Lines 339-372):**\n- Constructs a SQL query to compare heuristic model scores with other model scores, ranking them for each customer and run date.\n\n**[Calculate recall metrics for all models] (Lines 375-430):**\n- Constructs a SQL query to calculate recall metrics at different ranks for various models, grouping by model type.\n\n**[Save and plot performance metrics for all models] (Lines 433-447):**\n- Saves the performance metrics to a CSV file.\n- Plots the performance metrics using seaborn and matplotlib, saving the plot as an image.\n\n**[Calculate recall metrics for first-time users] (Lines 448-513):**\n- Constructs a SQL query to calculate recall metrics for first-time users, grouping by model type.\n\n**[Save and plot performance metrics for first-time users] (Lines 516-530):**\n- Saves the performance metrics for first-time users to a CSV file.\n- Plots the performance metrics using seaborn and matplotlib, saving the plot as an image.\n\n**[Calculate recall metrics excluding specific merchants] (Lines 532-590):**\n- Constructs a SQL query to calculate recall metrics excluding specific merchants, grouping by model type.\n\n**[Save and plot performance metrics excluding specific merchants] (Lines 592-607):**\n- Saves the performance metrics excluding specific merchants to a CSV file.\n- Plots the performance metrics using seaborn and matplotlib, saving the plot as an image.\n\n**[Calculate recall metrics for recent data] (Lines 608-728):**\n- Constructs a SQL query to calculate recall metrics for recent data (last 7 days), grouping by model type.\n\n**[Save and plot performance metrics for recent data] (Lines 730-731):**\n- (Commented out) Intended to save the performance metrics for recent data to a CSV file."
    }
  ],
  "component_identification": [],
  "component_parsing": [],
  "verified_components": [],
  "attribute_identification": [],
  "attribute_parsing": [],
  "node_aggregator": [],
  "edges": [],
  "dag_yaml": "",
  "verified_dag": {},
  "config": {},
  "notebooks": []
}