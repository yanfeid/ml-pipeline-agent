{
  "github_url": "https://github.paypal.com/GADS-Consumer-ML/ql-store-recommendation-prod.git",
  "input_files": [
    "research/pipeline/00_driver.ipynb",
    "research/pipeline/01_bq_feat.ipynb",
    "research/pipeline/01_varmart_feat.ipynb",
    "research/pipeline/02_combine.ipynb",
    "research/pipeline/03_prepare_training_data.ipynb",
    "research/pipeline/04_training.ipynb",
    "research/pipeline/05_scoring_oot.ipynb",
    "research/pipeline/06_evaluation.ipynb"
  ],
  "repo_name": "ql-store-recommendation-prod",
  "local_repo_path": "repos/ql-store-recommendation-prod",
  "existing_config_path": null,
  "files": [
    "repos/ql-store-recommendation-prod/research/pipeline/00_driver.py",
    "repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.py",
    "repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.py",
    "repos/ql-store-recommendation-prod/research/pipeline/02_combine.py",
    "repos/ql-store-recommendation-prod/research/pipeline/03_prepare_training_data.py",
    "repos/ql-store-recommendation-prod/research/pipeline/04_training.py",
    "repos/ql-store-recommendation-prod/research/pipeline/05_scoring_oot.py",
    "repos/ql-store-recommendation-prod/research/pipeline/06_evaluation.py"
  ],
  "summaries": [
    {
      "summary": "**Load YAML configuration file (Lines 4-13):**\n- Defines a function to load a YAML file.\n- Loads the configuration from a specified file path.\n\n**Extract BigQuery project dataset prefix from configuration (Lines 14-16):**\n- Retrieves the BigQuery project dataset prefix from the loaded configuration.\n\n**Create table for eligible merchants (Lines 18-52):**\n- Drops and creates a table for live unique merchants with specific attributes.\n- Joins multiple tables to gather necessary merchant information.\n\n**Create initial driver table (Lines 54-83):**\n- Drops and creates a table with customer and merchant interaction data.\n- Filters data based on specific conditions like event date and customer ID.\n\n**Filter driver table based on placement count (Lines 84-102):**\n- Drops and creates a table to filter customers based on the count of distinct placements.\n- Ensures customers with more than one placement are included, and those with one placement are included only if they have specific interactions.\n\n**Create transaction data table (Lines 105-120):**\n- Drops and creates a table with transaction data for customers and merchants.\n- Filters transactions based on specific conditions like transaction status and date.\n\n**Create positive samples for training (Lines 124-168):**\n- Drops and creates a table for positive training samples attributed to transactions.\n- Applies sampling ratios and splits data into training and validation sets.\n\n**Create organic positive samples (Lines 170-204):**\n- Drops and creates a table for organic positive training samples.\n- Filters out highly active users and user-merchant pairs to avoid bias.\n\n**Combine positive samples (Lines 207-277):**\n- Drops and creates a table combining attributed and organic positive samples.\n- Includes both training and out-of-time (OOT) samples.\n\n**Create negative samples (Lines 281-307):**\n- Drops and creates a table for negative samples.\n- Identifies hard and uniform negative samples based on specific conditions.\n\n**Add day positive count to training split (Lines 310-321):**\n- Drops and creates a table adding the count of positive samples per day for each customer.\n\n**Generate hard negative samples (Lines 324-336):**\n- Drops and creates a table for hard negative samples.\n- Joins with the driver table to filter out specific interactions.\n\n**Downsample hard negative samples (Lines 339-349):**\n- Drops and creates a table to downsample hard negative samples.\n- Ensures the number of samples per customer and day is limited.\n\n**Generate uniform negative samples (Lines 352-364):**\n- Drops and creates a table for uniform negative samples.\n- Ensures negative samples do not overlap with positive samples.\n\n**Remove window positive samples from uniform negatives (Lines 367-376):**\n- Drops and creates a table to remove uniform negative samples that overlap with positive samples within a specific time window.\n\n**Downsample uniform negative samples (Lines 379-407):**\n- Drops and creates a table to downsample uniform negative samples.\n- Applies a sampling probability based on merchant activity.\n\n**Create development dataset (Lines 410-445):**\n- Drops and creates a table combining positive and negative samples for model development.\n- Includes both hard and uniform negative samples.\n\n**Generate OOT uniform negative samples (Lines 449-469):**\n- Drops and creates a table for OOT uniform negative samples.\n- Ensures each customer and day combination has a unique negative sample.\n\n**Filter OOT uniform negative samples (Lines 472-478):**\n- Drops and creates a table to filter out duplicate OOT uniform negative samples.\n\n**Create OOT dataset (Lines 481-495):**\n- Drops and creates a table combining positive and negative samples for OOT evaluation.\n\n**Create simulation dataset (Lines 498-506):**\n- Drops and creates a table combining development and OOT datasets for simulation.\n\n**Create simulation consumer dataset (Lines 509-512):**\n- Drops and creates a table with distinct customer and run date combinations for simulation."
    },
    {
      "summary": "**Load YAML configuration file (Lines 4-13):**\n- Defines a function to load a YAML file.\n- Loads the configuration from a specified file path.\n\n**Extract BigQuery project dataset prefix from configuration (Lines 14-16):**\n- Retrieves the BigQuery project dataset prefix from the loaded configuration.\n\n**Create driver_simu_txn_365d table (Lines 17-40):**\n- Drops the existing table if it exists.\n- Creates a new table with transaction data joined with payment data, including various date intervals.\n\n**Create driver_simu_txn_365d_agg table (Lines 43-55):**\n- Drops the existing table if it exists.\n- Aggregates transaction data over different time intervals (7, 30, 180, 365 days).\n\n**Create driver_consumer_base table (Lines 58-66):**\n- Drops the existing table if it exists.\n- Creates a base table with customer IDs and run dates, including various date intervals.\n\n**Create driver_consumer_base_txn_5k_merch_category table (Lines 69-93):**\n- Drops the existing table if it exists.\n- Joins consumer base data with transaction data and merchant category data, including recency ranking.\n\n**Create driver_consumer_base_last_10_txn table (Lines 96-175):**\n- Drops the existing table if it exists.\n- Aggregates the last 10 transactions for each customer, calculating average transaction amounts.\n\n**Create driver_consumer_base_all_history_array_0 table (Lines 178-185):**\n- Drops the existing table if it exists.\n- Selects the most recent 100 transactions for each customer.\n\n**Create driver_consumer_base_all_history_array table (Lines 188-194):**\n- Drops the existing table if it exists.\n- Aggregates the most recent 100 merchants and their categories for each customer.\n\n**Create driver_combine_category table (Lines 198-203):**\n- Drops the existing table if it exists.\n- Combines driver simulation data with merchant category data.\n\n**Create driver_combine_category_agg_0 table (Lines 206-219):**\n- Drops the existing table if it exists.\n- Aggregates transaction data by category over different time intervals (30, 180, 365 days).\n\n**Create driver_combine_category_agg_1 table (Lines 222-231):**\n- Drops the existing table if it exists.\n- Aggregates total transaction numbers over different time intervals (30, 180, 365 days).\n\n**Create driver_combine_category_agg_2 table (Lines 234-257):**\n- Drops the existing table if it exists.\n- Combines category-specific and overall transaction data, calculating average transaction amounts.\n\n**Create driver_combine_category_agg_3 table (Lines 260-286):**\n- Drops the existing table if it exists.\n- Combines customer and receiver category data, including various transaction metrics.\n\n**Create driver_combine_category_agg_4 table (Lines 289-302):**\n- Drops the existing table if it exists.\n- Ranks categories by transaction frequency over different time intervals (30, 180, 365 days).\n\n**Create driver_combine_category_agg_5 table (Lines 305-464):**\n- Drops the existing table if it exists.\n- Aggregates the top 3 frequent merchant categories over different time intervals (30, 180, 365 days).\n\n**Create driver_merchant_base table (Lines 468-476):**\n- Drops the existing table if it exists.\n- Creates a base table with receiver IDs and run dates, including a 30-day interval.\n\n**Create driver_merchant_base_txn_30d table (Lines 479-506):**\n- Drops the existing table if it exists.\n- Joins merchant base data with transaction data within a 30-day interval.\n\n**Create driver_merchant_base_txn_30d_filter_sndr table (Lines 509-523):**\n- Drops the existing table if it exists.\n- Filters transactions to include only specific customer account types.\n\n**Create driver_merchant_base_price_agg table (Lines 526-551):**\n- Drops the existing table if it exists.\n- Aggregates transaction prices, calculating average and percentile values over 30 days.\n\n**Create driver_merchant_base_sales_agg table (Lines 554-566):**\n- Drops the existing table if it exists.\n- Aggregates sales data, including transaction counts and amounts over 30 days.\n\n**Create driver_elig_save_365d_category table (Lines 569-616):**\n- Drops the existing table if it exists.\n- Joins consumer base data with save event data and merchant category data, including recency ranking.\n\n**Create driver_elig_save_agg_00 table (Lines 619-630):**\n- Drops the existing table if it exists.\n- Aggregates save event counts over different time intervals (7, 30, 180 days) for each customer and receiver.\n\n**Create driver_elig_save_agg_0 table (Lines 633-643):**\n- Drops the existing table if it exists.\n- Aggregates save event counts over different time intervals (7, 30, 180 days) for each customer.\n\n**Create driver_elig_save_agg_1 table (Lines 646-717):**\n- Drops the existing table if it exists.\n- Aggregates the last 5 save events for each customer.\n\n**Create driver_elig_save_agg_2 table (Lines 720-731):**\n- Drops the existing table if it exists.\n- Aggregates save event counts by category over different time intervals (7, 30, 180 days).\n\n**Create driver_elig_save_agg_3 table (Lines 734-751):**\n- Drops the existing table if it exists.\n- Combines customer and receiver category save event data, including various save metrics.\n\n**Create driver_merchant_base_click_save table (Lines 754-793):**\n- Drops the existing table if it exists.\n- Aggregates save event counts for merchants, including various placements and engagement segments over 30 days.\n\n**Create driver_consumer_base_gender table (Lines 796-817):**\n- Drops the existing table if it exists.\n- Joins consumer base data with gender prediction data to determine the gender of each customer."
    },
    {
      "summary": "**User authentication and environment setup (Lines 4-9):**\n- Imports necessary modules for cloud operations.\n- Authenticates the user.\n- Sets an environment variable to disable a specific development feature.\n\n**Export data to Google Cloud Storage (Lines 13-22):**\n- Exports data from a BigQuery table to Google Cloud Storage in Parquet format.\n\n**Import modules and initialize variables (Lines 23-34):**\n- Imports additional necessary modules.\n- Initializes variables for the current date, user, production status, stage, sequence number, and job name.\n\n**Configure and initialize Fetcher object (Lines 34-66):**\n- Creates and configures a Fetcher object with various settings including job name, group name, model name, owner, description, and manager.\n- Sets GCP-specific configurations such as project ID, bucket name, and data locations.\n- Specifies the variables to be fetched and the data split ratio.\n\n**Run Fetcher and create external table (Lines 67-73):**\n- Executes the Fetcher to fetch the data.\n- Creates or replaces an external table in BigQuery using the fetched data stored in Google Cloud Storage."
    },
    {
      "summary": "**Load YAML configuration file** (Lines 4-13):\n- Defines a function to load a YAML file.\n- Attempts to load a configuration file and assigns its content to a variable.\n\n**Extract BigQuery project dataset prefix** (Lines 14-16):\n- Extracts the BigQuery project dataset prefix from the loaded configuration.\n\n**Create driver_dev_features table** (Lines 17-216):\n- Constructs a SQL query to drop and create a new table `driver_dev_features`.\n- Selects various features from multiple joined tables, using COALESCE to handle null values.\n\n**Create driver_oot_features table** (Lines 219-418):\n- Constructs a SQL query to drop and create a new table `driver_oot_features`.\n- Similar to the previous block, selects features from multiple joined tables with COALESCE.\n\n**Expand sequence features in driver_oot_features_expand_seq table** (Lines 421-625):\n- Constructs a SQL query to drop and create a new table `driver_oot_features_expand_seq`.\n- Splits a comma-separated list into multiple columns for historical receiver IDs and categories.\n\n**Export data to Google Cloud Storage** (Lines 630-639):\n- Exports the `ql_store_rmr_driver_dev_features` table to Google Cloud Storage in Parquet format."
    },
    {
      "summary": "**[Import necessary libraries and set up paths] (Lines 1-20):**\n- Import various libraries for data manipulation, preprocessing, and model saving.\n- Set up paths and directories for saving model artifacts and feature transformers.\n\n**[Load and concatenate data] (Lines 21-30):**\n- Load parquet files from a specified directory.\n- Concatenate these files into a single DataFrame.\n\n**[Map state names to abbreviations] (Lines 31-94):**\n- Create a dictionary to map full state names to their abbreviations.\n- Define a function to clean state names in the data and apply this function to the relevant column.\n\n**[Encode categorical features] (Lines 95-112):**\n- Encode the cleaned state names using LabelEncoder.\n- Encode other specified categorical features and store the encoders.\n\n**[Scale numerical features] (Lines 113-134):**\n- Define a list of numerical features to be scaled.\n- Scale these features using StandardScaler and store the scalers.\n\n**[Process sequence features] (Lines 135-153):**\n- Replace and fill missing values in sequence-related columns.\n- Tokenize and pad sequences for these columns.\n- Drop the original sequence columns after processing.\n\n**[Save processed data in chunks] (Lines 154-166):**\n- Define a function to write DataFrame chunks to parquet files.\n- Split the data into chunks and save each chunk to a specified directory.\n\n**[Save feature transformers] (Lines 167-175):**\n- Save the tokenizer, categorical feature encoders, and numerical feature scalars to disk using pickle."
    },
    {
      "summary": "**[Import necessary libraries and modules] (Lines 2-11):**\n- Import various libraries and modules required for data processing, model building, and training.\n\n**[Configure GPU settings] (Lines 12-14):**\n- List available GPUs and set memory growth to avoid memory allocation issues.\n\n**[Load YAML configuration file] (Lines 15-21):**\n- Define a function to load a YAML file and handle file not found exceptions.\n\n**[Load configuration and model version] (Lines 22-34):**\n- Load the YAML configuration file and model version, create necessary directories for saving models and artifacts.\n\n**[Load and concatenate data] (Lines 36-43):**\n- Load multiple parquet files from a directory, concatenate them into a single DataFrame.\n\n**[Load feature encoders and tokenizers] (Lines 44-47):**\n- Load pre-trained categorical feature encoders and tokenizers from pickle files.\n\n**[Split data into training and validation sets] (Lines 48-49):**\n- Split the concatenated data into training and validation sets based on a 'split' column.\n\n**[Define feature names] (Lines 52-74):**\n- Define lists of numeric, categorical, and ID feature names to be used in the model.\n\n**[Function to prepare data for model input] (Lines 75-106):**\n- Define a function to prepare feature columns and data for model input, including handling sparse and dense features.\n\n**[Prepare training and validation data] (Lines 107-109):**\n- Prepare training and validation data using the defined function, extracting features and labels.\n\n**[Data generator function] (Lines 110-120):**\n- Define a generator function to yield batches of data for training and validation.\n\n**[Create TensorFlow datasets] (Lines 121-137):**\n- Create TensorFlow datasets for training and validation using the data generator function.\n\n**[Define and compile the model] (Lines 140-155):**\n- Define and compile the DIN model within a distributed strategy scope, using parameters from the configuration file.\n\n**[Early stopping callback] (Lines 156-162):**\n- Define an early stopping callback to monitor validation loss and stop training early if no improvement is seen.\n\n**[Train the model] (Lines 163-169):**\n- Train the model using the training and validation datasets, with early stopping.\n\n**[Save the trained model] (Lines 171-175):**\n- Save the trained model in both H5 and TensorFlow SavedModel formats.\n\n**[Convert TensorFlow model to ONNX] (Lines 176-185):**\n- Convert the trained TensorFlow model to ONNX format and save the ONNX specification.\n\n**[Prepare for out-of-time (OOT) scoring] (Lines 188-229):**\n- Prepare directories and files for OOT scoring, including renaming and moving files, and saving the model graph.\n\n**[Generate test data for prediction] (Lines 230-248):**\n- Generate test data for prediction, save it to a JSON file for later use.\n\n**[Convert and test ONNX model] (Lines 250-269):**\n- Convert the TensorFlow model to ONNX format, run predictions using ONNX runtime, and compare results with TensorFlow predictions to ensure consistency."
    },
    {
      "summary": "**[Setup and configuration] (Lines 2-24):**\n- Imports necessary modules and libraries.\n- Sets the working directory and user environment.\n- Loads configuration parameters and model version information.\n\n**[Define scoring function] (Lines 26-48):**\n- Defines a function `oot_data_eval` to evaluate out-of-time (OOT) data.\n- Loads OOT data and model specifications.\n- Scores the OOT data using the loaded models.\n- Saves the scored data to a specified path.\n\n**[Prepare model paths and scoring parameters] (Lines 49-69):**\n- Sets local and GCP paths for model files and data.\n- Loads model specifications and prepares a list of model paths and scoring outputs.\n\n**[Submit Spark job to GCP] (Lines 70-86):**\n- Configures and submits a Spark job to Google Cloud Platform (GCP) to run the `oot_data_eval` function.\n- Specifies necessary packages and parameters for the Spark job.\n\n**[Check job status and save logs] (Lines 87-94):**\n- Monitors the status of the submitted Spark job.\n- Saves the job logs to a specified file.\n\n**[Create external table in BigQuery] (Lines 100-104):**\n- Creates or replaces an external table in BigQuery using the scored data stored in GCP."
    },
    {
      "summary": "**[Load model version and create directories] (Lines 4-12):**\n- Imports necessary libraries.\n- Loads the current model version from a file.\n- Constructs paths for model artifacts and evaluation readouts.\n- Creates the evaluation readout directory if it does not exist.\n\n**[Load configuration file] (Lines 13-25):**\n- Defines a function to load a YAML configuration file.\n- Loads the configuration file and extracts the BigQuery project dataset prefix.\n\n**[Create driver_oot_txn_365d table] (Lines 26-44):**\n- Constructs a SQL query to create a table `driver_oot_txn_365d` by joining `driver_oot` with transaction data from the past 365 days.\n\n**[Create driver_oot_txn_save_365d table] (Lines 47-65):**\n- Constructs a SQL query to create a table `driver_oot_txn_save_365d` by joining `driver_oot_txn_365d` with offer save events from the past 365 days.\n\n**[Create mlv2_gpt_similar_map_snapshot table] (Lines 68-137):**\n- Constructs a SQL query to create a snapshot table of similar merchants based on the latest data from a specific source table.\n\n**[Create mlv2_gpt_similar_map_snapshot_1 table] (Lines 140-193):**\n- Constructs a SQL query to create a table with concatenated similar merchant IDs, splitting them into individual columns.\n\n**[Create driver_oot_txn_save_365d_similar table] (Lines 196-207):**\n- Constructs a SQL query to create a table `driver_oot_txn_save_365d_similar` by joining `driver_oot_txn_save_365d` with the similar merchants snapshot.\n\n**[Create driver_oot_txn_save_365d_similar_dedup table] (Lines 209-247):**\n- Constructs a SQL query to create a deduplicated table of similar merchants for each customer and run date.\n\n**[Create driver_oot_two_tower_similar_score table] (Lines 249-336):**\n- Constructs a SQL query to calculate similarity scores between customers and merchants using embeddings from two towers.\n\n**[Create driver_oot_hueristic_model_comparison table] (Lines 339-372):**\n- Constructs a SQL query to compare heuristic model scores with other model scores, ranking them for each customer and run date.\n\n**[Calculate recall metrics for all models] (Lines 375-430):**\n- Constructs a SQL query to calculate recall metrics at different ranks for various models, grouping by model type.\n\n**[Save and plot performance metrics for all models] (Lines 433-447):**\n- Saves the performance metrics to a CSV file.\n- Plots the performance metrics using seaborn and matplotlib, saving the plot as an image.\n\n**[Calculate recall metrics for first-time users] (Lines 448-513):**\n- Constructs a SQL query to calculate recall metrics for first-time users, grouping by model type.\n\n**[Save and plot performance metrics for first-time users] (Lines 516-530):**\n- Saves the performance metrics for first-time users to a CSV file.\n- Plots the performance metrics using seaborn and matplotlib, saving the plot as an image.\n\n**[Calculate recall metrics excluding specific merchants] (Lines 532-590):**\n- Constructs a SQL query to calculate recall metrics excluding specific merchants, grouping by model type.\n\n**[Save and plot performance metrics excluding specific merchants] (Lines 592-607):**\n- Saves the performance metrics excluding specific merchants to a CSV file.\n- Plots the performance metrics using seaborn and matplotlib, saving the plot as an image.\n\n**[Calculate recall metrics for recent data] (Lines 608-728):**\n- Constructs a SQL query to calculate recall metrics for recent data (last 7 days), grouping by model type.\n\n**[Save and plot performance metrics for recent data] (Lines 730-731):**\n- (Commented out) Intended to save the performance metrics for recent data to a CSV file."
    }
  ],
  "component_identification": [
    "MAJOR COMPONENTS IDENTIFIED: [Driver Creation]\n\nDETAILS FOR EACH:\nDriver Creation:\n    - Line Range: Lines 4-512\n    - Evidence:\n        - \"Create table for eligible merchants (Lines 18-52): Drops and creates a table for live unique merchants with specific attributes. Joins multiple tables to gather necessary merchant information.\" \u2013 This indicates the initial step of creating a foundational dataset.\n        - \"Create initial driver table (Lines 54-83): Drops and creates a table with customer and merchant interaction data. Filters data based on specific conditions like event date and customer ID.\" \u2013 This step is crucial for forming the primary dataset used in subsequent steps.\n        - \"Create positive samples for training (Lines 124-168): Drops and creates a table for positive training samples attributed to transactions. Applies sampling ratios and splits data into training and validation sets.\" \u2013 This involves creating labeled data, a key part of the driver dataset.\n        - \"Create development dataset (Lines 410-445): Drops and creates a table combining positive and negative samples for model development. Includes both hard and uniform negative samples.\" \u2013 This final step consolidates the driver dataset for model development.\n\nThe entire code summary revolves around creating and refining the driver dataset, which includes loading configurations, extracting necessary data, and preparing it for further stages in the ML pipeline. Each step builds upon the previous one to form a comprehensive driver dataset, making it a single, cohesive component.",
    "MAJOR COMPONENTS IDENTIFIED: [Driver Creation]\n\nDETAILS FOR EACH:\nDriver Creation:\n    - Line Range: Lines 17-817\n    - Evidence:\n        - \"Create driver_simu_txn_365d table (Lines 17-40)\" \u2013 This indicates the creation of a driver dataset by joining transaction data with payment data.\n        - \"Create driver_consumer_base table (Lines 58-66)\" \u2013 This shows the creation of a base table with customer IDs and run dates, which is a fundamental part of the driver dataset.\n        - \"Create driver_merchant_base table (Lines 468-476)\" \u2013 This involves creating a base table with receiver IDs and run dates, another essential part of the driver dataset.\n        - \"Create driver_consumer_base_gender table (Lines 796-817)\" \u2013 This involves joining consumer base data with gender prediction data, which is part of the initial data loading and extraction process.\n\nThe entire file is focused on creating various tables that form the driver dataset, which includes basic rows and metadata necessary for the subsequent steps in the ML pipeline. The operations described involve loading data, joining tables, and creating base tables, all of which are characteristic of the Driver Creation component. There is no strong justification to split these operations into separate components as they all contribute to the creation of the driver dataset.",
    "MAJOR COMPONENTS IDENTIFIED: [Data Pulling]\n\nDETAILS FOR EACH:\n[Data Pulling]:\n    - Line Range: Lines 34-73\n    - Evidence:\n        - \"Creates and configures a Fetcher object with various settings including job name, group name, model name, owner, description, and manager.\" \u2013 This indicates the setup of a tool specifically designed to pull data.\n        - \"Specifies the variables to be fetched and the data split ratio.\" \u2013 This further supports the configuration of data pulling.\n        - \"Executes the Fetcher to fetch the data.\" \u2013 This confirms the action of pulling data from a source.\n        - \"Creates or replaces an external table in BigQuery using the fetched data stored in Google Cloud Storage.\" \u2013 This shows the final step of storing the pulled data into a usable format for further processing.\n\nFULL ML PIPELINE FILE LIST:\n['repos/ql-store-recommendation-prod/research/pipeline/00_driver.py', 'repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.py', 'repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.py', 'repos/ql-store-recommendation-prod/research/pipeline/02_combine.py', 'repos/ql-store-recommendation-prod/research/pipeline/03_prepare_training_data.py', 'repos/ql-store-recommendation-prod/research/pipeline/04_training.py', 'repos/ql-store-recommendation-prod/research/pipeline/05_scoring_oot.py', 'repos/ql-store-recommendation-prod/research/pipeline/06_evaluation.py']\n\nCURRENT FILE'S NAME:\n01_varmart_feat.ipynb\n\nCURRENT FILE'S CODE SUMMARY:\n**User authentication and environment setup (Lines 4-9):**\n- Imports necessary modules for cloud operations.\n- Authenticates the user.\n- Sets an environment variable to disable a specific development feature.\n\n**Export data to Google Cloud Storage (Lines 13-22):**\n- Exports data from a BigQuery table to Google Cloud Storage in Parquet format.\n\n**Import modules and initialize variables (Lines 23-34):**\n- Imports additional necessary modules.\n- Initializes variables for the current date, user, production status, stage, sequence number, and job name.\n\n**Configure and initialize Fetcher object (Lines 34-66):**\n- Creates and configures a Fetcher object with various settings including job name, group name, model name, owner, description, and manager.\n- Sets GCP-specific configurations such as project ID, bucket name, and data locations.\n- Specifies the variables to be fetched and the data split ratio.\n\n**Run Fetcher and create external table (Lines 67-73):**\n- Executes the Fetcher to fetch the data.\n- Creates or replaces an external table in BigQuery using the fetched data stored in Google Cloud Storage.",
    "MAJOR COMPONENTS IDENTIFIED: [Feature Consolidation]\n\nDETAILS FOR EACH:\n[Feature Consolidation]:\n    - Line Range: Lines 4-639\n    - Evidence:\n        - \"Create driver_dev_features table\" (Lines 17-216) \u2013 This section constructs a SQL query to create a new table by joining multiple tables and selecting various features, which is indicative of merging datasets into a unified feature set.\n        - \"Create driver_oot_features table\" (Lines 219-418) \u2013 Similar to the previous block, this section constructs a SQL query to create another table by joining multiple tables and selecting features, further supporting the merging of datasets.\n        - \"Expand sequence features in driver_oot_features_expand_seq table\" (Lines 421-625) \u2013 This section constructs a SQL query to create a new table by expanding sequence features, which involves merging and transforming data.\n        - \"Export data to Google Cloud Storage\" (Lines 630-639) \u2013 This section exports the consolidated feature set to Google Cloud Storage, indicating the completion of the feature consolidation process.\n\nThe entire file is focused on merging multiple datasets into unified feature sets for modeling, which aligns with the Feature Consolidation category. There is no distinct line of separation that justifies splitting this into multiple components, as all sections contribute to the overall goal of consolidating features.",
    "MAJOR COMPONENTS IDENTIFIED: [Data Preprocessing]\n\nDETAILS FOR EACH:\n[Data Preprocessing]:\n    - Line Range: Lines 21-175\n    - Evidence:\n        - \"Load and concatenate data\" (Lines 21-30) \u2013 This step involves loading and combining data, which is a preliminary step in data preprocessing.\n        - \"Map state names to abbreviations\" (Lines 31-94) \u2013 Cleaning and transforming state names is a part of data preprocessing.\n        - \"Encode categorical features\" (Lines 95-112) \u2013 Encoding categorical features is a common data preprocessing task.\n        - \"Scale numerical features\" (Lines 113-134) \u2013 Scaling numerical features is a standard preprocessing step.\n        - \"Process sequence features\" (Lines 135-153) \u2013 Handling missing values and tokenizing sequences are preprocessing activities.\n        - \"Save processed data in chunks\" (Lines 154-166) \u2013 Saving the processed data is part of the preprocessing workflow.\n        - \"Save feature transformers\" (Lines 167-175) \u2013 Saving the transformers used in preprocessing ensures consistency in future data transformations.\n\nThe entire file is dedicated to various data preprocessing tasks, from loading and cleaning data to encoding, scaling, and saving the processed data and transformers. These steps are all integral parts of the data preprocessing component and do not warrant separation into distinct nodes.",
    "MAJOR COMPONENTS IDENTIFIED: [Model Training, Model Packaging]\n\nDETAILS FOR EACH:\n[Model Training]:\n    - Line Range: Lines 140-169\n    - Evidence:\n        - \"Define and compile the model\" \u2013 This indicates the setup of the model architecture and compilation, which are essential steps in model training.\n        - \"Train the model using the training and validation datasets, with early stopping\" \u2013 This clearly describes the process of fitting the model to the training data, which is the core of model training.\n\n[Model Packaging]:\n    - Line Range: Lines 171-185\n    - Evidence:\n        - \"Save the trained model in both H5 and TensorFlow SavedModel formats\" \u2013 This step involves saving the trained model in formats suitable for deployment, which is a key aspect of model packaging.\n        - \"Convert the trained TensorFlow model to ONNX format and save the ONNX specification\" \u2013 Converting the model to ONNX format is part of preparing the model for deployment, further supporting this as model packaging.\n\n    - Why This Is Separate: The process of saving and converting the model into deployment-ready formats is distinct from the training process. The training process involves fitting the model to data, while packaging involves preparing the trained model for deployment. There is no overlap with the model training lines (140-169), ensuring clear separation.\n\nFULL ML PIPELINE FILE LIST:\n['repos/ql-store-recommendation-prod/research/pipeline/00_driver.py', 'repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.py', 'repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.py', 'repos/ql-store-recommendation-prod/research/pipeline/02_combine.py', 'repos/ql-store-recommendation-prod/research/pipeline/03_prepare_training_data.py', 'repos/ql-store-recommendation-prod/research/pipeline/04_training.py', 'repos/ql-store-recommendation-prod/research/pipeline/05_scoring_oot.py', 'repos/ql-store-recommendation-prod/research/pipeline/06_evaluation.py']\n\nCURRENT FILE'S NAME:\n04_training.ipynb\n\nCURRENT FILE'S CODE SUMMARY:\n**[Import necessary libraries and modules] (Lines 2-11):**\n- Import various libraries and modules required for data processing, model building, and training.\n\n**[Configure GPU settings] (Lines 12-14):**\n- List available GPUs and set memory growth to avoid memory allocation issues.\n\n**[Load YAML configuration file] (Lines 15-21):**\n- Define a function to load a YAML file and handle file not found exceptions.\n\n**[Load configuration and model version] (Lines 22-34):**\n- Load the YAML configuration file and model version, create necessary directories for saving models and artifacts.\n\n**[Load and concatenate data] (Lines 36-43):**\n- Load multiple parquet files from a directory, concatenate them into a single DataFrame.\n\n**[Load feature encoders and tokenizers] (Lines 44-47):**\n- Load pre-trained categorical feature encoders and tokenizers from pickle files.\n\n**[Split data into training and validation sets] (Lines 48-49):**\n- Split the concatenated data into training and validation sets based on a 'split' column.\n\n**[Define feature names] (Lines 52-74):**\n- Define lists of numeric, categorical, and ID feature names to be used in the model.\n\n**[Function to prepare data for model input] (Lines 75-106):**\n- Define a function to prepare feature columns and data for model input, including handling sparse and dense features.\n\n**[Prepare training and validation data] (Lines 107-109):**\n- Prepare training and validation data using the defined function, extracting features and labels.\n\n**[Data generator function] (Lines 110-120):**\n- Define a generator function to yield batches of data for training and validation.\n\n**[Create TensorFlow datasets] (Lines 121-137):**\n- Create TensorFlow datasets for training and validation using the data generator function.\n\n**[Define and compile the model] (Lines 140-155):**\n- Define and compile the DIN model within a distributed strategy scope, using parameters from the configuration file.\n\n**[Early stopping callback] (Lines 156-162):**\n- Define an early stopping callback to monitor validation loss and stop training early if no improvement is seen.\n\n**[Train the model] (Lines 163-169):**\n- Train the model using the training and validation datasets, with early stopping.\n\n**[Save the trained model] (Lines 171-175):**\n- Save the trained model in both H5 and TensorFlow SavedModel formats.\n\n**[Convert TensorFlow model to ONNX] (Lines 176-185):**\n- Convert the trained TensorFlow model to ONNX format and save the ONNX specification.\n\n**[Prepare for out-of-time (OOT) scoring] (Lines 188-229):**\n- Prepare directories and files for OOT scoring, including renaming and moving files, and saving the model graph.\n\n**[Generate test data for prediction] (Lines 230-248):**\n- Generate test data for prediction, save it to a JSON file for later use.\n\n**[Convert and test ONNX model] (Lines 250-269):**\n- Convert the TensorFlow model to ONNX format, run predictions using ONNX runtime, and compare results with TensorFlow predictions to ensure consistency.",
    "MAJOR COMPONENTS IDENTIFIED: [Model Scoring]\n\nDETAILS FOR EACH:\n[Model Scoring]:\n    - Line Range: Lines 26-104\n    - Evidence:\n        - \"Defines a function `oot_data_eval` to evaluate out-of-time (OOT) data.\" \u2013 This indicates the primary function of the code is to score the OOT data using a pre-trained model.\n        - \"Scores the OOT data using the loaded models.\" \u2013 This confirms that the function is performing model scoring.\n        - \"Configures and submits a Spark job to Google Cloud Platform (GCP) to run the `oot_data_eval` function.\" \u2013 This shows that the scoring function is executed as a Spark job on GCP, which is a typical setup for large-scale model scoring tasks.\n        - \"Creates or replaces an external table in BigQuery using the scored data stored in GCP.\" \u2013 This step involves saving the scored data, which is a part of the model scoring process.\n\nThe entire code summary revolves around the process of scoring the OOT data, from defining the scoring function to executing it and saving the results. There are no distinct separable components that fit into other categories based on the provided summary.",
    "MAJOR COMPONENTS IDENTIFIED: [Model Evaluation]\n\nDETAILS FOR EACH:\n[Model Evaluation]:\n    - Line Range: Lines 375-731\n    - Evidence:\n        - \"Constructs a SQL query to calculate recall metrics at different ranks for various models, grouping by model type.\" \u2013 This indicates the calculation of performance metrics, which is a key part of model evaluation.\n        - \"Saves the performance metrics to a CSV file. Plots the performance metrics using seaborn and matplotlib, saving the plot as an image.\" \u2013 This shows the process of saving and visualizing the evaluation results, which is essential for model evaluation.\n        - \"Constructs a SQL query to calculate recall metrics for first-time users, grouping by model type.\" \u2013 This further supports the evaluation of model performance for a specific user group.\n        - \"Constructs a SQL query to calculate recall metrics excluding specific merchants, grouping by model type.\" \u2013 This indicates the evaluation of model performance under different conditions.\n        - \"Constructs a SQL query to calculate recall metrics for recent data (last 7 days), grouping by model type.\" \u2013 This shows the evaluation of model performance on recent data, which is crucial for understanding model behavior over time.\n\nThe above evidence collectively supports that the primary focus of the code from lines 375 to 731 is on evaluating the model's performance through various metrics and conditions, which aligns with the \"Model Evaluation\" component category."
  ],
  "component_parsing": [
    {
      "Driver Creation": {
        "line_range": "Lines 4-512",
        "evidence": [
          "\"Create table for eligible merchants (Lines 18-52): Drops and creates a table for live unique merchants with specific attributes. Joins multiple tables to gather necessary merchant information.\" \u2013 This indicates the initial step of creating a foundational dataset.",
          "\"Create initial driver table (Lines 54-83): Drops and creates a table with customer and merchant interaction data. Filters data based on specific conditions like event date and customer ID.\" \u2013 This step is crucial for forming the primary dataset used in subsequent steps.",
          "\"Create positive samples for training (Lines 124-168): Drops and creates a table for positive training samples attributed to transactions. Applies sampling ratios and splits data into training and validation sets.\" \u2013 This involves creating labeled data, a key part of the driver dataset.",
          "\"Create development dataset (Lines 410-445): Drops and creates a table combining positive and negative samples for model development. Includes both hard and uniform negative samples.\" \u2013 This final step consolidates the driver dataset for model development."
        ],
        "why_separate": null,
        "location": "repos/ql-store-recommendation-prod/research/pipeline/00_driver.ipynb"
      }
    },
    {
      "Driver Creation": {
        "line_range": "Lines 17-817",
        "evidence": [
          "\"Create driver_simu_txn_365d table (Lines 17-40)\" \u2013 This indicates the creation of a driver dataset by joining transaction data with payment data.",
          "\"Create driver_consumer_base table (Lines 58-66)\" \u2013 This shows the creation of a base table with customer IDs and run dates, which is a fundamental part of the driver dataset.",
          "\"Create driver_merchant_base table (Lines 468-476)\" \u2013 This involves creating a base table with receiver IDs and run dates, another essential part of the driver dataset.",
          "\"Create driver_consumer_base_gender table (Lines 796-817)\" \u2013 This involves joining consumer base data with gender prediction data, which is part of the initial data loading and extraction process."
        ],
        "why_separate": "The entire file is focused on creating various tables that form the driver dataset, which includes basic rows and metadata necessary for the subsequent steps in the ML pipeline. The operations described involve loading data, joining tables, and creating base tables, all of which are characteristic of the Driver Creation component. There is no strong justification to split these operations into separate components as they all contribute to the creation of the driver dataset.",
        "location": "repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.ipynb"
      }
    },
    {
      "Data Pulling": {
        "line_range": "Lines 34-73",
        "evidence": [
          "\"Creates and configures a Fetcher object with various settings including job name, group name, model name, owner, description, and manager.\" \u2013 This indicates the setup of a tool specifically designed to pull data.",
          "\"Specifies the variables to be fetched and the data split ratio.\" \u2013 This further supports the configuration of data pulling.",
          "\"Executes the Fetcher to fetch the data.\" \u2013 This confirms the action of pulling data from a source.",
          "\"Creates or replaces an external table in BigQuery using the fetched data stored in Google Cloud Storage.\" \u2013 This shows the final step of storing the pulled data into a usable format for further processing."
        ],
        "why_separate": null,
        "location": "repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.ipynb"
      }
    },
    {
      "Feature Consolidation": {
        "line_range": "Lines 4-639",
        "evidence": [
          "\"Create driver_dev_features table\" (Lines 17-216) \u2013 This section constructs a SQL query to create a new table by joining multiple tables and selecting various features, which is indicative of merging datasets into a unified feature set.",
          "\"Create driver_oot_features table\" (Lines 219-418) \u2013 Similar to the previous block, this section constructs a SQL query to create another table by joining multiple tables and selecting features, further supporting the merging of datasets.",
          "\"Expand sequence features in driver_oot_features_expand_seq table\" (Lines 421-625) \u2013 This section constructs a SQL query to create a new table by expanding sequence features, which involves merging and transforming data.",
          "\"Export data to Google Cloud Storage\" (Lines 630-639) \u2013 This section exports the consolidated feature set to Google Cloud Storage, indicating the completion of the feature consolidation process."
        ],
        "why_separate": "The entire file is focused on merging multiple datasets into unified feature sets for modeling, which aligns with the Feature Consolidation category. There is no distinct line of separation that justifies splitting this into multiple components, as all sections contribute to the overall goal of consolidating features.",
        "location": "repos/ql-store-recommendation-prod/research/pipeline/02_combine.ipynb"
      }
    },
    {
      "Data Preprocessing": {
        "line_range": "Lines 21-175",
        "evidence": [
          "\"Load and concatenate data\" (Lines 21-30) \u2013 This step involves loading and combining data, which is a preliminary step in data preprocessing.",
          "\"Map state names to abbreviations\" (Lines 31-94) \u2013 Cleaning and transforming state names is a part of data preprocessing.",
          "\"Encode categorical features\" (Lines 95-112) \u2013 Encoding categorical features is a common data preprocessing task.",
          "\"Scale numerical features\" (Lines 113-134) \u2013 Scaling numerical features is a standard preprocessing step.",
          "\"Process sequence features\" (Lines 135-153) \u2013 Handling missing values and tokenizing sequences are preprocessing activities.",
          "\"Save processed data in chunks\" (Lines 154-166) \u2013 Saving the processed data is part of the preprocessing workflow.",
          "\"Save feature transformers\" (Lines 167-175) \u2013 Saving the transformers used in preprocessing ensures consistency in future data transformations."
        ],
        "why_separate": null,
        "location": "repos/ql-store-recommendation-prod/research/pipeline/03_prepare_training_data.ipynb"
      }
    },
    {
      "Model Training": {
        "line_range": "Lines 140-169",
        "evidence": [
          "\"Define and compile the model\" \u2013 This indicates the setup of the model architecture and compilation, which are essential steps in model training.",
          "\"Train the model using the training and validation datasets, with early stopping\" \u2013 This clearly describes the process of fitting the model to the training data, which is the core of model training."
        ],
        "why_separate": null,
        "location": "repos/ql-store-recommendation-prod/research/pipeline/04_training.ipynb"
      },
      "Model Packaging": {
        "line_range": "Lines 171-185",
        "evidence": [
          "\"Save the trained model in both H5 and TensorFlow SavedModel formats\" \u2013 This step involves saving the trained model in formats suitable for deployment, which is a key aspect of model packaging.",
          "\"Convert the trained TensorFlow model to ONNX format and save the ONNX specification\" \u2013 Converting the model to ONNX format is part of preparing the model for deployment, further supporting this as model packaging."
        ],
        "why_separate": "The process of saving and converting the model into deployment-ready formats is distinct from the training process. The training process involves fitting the model to data, while packaging involves preparing the trained model for deployment. There is no overlap with the model training lines (140-169), ensuring clear separation.",
        "location": "repos/ql-store-recommendation-prod/research/pipeline/04_training.ipynb"
      }
    },
    {
      "Model Scoring": {
        "line_range": "Lines 26-104",
        "evidence": [
          "\"Defines a function `oot_data_eval` to evaluate out-of-time (OOT) data.\" \u2013 This indicates the primary function of the code is to score the OOT data using a pre-trained model.",
          "\"Scores the OOT data using the loaded models.\" \u2013 This confirms that the function is performing model scoring.",
          "\"Configures and submits a Spark job to Google Cloud Platform (GCP) to run the `oot_data_eval` function.\" \u2013 This shows that the scoring function is executed as a Spark job on GCP, which is a typical setup for large-scale model scoring tasks.",
          "\"Creates or replaces an external table in BigQuery using the scored data stored in GCP.\" \u2013 This step involves saving the scored data, which is a part of the model scoring process."
        ],
        "why_separate": null,
        "location": "repos/ql-store-recommendation-prod/research/pipeline/05_scoring_oot.ipynb"
      }
    },
    {
      "Model Evaluation": {
        "line_range": "Lines 375-731",
        "evidence": [
          "\"Constructs a SQL query to calculate recall metrics at different ranks for various models, grouping by model type.\" \u2013 This indicates the calculation of performance metrics, which is a key part of model evaluation.",
          "\"Saves the performance metrics to a CSV file. Plots the performance metrics using seaborn and matplotlib, saving the plot as an image.\" \u2013 This shows the process of saving and visualizing the evaluation results, which is essential for model evaluation.",
          "\"Constructs a SQL query to calculate recall metrics for first-time users, grouping by model type.\" \u2013 This further supports the evaluation of model performance for a specific user group.",
          "\"Constructs a SQL query to calculate recall metrics excluding specific merchants, grouping by model type.\" \u2013 This indicates the evaluation of model performance under different conditions.",
          "\"Constructs a SQL query to calculate recall metrics for recent data (last 7 days), grouping by model type.\" \u2013 This shows the evaluation of model performance on recent data, which is crucial for understanding model behavior over time."
        ],
        "why_separate": null,
        "location": "repos/ql-store-recommendation-prod/research/pipeline/06_evaluation.ipynb"
      }
    }
  ],
  "verified_components": [
    {
      "Driver Creation": {
        "line_range": "Lines 4-512",
        "evidence": [
          "\"Create table for eligible merchants (Lines 18-52): Drops and creates a table for live unique merchants with specific attributes. Joins multiple tables to gather necessary merchant information.\" \u2013 This indicates the initial step of creating a foundational dataset.",
          "\"Create initial driver table (Lines 54-83): Drops and creates a table with customer and merchant interaction data. Filters data based on specific conditions like event date and customer ID.\" \u2013 This step is crucial for forming the primary dataset used in subsequent steps.",
          "\"Create positive samples for training (Lines 124-168): Drops and creates a table for positive training samples attributed to transactions. Applies sampling ratios and splits data into training and validation sets.\" \u2013 This involves creating labeled data, a key part of the driver dataset.",
          "\"Create development dataset (Lines 410-445): Drops and creates a table combining positive and negative samples for model development. Includes both hard and uniform negative samples.\" \u2013 This final step consolidates the driver dataset for model development."
        ],
        "why_separate": null,
        "location": "repos/ql-store-recommendation-prod/research/pipeline/00_driver.ipynb"
      }
    },
    {
      "Feature Engineering": {
        "line_range": "Lines 17-817",
        "evidence": [
          "\"Create driver_simu_txn_365d table (Lines 17-40)\" \u2013 This indicates the creation of a driver dataset by joining transaction data with payment data.",
          "\"Create driver_consumer_base table (Lines 58-66)\" \u2013 This shows the creation of a base table with customer IDs and run dates, which is a fundamental part of the driver dataset.",
          "\"Create driver_merchant_base table (Lines 468-476)\" \u2013 This involves creating a base table with receiver IDs and run dates, another essential part of the driver dataset.",
          "\"Create driver_consumer_base_gender table (Lines 796-817)\" \u2013 This involves joining consumer base data with gender prediction data, which is part of the initial data loading and extraction process."
        ],
        "why_separate": "The entire file is focused on creating various tables that form the driver dataset, which includes basic rows and metadata necessary for the subsequent steps in the ML pipeline. The operations described involve loading data, joining tables, and creating base tables, all of which are characteristic of the Driver Creation component. There is no strong justification to split these operations into separate components as they all contribute to the creation of the driver dataset.",
        "location": "repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.ipynb"
      }
    },
    {
      "Data Pulling": {
        "line_range": "Lines 34-73",
        "evidence": [
          "\"Creates and configures a Fetcher object with various settings including job name, group name, model name, owner, description, and manager.\" \u2013 This indicates the setup of a tool specifically designed to pull data.",
          "\"Specifies the variables to be fetched and the data split ratio.\" \u2013 This further supports the configuration of data pulling.",
          "\"Executes the Fetcher to fetch the data.\" \u2013 This confirms the action of pulling data from a source.",
          "\"Creates or replaces an external table in BigQuery using the fetched data stored in Google Cloud Storage.\" \u2013 This shows the final step of storing the pulled data into a usable format for further processing."
        ],
        "why_separate": null,
        "location": "repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.ipynb"
      }
    },
    {
      "Feature Consolidation": {
        "line_range": "Lines 4-639",
        "evidence": [
          "\"Create driver_dev_features table\" (Lines 17-216) \u2013 This section constructs a SQL query to create a new table by joining multiple tables and selecting various features, which is indicative of merging datasets into a unified feature set.",
          "\"Create driver_oot_features table\" (Lines 219-418) \u2013 Similar to the previous block, this section constructs a SQL query to create another table by joining multiple tables and selecting features, further supporting the merging of datasets.",
          "\"Expand sequence features in driver_oot_features_expand_seq table\" (Lines 421-625) \u2013 This section constructs a SQL query to create a new table by expanding sequence features, which involves merging and transforming data.",
          "\"Export data to Google Cloud Storage\" (Lines 630-639) \u2013 This section exports the consolidated feature set to Google Cloud Storage, indicating the completion of the feature consolidation process."
        ],
        "why_separate": "The entire file is focused on merging multiple datasets into unified feature sets for modeling, which aligns with the Feature Consolidation category. There is no distinct line of separation that justifies splitting this into multiple components, as all sections contribute to the overall goal of consolidating features.",
        "location": "repos/ql-store-recommendation-prod/research/pipeline/02_combine.ipynb"
      }
    },
    {
      "Data Preprocessing": {
        "line_range": "Lines 21-175",
        "evidence": [
          "\"Load and concatenate data\" (Lines 21-30) \u2013 This step involves loading and combining data, which is a preliminary step in data preprocessing.",
          "\"Map state names to abbreviations\" (Lines 31-94) \u2013 Cleaning and transforming state names is a part of data preprocessing.",
          "\"Encode categorical features\" (Lines 95-112) \u2013 Encoding categorical features is a common data preprocessing task.",
          "\"Scale numerical features\" (Lines 113-134) \u2013 Scaling numerical features is a standard preprocessing step.",
          "\"Process sequence features\" (Lines 135-153) \u2013 Handling missing values and tokenizing sequences are preprocessing activities.",
          "\"Save processed data in chunks\" (Lines 154-166) \u2013 Saving the processed data is part of the preprocessing workflow.",
          "\"Save feature transformers\" (Lines 167-175) \u2013 Saving the transformers used in preprocessing ensures consistency in future data transformations."
        ],
        "why_separate": null,
        "location": "repos/ql-store-recommendation-prod/research/pipeline/03_prepare_training_data.ipynb"
      }
    },
    {
      "Model Training": {
        "line_range": "Lines 140-169",
        "evidence": [
          "\"Define and compile the model\" \u2013 This indicates the setup of the model architecture and compilation, which are essential steps in model training.",
          "\"Train the model using the training and validation datasets, with early stopping\" \u2013 This clearly describes the process of fitting the model to the training data, which is the core of model training."
        ],
        "why_separate": null,
        "location": "repos/ql-store-recommendation-prod/research/pipeline/04_training.ipynb"
      },
      "Model Packaging": {
        "line_range": "Lines 171-185",
        "evidence": [
          "\"Save the trained model in both H5 and TensorFlow SavedModel formats\" \u2013 This step involves saving the trained model in formats suitable for deployment, which is a key aspect of model packaging.",
          "\"Convert the trained TensorFlow model to ONNX format and save the ONNX specification\" \u2013 Converting the model to ONNX format is part of preparing the model for deployment, further supporting this as model packaging."
        ],
        "why_separate": "The process of saving and converting the model into deployment-ready formats is distinct from the training process. The training process involves fitting the model to data, while packaging involves preparing the trained model for deployment. There is no overlap with the model training lines (140-169), ensuring clear separation.",
        "location": "repos/ql-store-recommendation-prod/research/pipeline/04_training.ipynb"
      }
    },
    {
      "Model Scoring": {
        "line_range": "Lines 26-104",
        "evidence": [
          "\"Defines a function `oot_data_eval` to evaluate out-of-time (OOT) data.\" \u2013 This indicates the primary function of the code is to score the OOT data using a pre-trained model.",
          "\"Scores the OOT data using the loaded models.\" \u2013 This confirms that the function is performing model scoring.",
          "\"Configures and submits a Spark job to Google Cloud Platform (GCP) to run the `oot_data_eval` function.\" \u2013 This shows that the scoring function is executed as a Spark job on GCP, which is a typical setup for large-scale model scoring tasks.",
          "\"Creates or replaces an external table in BigQuery using the scored data stored in GCP.\" \u2013 This step involves saving the scored data, which is a part of the model scoring process."
        ],
        "why_separate": null,
        "location": "repos/ql-store-recommendation-prod/research/pipeline/05_scoring_oot.ipynb"
      }
    },
    {
      "Model Evaluation": {
        "line_range": "Lines 375-731",
        "evidence": [
          "\"Constructs a SQL query to calculate recall metrics at different ranks for various models, grouping by model type.\" \u2013 This indicates the calculation of performance metrics, which is a key part of model evaluation.",
          "\"Saves the performance metrics to a CSV file. Plots the performance metrics using seaborn and matplotlib, saving the plot as an image.\" \u2013 This shows the process of saving and visualizing the evaluation results, which is essential for model evaluation.",
          "\"Constructs a SQL query to calculate recall metrics for first-time users, grouping by model type.\" \u2013 This further supports the evaluation of model performance for a specific user group.",
          "\"Constructs a SQL query to calculate recall metrics excluding specific merchants, grouping by model type.\" \u2013 This indicates the evaluation of model performance under different conditions.",
          "\"Constructs a SQL query to calculate recall metrics for recent data (last 7 days), grouping by model type.\" \u2013 This shows the evaluation of model performance on recent data, which is crucial for understanding model behavior over time."
        ],
        "why_separate": null,
        "location": "repos/ql-store-recommendation-prod/research/pipeline/06_evaluation.ipynb"
      }
    }
  ],
  "attribute_identification": [],
  "attribute_parsing": [],
  "node_aggregator": [],
  "edges": [],
  "dag_yaml": "",
  "verified_dag": {},
  "config": {},
  "notebooks": []
}