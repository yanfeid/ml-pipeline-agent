{
  "summaries": [
    {
      "summary": "**Load YAML configuration file** (Lines 4-13):\n- Defines a function to load a YAML file.\n- Loads the configuration from a specified file path.\n\n**Extract BigQuery project dataset prefix** (Lines 14-16):\n- Extracts the BigQuery project dataset prefix from the loaded configuration.\n\n**Create live unique merchants training table** (Lines 18-52):\n- Drops the existing table if it exists.\n- Creates a new table with unique merchants by joining multiple tables and filtering based on specific conditions.\n\n**Create driver_00 table** (Lines 54-83):\n- Drops the existing table if it exists.\n- Creates a new table with customer and merchant data, including various attributes and conditions.\n\n**Create driver_0 table** (Lines 84-103):\n- Drops the existing table if it exists.\n- Creates a new table by filtering customers based on the number of distinct placements and specific conditions.\n\n**Create driver_1 table** (Lines 105-121):\n- Drops the existing table if it exists.\n- Creates a new table with transaction data by joining with the live unique merchants training table.\n\n**Create driver_positive_train_attributed table** (Lines 124-168):\n- Drops the existing table if it exists.\n- Creates a new table with positive training samples attributed to transactions, including various calculations and sampling.\n\n**Create driver_positive_train_organic_0 table** (Lines 170-204):\n- Drops the existing table if it exists.\n- Creates a new table with organic positive training samples by filtering out specific conditions and removing biases.\n\n**Create driver_positive_train_organic table** (Lines 207-244):\n- Drops the existing table if it exists.\n- Creates a new table with organic positive training samples, including various calculations and sampling.\n\n**Create driver_positive table** (Lines 247-277):\n- Drops the existing table if it exists.\n- Combines multiple positive training samples into a single table with different tags and splits.\n\n**Create driver_positive_training_split_0 table** (Lines 281-308):\n- Drops the existing table if it exists.\n- Creates a new table by splitting positive training samples into hard and uniform negatives based on specific conditions.\n\n**Create driver_positive_training_split table** (Lines 310-322):\n- Drops the existing table if it exists.\n- Adds a count of daily positive samples to the training split table.\n\n**Create driver_training_hard_negative table** (Lines 324-336):\n- Drops the existing table if it exists.\n- Creates a new table with hard negative samples by joining with the driver_0 table and applying specific conditions.\n\n**Create driver_training_hard_negative_downsample table** (Lines 339-349):\n- Drops the existing table if it exists.\n- Downsamples hard negative samples based on specific conditions and joins.\n\n**Create driver_training_uniform_negative table** (Lines 352-364):\n- Drops the existing table if it exists.\n- Creates a new table with uniform negative samples by joining with the live unique merchants training table.\n\n**Create driver_training_uniform_negative_remove_window_positive table** (Lines 367-376):\n- Drops the existing table if it exists.\n- Removes uniform negative samples that fall within a specific time window of positive feedback.\n\n**Create driver_training_uniform_negative_downsample_0 table** (Lines 379-401):\n- Drops the existing table if it exists.\n- Downsamples uniform negative samples based on merchant sampling probability.\n\n**Create driver_training_uniform_negative_downsample table** (Lines 404-408):\n- Drops the existing table if it exists.\n- Further downsamples uniform negative samples based on a specified ratio.\n\n**Create driver_dev table** (Lines 410-445):\n- Drops the existing table if it exists.\n- Combines positive and negative samples into a development table with target labels.\n\n**Create driver_oot_uniform_negative_0 table** (Lines 449-469):\n- Drops the existing table if it exists.\n- Creates a new table with out-of-time (OOT) uniform negative samples by joining with the live unique merchants training table.\n\n**Create driver_oot_uniform_negative table** (Lines 472-478):\n- Drops the existing table if it exists.\n- Removes duplicate OOT uniform negative samples.\n\n**Create driver_oot table** (Lines 481-495):\n- Drops the existing table if it exists.\n- Combines positive and negative OOT samples into a single table with target labels.\n\n**Create driver_simu table** (Lines 498-507):\n- Drops the existing table if it exists.\n- Combines distinct customer and receiver pairs from development and OOT tables.\n\n**Create driver_simu_consumer table** (Lines 509-512):\n- Drops the existing table if it exists.\n- Creates a new table with distinct customers and run dates from the simulation table."
    },
    {
      "summary": "**Load YAML configuration file** (Lines 4-13):\n- Defines a function to load a YAML file.\n- Loads the configuration from a specified file path.\n\n**Extract BigQuery project dataset prefix** (Lines 14-16):\n- Extracts the BigQuery project dataset prefix from the loaded configuration.\n\n**Create driver_simu_txn_365d table** (Lines 17-40):\n- Drops the existing table if it exists.\n- Creates a new table with transaction data and additional date fields.\n\n**Create driver_simu_txn_365d_agg table** (Lines 43-55):\n- Drops the existing table if it exists.\n- Aggregates transaction data over different time intervals (7, 30, 180, 365 days).\n\n**Create driver_consumer_base table** (Lines 58-66):\n- Drops the existing table if it exists.\n- Creates a base table with customer IDs and run dates, including additional date fields.\n\n**Create driver_consumer_base_txn_5k_merch_category table** (Lines 69-93):\n- Drops the existing table if it exists.\n- Joins consumer base data with transaction data and merchant category information.\n\n**Create driver_consumer_base_last_10_txn table** (Lines 96-175):\n- Drops the existing table if it exists.\n- Aggregates the last 10 transactions for each customer, calculating average transaction amounts.\n\n**Create driver_consumer_base_all_history_array_0 table** (Lines 178-185):\n- Drops the existing table if it exists.\n- Selects the most recent 100 transactions for each customer.\n\n**Create driver_consumer_base_all_history_array table** (Lines 188-194):\n- Drops the existing table if it exists.\n- Aggregates the most recent 100 merchants and their categories into arrays.\n\n**Create driver_combine_category table** (Lines 198-203):\n- Drops the existing table if it exists.\n- Combines driver simulation data with merchant category information.\n\n**Create driver_combine_category_agg_0 table** (Lines 206-219):\n- Drops the existing table if it exists.\n- Aggregates transaction data by category over different time intervals.\n\n**Create driver_combine_category_agg_1 table** (Lines 222-231):\n- Drops the existing table if it exists.\n- Aggregates transaction counts over different time intervals.\n\n**Create driver_combine_category_agg_2 table** (Lines 234-257):\n- Drops the existing table if it exists.\n- Combines category transaction data with overall transaction counts to calculate average amounts.\n\n**Create driver_combine_category_agg_3 table** (Lines 260-286):\n- Drops the existing table if it exists.\n- Combines customer and receiver category data with aggregated transaction data.\n\n**Create driver_combine_category_agg_4 table** (Lines 289-302):\n- Drops the existing table if it exists.\n- Ranks categories by transaction frequency over different time intervals.\n\n**Create driver_combine_category_agg_5 table** (Lines 305-464):\n- Drops the existing table if it exists.\n- Joins consumer base data with the top 3 frequent merchant categories over different time intervals.\n\n**Create driver_merchant_base table** (Lines 468-476):\n- Drops the existing table if it exists.\n- Creates a base table with receiver IDs and run dates, including additional date fields.\n\n**Create driver_merchant_base_txn_30d table** (Lines 479-506):\n- Drops the existing table if it exists.\n- Joins merchant base data with transaction data within the last 30 days.\n\n**Create driver_merchant_base_txn_30d_filter_sndr table** (Lines 509-523):\n- Drops the existing table if it exists.\n- Filters transactions to include only specific customer account types.\n\n**Create driver_merchant_base_price_agg table** (Lines 526-551):\n- Drops the existing table if it exists.\n- Aggregates transaction prices for merchants over the last 30 days.\n\n**Create driver_merchant_base_sales_agg table** (Lines 554-566):\n- Drops the existing table if it exists.\n- Aggregates sales data for merchants over the last 30 days.\n\n**Create driver_elig_save_365d_category table** (Lines 569-616):\n- Drops the existing table if it exists.\n- Joins consumer base data with save event data and merchant category information.\n\n**Create driver_elig_save_agg_00 table** (Lines 619-630):\n- Drops the existing table if it exists.\n- Aggregates save counts for each customer and receiver over different time intervals.\n\n**Create driver_elig_save_agg_0 table** (Lines 633-643):\n- Drops the existing table if it exists.\n- Aggregates save counts for each customer over different time intervals.\n\n**Create driver_elig_save_agg_1 table** (Lines 646-717):\n- Drops the existing table if it exists.\n- Joins consumer base data with the last 5 save events for each customer.\n\n**Create driver_elig_save_agg_2 table** (Lines 720-731):\n- Drops the existing table if it exists.\n- Aggregates save counts by category for each customer over different time intervals.\n\n**Create driver_elig_save_agg_3 table** (Lines 734-751):\n- Drops the existing table if it exists.\n- Joins combined category data with aggregated save counts by category.\n\n**Create driver_merchant_base_click_save table** (Lines 754-793):\n- Drops the existing table if it exists.\n- Aggregates save event data for merchants over the last 30 days, including various segmentations.\n\n**Create driver_consumer_base_gender table** (Lines 796-817):\n- Drops the existing table if it exists.\n- Joins consumer base data with gender prediction data to assign gender to each customer."
    },
    {
      "summary": "**Authenticate user and set environment variables (Lines 4-9):**\n- Imports necessary modules and authenticates the user.\n- Sets an environment variable to disable a specific development feature.\n\n**Export data from BigQuery to Google Cloud Storage (Lines 13-22):**\n- Executes a BigQuery command to export data from a specified table to a Google Cloud Storage bucket in Parquet format.\n\n**Initialize Fetcher object with configurations (Lines 23-66):**\n- Imports additional modules and sets up date, user, and job-related variables.\n- Initializes a Fetcher object with specific configurations including job name, group name, model name, and owner.\n- Configures GCP settings, data locations, and variables to be fetched.\n- Sets the data split ratio for training.\n\n**Run Fetcher to fetch data (Line 67):**\n- Executes the fetcher to run the data fetching process based on the provided configurations.\n\n**Create or replace an external table in BigQuery (Lines 68-73):**\n- Executes a BigQuery command to create or replace an external table that references Parquet files stored in a Google Cloud Storage bucket."
    },
    {
      "summary": "**Load YAML configuration file (Lines 4-13):**\n- Defines a function to load a YAML file.\n- Attempts to load a configuration file and assigns its content to a variable.\n\n**Extract BigQuery project dataset prefix from configuration (Lines 14-16):**\n- Checks if the configuration is loaded.\n- Extracts the BigQuery project dataset prefix from the configuration.\n\n**Create driver_dev_features table in BigQuery (Lines 17-216):**\n- Constructs a SQL query to drop and create a new table `driver_dev_features`.\n- Selects various features from multiple joined tables, applying COALESCE to handle null values.\n\n**Create driver_oot_features table in BigQuery (Lines 219-418):**\n- Constructs a SQL query to drop and create a new table `driver_oot_features`.\n- Similar to the previous block, selects features from multiple joined tables with COALESCE.\n\n**Expand sequence features in driver_oot_features_expand_seq table (Lines 421-624):**\n- Constructs a SQL query to drop and create a new table `driver_oot_features_expand_seq`.\n- Splits a comma-separated list into individual columns for historical receiver IDs and categories.\n\n**Export data to Google Cloud Storage in Parquet format (Lines 631-639):**\n- Constructs a SQL query to export data from a BigQuery table to Google Cloud Storage in Parquet format."
    },
    {
      "summary": "**[Import necessary libraries and set up paths] (Lines 1-20):**\n- Import various libraries for data manipulation, preprocessing, and saving.\n- Set up paths and directories for saving model versions and feature transformers.\n\n**[Load and concatenate data] (Lines 21-30):**\n- Define the directory containing data files.\n- Load and concatenate multiple parquet files into a single DataFrame.\n\n**[Map state names to abbreviations] (Lines 31-94):**\n- Create a dictionary to map full state names to their abbreviations.\n- Define a function to clean state names in the data and apply it.\n\n**[Encode categorical features] (Lines 95-112):**\n- Encode the 'sndr_prmry_addr_state' feature using LabelEncoder.\n- Encode other specified categorical features and store the encoders.\n\n**[Scale numerical features] (Lines 113-134):**\n- Define a list of numerical features to be scaled.\n- Scale these features using StandardScaler and store the scalers.\n\n**[Process sequence features] (Lines 135-153):**\n- Clean and pad sequences for 'sndr_most_recent_100_merch_category' and 'sndr_most_recent_100_merch_list'.\n- Tokenize and pad sequences for 'sndr_most_recent_100_merch_list'.\n- Drop the original sequence columns from the data.\n\n**[Save processed data in chunks] (Lines 154-166):**\n- Define a function to write data chunks to parquet files.\n- Split the data into chunks and save each chunk to a specified directory.\n\n**[Export feature transformers] (Lines 167-175):**\n- Save the trained Tokenizer and feature encoders/scalers to disk using pickle."
    },
    {
      "summary": "**[Import necessary libraries and modules] (Lines 2-11):**\n- Import various libraries and modules required for data processing, machine learning, and model training.\n\n**[Configure GPU settings] (Lines 12-14):**\n- List available GPUs and set memory growth to avoid memory allocation issues.\n\n**[Load YAML configuration file] (Lines 15-21):**\n- Define a function to load a YAML file and handle file not found exceptions.\n\n**[Load configuration and model version] (Lines 22-34):**\n- Load the configuration file and model version, create necessary directories for saving models and artifacts.\n\n**[Load and concatenate data] (Lines 36-43):**\n- Load multiple parquet files from a directory and concatenate them into a single DataFrame.\n\n**[Load feature encoders and tokenizers] (Lines 44-47):**\n- Load pre-trained categorical feature encoders and tokenizers from pickle files.\n\n**[Split data into training and validation sets] (Lines 48-49):**\n- Split the concatenated data into training and validation sets based on a 'split' column.\n\n**[Define feature names] (Lines 52-74):**\n- Define lists of numeric, categorical, and ID feature names to be used in the model.\n\n**[Function to prepare data for model input] (Lines 75-106):**\n- Define a function to prepare feature columns and transform data into the format required by the model.\n\n**[Prepare training and validation data] (Lines 107-109):**\n- Prepare training and validation data using the defined function.\n\n**[Data generator function] (Lines 110-120):**\n- Define a generator function to yield batches of data for training.\n\n**[Create TensorFlow datasets] (Lines 121-137):**\n- Create TensorFlow datasets for training and validation using the data generator function.\n\n**[Define and compile the model] (Lines 140-155):**\n- Define the model architecture using the DIN model and compile it with the Adam optimizer and binary cross-entropy loss.\n\n**[Early stopping callback] (Lines 156-162):**\n- Define an early stopping callback to monitor validation loss and stop training if no improvement is observed.\n\n**[Train the model] (Lines 163-170):**\n- Train the model using the training dataset and validate it using the validation dataset, with early stopping.\n\n**[Save the trained model] (Lines 171-175):**\n- Save the trained model in both H5 and TensorFlow SavedModel formats.\n\n**[Convert TensorFlow model to ONNX] (Lines 176-185):**\n- Convert the trained TensorFlow model to ONNX format and save the ONNX specification.\n\n**[Prepare for out-of-time (OOT) scoring] (Lines 188-229):**\n- Prepare the model and data for OOT scoring, including renaming files and setting up preprocessing layers.\n\n**[Generate test data and save as JSON] (Lines 230-248):**\n- Generate test data for a specific customer and date, and save it as a JSON file.\n\n**[Convert TensorFlow model to ONNX format] (Lines 251-253):**\n- Convert the saved TensorFlow model to ONNX format using a command-line tool.\n\n**[Run inference with ONNX model] (Lines 254-267):**\n- Load the ONNX model and run inference, comparing the results with the TensorFlow model to ensure consistency."
    },
    {
      "summary": "**[Setup and Configuration] (Lines 2-24):**\n- Imports necessary modules and libraries.\n- Sets the working directory and user environment.\n- Loads configuration settings and model version information.\n\n**[Define Function for Out-of-Time Data Evaluation] (Lines 25-48):**\n- Defines a function `oot_data_eval` to evaluate out-of-time data.\n- Loads the model and data, performs scoring, and saves the results.\n\n**[Model and Data Paths Setup] (Lines 49-53):**\n- Sets local and GCP paths for model and data storage.\n\n**[Retrieve Model Information] (Lines 54-69):**\n- Retrieves model specifications and prepares a list of model paths and score outputs.\n\n**[Submit Spark Job to GCP] (Lines 70-86):**\n- Configures and submits a Spark job to Google Cloud Platform for data evaluation.\n- Specifies the function to run, packages to install, and other job parameters.\n\n**[Check Job Status and Save Logs] (Lines 87-94):**\n- Monitors the job status and saves the job logs to a specified location.\n\n**[Create External Table in BigQuery] (Lines 100-104):**\n- Creates or replaces an external table in BigQuery using the scored data stored in GCP."
    },
    {
      "summary": "**[Load model version and create directories] (Lines 4-12):**\n- Imports necessary libraries.\n- Loads the current model version from a file.\n- Constructs paths for model artifacts and evaluation readouts.\n- Creates the evaluation readout directory if it does not exist.\n\n**[Load configuration file] (Lines 13-25):**\n- Defines a function to load a YAML configuration file.\n- Loads the base configuration file and extracts the BigQuery project dataset prefix.\n\n**[Create driver_oot_txn_365d table] (Lines 26-44):**\n- Constructs a SQL query to create a table with transaction data joined with the last purchase timestamp.\n\n**[Create driver_oot_txn_save_365d table] (Lines 47-65):**\n- Constructs a SQL query to create a table with transaction data joined with the last save date.\n\n**[Create mlv2_gpt_similar_map_snapshot table] (Lines 68-137):**\n- Constructs a SQL query to create a snapshot table of similar merchants based on a ranking system.\n\n**[Create mlv2_gpt_similar_map_snapshot_1 table] (Lines 140-193):**\n- Constructs a SQL query to create a table with concatenated similar merchant IDs.\n\n**[Create driver_oot_txn_save_365d_similar table] (Lines 196-207):**\n- Constructs a SQL query to create a table with transaction data and similar merchant information.\n\n**[Create driver_oot_txn_save_365d_similar_dedup table] (Lines 209-247):**\n- Constructs a SQL query to create a deduplicated table of similar merchants for each customer and run date.\n\n**[Create driver_oot_two_tower_similar_score table] (Lines 249-336):**\n- Constructs a SQL query to create a table with similarity scores between customer and merchant embeddings.\n\n**[Create driver_oot_hueristic_model_comparison table] (Lines 339-372):**\n- Constructs a SQL query to create a table comparing heuristic model scores with various ranking metrics.\n\n**[Calculate recall metrics for all models] (Lines 375-430):**\n- Constructs a SQL query to calculate recall metrics for different models and transaction types.\n- Exports the results to a CSV file and generates bar plots for visualization.\n\n**[Calculate recall metrics for first-time users] (Lines 448-530):**\n- Constructs a SQL query to calculate recall metrics for first-time users.\n- Exports the results to a CSV file and generates bar plots for visualization.\n\n**[Calculate recall metrics excluding specific merchants] (Lines 532-607):**\n- Constructs a SQL query to calculate recall metrics excluding specific merchants.\n- Exports the results to a CSV file and generates bar plots for visualization.\n\n**[Calculate recall metrics for recent and older data] (Lines 608-731):**\n- Constructs a SQL query to calculate recall metrics for recent and older data.\n- Exports the results to a CSV file and generates bar plots for visualization."
    }
  ]
}