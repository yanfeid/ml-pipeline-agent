{
  "verified_components": [
    {
      "Driver Creation": {
        "line_range": "Lines 4-512",
        "evidence": [
          "\"Create live unique merchants training table\" (Lines 18-52) \u2013 This section involves creating a new table with unique merchants by joining multiple tables and filtering based on specific conditions, which is part of the initial data loading and extraction process.",
          "\"Create driver_00 table\" (Lines 54-83) \u2013 This section involves creating a new table with customer and merchant data, including various attributes and conditions, which is a key part of creating the driver dataset.",
          "\"Create driver_positive table\" (Lines 247-277) \u2013 This section combines multiple positive training samples into a single table with different tags and splits, which is essential for the driver dataset.",
          "\"Create driver_dev table\" (Lines 410-445) \u2013 This section combines positive and negative samples into a development table with target labels, which is a crucial part of the driver dataset.",
          "\"Create driver_oot table\" (Lines 481-495) \u2013 This section combines positive and negative OOT samples into a single table with target labels, which is part of the driver dataset creation."
        ],
        "why_separate": "The entire notebook is dedicated to the creation of the driver dataset, which is the initial and substantial step in the ML workflow. This component is distinct and does not overlap with other components such as feature engineering or model training, which would occur in subsequent steps of the pipeline. The driver dataset creation involves multiple steps of data extraction, transformation, and combination, all of which are integral to this component.",
        "file_name": "repos/ql-store-recommendation-prod/research/pipeline/00_driver.ipynb"
      }
    },
    {
      "Feature Engineering": {
        "line_range": "Lines 17-817",
        "evidence": [
          "\"Create driver_simu_txn_365d table\" (Lines 17-40) \u2013 This section involves creating a table with transaction data and additional date fields, which is a fundamental part of the driver dataset.",
          "\"Create driver_consumer_base table\" (Lines 58-66) \u2013 This section involves creating a base table with customer IDs and run dates, which is essential for the driver dataset.",
          "\"Create driver_merchant_base table\" (Lines 468-476) \u2013 This section involves creating a base table with receiver IDs and run dates, which is another fundamental part of the driver dataset.",
          "\"Create driver_elig_save_365d_category table\" (Lines 569-616) \u2013 This section involves joining consumer base data with save event data and merchant category information, contributing to the driver dataset.",
          "\"Create driver_consumer_base_gender table\" (Lines 796-817) \u2013 This section involves joining consumer base data with gender prediction data to assign gender to each customer, which is part of the driver dataset."
        ],
        "why_separate": null,
        "file_name": "repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.ipynb"
      }
    },
    {
      "Data Pulling": {
        "line_range": "Lines 23-67",
        "evidence": [
          "\"Initializes a Fetcher object with specific configurations including job name, group name, model name, and owner.\" \u2013 This indicates the setup of a data fetching process, which is a key part of enriching the driver dataset with additional data/features.",
          "\"Executes the fetcher to run the data fetching process based on the provided configurations.\" \u2013 This confirms the execution of the data fetching process, which aligns with the Data Pulling category."
        ],
        "why_separate": null,
        "file_name": "repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.ipynb"
      }
    },
    {
      "Feature Consolidation": {
        "line_range": "Lines 17-624",
        "evidence": [
          "\"Create driver_dev_features table in BigQuery (Lines 17-216): Constructs a SQL query to drop and create a new table `driver_dev_features`. Selects various features from multiple joined tables, applying COALESCE to handle null values.\" \u2013 This indicates the merging of multiple datasets into a unified feature set, which is characteristic of Feature Consolidation.",
          "\"Create driver_oot_features table in BigQuery (Lines 219-418): Constructs a SQL query to drop and create a new table `driver_oot_features`. Similar to the previous block, selects features from multiple joined tables with COALESCE.\" \u2013 This further supports the merging of datasets into a unified feature set.",
          "\"Expand sequence features in driver_oot_features_expand_seq table (Lines 421-624): Constructs a SQL query to drop and create a new table `driver_oot_features_expand_seq`. Splits a comma-separated list into individual columns for historical receiver IDs and categories.\" \u2013 This step involves further consolidation and transformation of features, which is part of the Feature Consolidation process."
        ],
        "why_separate": "The entire block from lines 17-624 is focused on merging and transforming multiple datasets into unified feature sets, which is distinct from other ML components like Driver Creation or Data Preprocessing. There is no overlap with other components' line ranges as the focus is solely on consolidating features.",
        "file_name": "repos/ql-store-recommendation-prod/research/pipeline/02_combine.ipynb"
      }
    },
    {
      "Data Preprocessing": {
        "line_range": "Lines 21-175",
        "evidence": [
          "\"Load and concatenate multiple parquet files into a single DataFrame.\" \u2013 This indicates the initial step of data loading, which is part of preprocessing.",
          "\"Create a dictionary to map full state names to their abbreviations.\" \u2013 This is a data cleaning step, which is a common preprocessing task.",
          "\"Encode the 'sndr_prmry_addr_state' feature using LabelEncoder.\" \u2013 Encoding categorical variables is a key part of data preprocessing.",
          "\"Scale these features using StandardScaler and store the scalers.\" \u2013 Scaling numerical features is another essential preprocessing step.",
          "\"Clean and pad sequences for 'sndr_most_recent_100_merch_category' and 'sndr_most_recent_100_merch_list'.\" \u2013 Processing sequence features is part of data transformation, which falls under preprocessing.",
          "\"Save the trained Tokenizer and feature encoders/scalers to disk using pickle.\" \u2013 Saving preprocessors/transformers is included in the preprocessing component."
        ],
        "why_separate": null,
        "file_name": "repos/ql-store-recommendation-prod/research/pipeline/03_prepare_training_data.ipynb"
      }
    },
    {
      "Model Training": {
        "line_range": "Lines 140-170",
        "evidence": [
          "\"Define and compile the model\" \u2013 This indicates the setup of the model architecture and compilation, which is a key part of model training.",
          "\"Train the model using the training dataset and validate it using the validation dataset, with early stopping\" \u2013 This clearly describes the process of training the model, which is the core activity in the Model Training component."
        ],
        "why_separate": null,
        "file_name": "repos/ql-store-recommendation-prod/research/pipeline/04_training.ipynb"
      },
      "Model Packaging": {
        "line_range": "Lines 171-185",
        "evidence": [
          "\"Save the trained model in both H5 and TensorFlow SavedModel formats\" \u2013 This indicates the process of saving the trained model in deployment-ready formats, which is the essence of Model Packaging.",
          "\"Convert the trained TensorFlow model to ONNX format and save the ONNX specification\" \u2013 This further supports the packaging of the model into a format suitable for deployment."
        ],
        "why_separate": "Model Packaging is distinct from Model Training as it involves saving the trained model into specific formats for deployment, which is a separate step from the training process itself. There is no overlap with the Model Training component's line range.",
        "file_name": "repos/ql-store-recommendation-prod/research/pipeline/04_training.ipynb"
      }
    },
    {
      "Model Scoring": {
        "line_range": "Lines 25-94",
        "evidence": [
          "\"Defines a function `oot_data_eval` to evaluate out-of-time data. Loads the model and data, performs scoring, and saves the results.\" \u2013 This indicates the primary function of the code is to score the model on out-of-time data.",
          "\"Configures and submits a Spark job to Google Cloud Platform for data evaluation.\" \u2013 This shows the scoring process is executed on GCP, which is a typical setup for model scoring tasks.",
          "\"Monitors the job status and saves the job logs to a specified location.\" \u2013 This is part of the scoring process, ensuring the job completes successfully and logs are saved for review."
        ],
        "why_separate": null,
        "file_name": "repos/ql-store-recommendation-prod/research/pipeline/05_scoring_oot.ipynb"
      }
    },
    {
      "Model Evaluation": {
        "line_range": "Lines 375-731",
        "evidence": [
          "Constructs a SQL query to calculate recall metrics for different models and transaction types. \u2013 This indicates the calculation of performance metrics, which is a key part of model evaluation.",
          "Exports the results to a CSV file and generates bar plots for visualization. \u2013 This further supports the evaluation process by showing that the results are being analyzed and visualized.",
          "Constructs a SQL query to calculate recall metrics for first-time users. \u2013 This is another instance of calculating performance metrics, specific to a subset of users.",
          "Constructs a SQL query to calculate recall metrics excluding specific merchants. \u2013 This shows the evaluation of model performance under different conditions.",
          "Constructs a SQL query to calculate recall metrics for recent and older data. \u2013 This indicates the evaluation of model performance over different time periods."
        ],
        "why_separate": null,
        "file_name": "repos/ql-store-recommendation-prod/research/pipeline/06_evaluation.ipynb"
      }
    }
  ]
}