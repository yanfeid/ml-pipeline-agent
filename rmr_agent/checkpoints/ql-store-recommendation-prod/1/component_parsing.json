{
  "component_parsing": [
    {
      "Driver Creation": {
        "line_range": "Lines 1-513",
        "evidence": [
          "\"Create live_unique_merchants_train table\" (Lines 18-52) \u2013 This section involves creating a new table with merchant information by joining multiple tables and applying filters, which is part of the initial data loading and extraction process.",
          "\"Create driver_00 table\" (Lines 54-82) \u2013 This section involves creating a new table with customer and merchant interaction data, applying various filters and transformations, which is part of the initial data loading and extraction process.",
          "\"Create driver_0 table\" (Lines 84-102) \u2013 This section involves creating a new table by filtering customers based on their interaction counts and specific conditions, which is part of the initial data loading and extraction process.",
          "\"Create driver_1 table\" (Lines 105-120) \u2013 This section involves creating a new table with transaction data for specific customers and merchants, which is part of the initial data loading and extraction process.",
          "\"Create driver_positive_train_attributed table\" (Lines 124-168) \u2013 This section involves creating a new table with positive samples for training, applying various sampling and filtering techniques, which is part of the initial data loading and extraction process.",
          "\"Create driver_positive_train_organic_0 table\" (Lines 170-204) \u2013 This section involves creating a new table with organic transaction data, applying multiple filters to remove biases, which is part of the initial data loading and extraction process.",
          "\"Create driver_positive_train_organic table\" (Lines 207-244) \u2013 This section involves creating a new table with sampled organic transactions, applying additional sampling techniques, which is part of the initial data loading and extraction process.",
          "\"Create driver_positive table\" (Lines 247-277) \u2013 This section involves combining multiple positive sample tables into one, including both training and out-of-time (OOT) samples, which is part of the initial data loading and extraction process.",
          "\"Create driver_positive_training_split_0 table\" (Lines 281-307) \u2013 This section involves creating a new table with positive samples, marking some as hard negatives based on specific conditions, which is part of the initial data loading and extraction process.",
          "\"Create driver_positive_training_split table\" (Lines 310-321) \u2013 This section involves adding a count of positive samples per day to the training split table, which is part of the initial data loading and extraction process.",
          "\"Create driver_training_hard_negative table\" (Lines 324-336) \u2013 This section involves creating a new table with hard negative samples by joining with the driver_0 table, which is part of the initial data loading and extraction process.",
          "\"Create driver_training_hard_negative_downsample table\" (Lines 339-349) \u2013 This section involves downsampling the hard negative samples based on specific conditions, which is part of the initial data loading and extraction process.",
          "\"Create driver_training_uniform_negative table\" (Lines 352-364) \u2013 This section involves creating a new table with uniform negative samples by joining with the live_unique_merchants_train table, which is part of the initial data loading and extraction process.",
          "\"Create driver_training_uniform_negative_remove_window_positive table\" (Lines 367-376) \u2013 This section involves removing uniform negative samples that overlap with positive samples within a specific time window, which is part of the initial data loading and extraction process.",
          "\"Create driver_training_uniform_negative_downsample_0 table\" (Lines 379-401) \u2013 This section involves downsampling uniform negative samples based on merchant sampling probabilities, which is part of the initial data loading and extraction process.",
          "\"Create driver_training_uniform_negative_downsample table\" (Lines 404-407) \u2013 This section involves further downsampling uniform negative samples based on a specified ratio, which is part of the initial data loading and extraction process.",
          "\"Create driver_dev table\" (Lines 410-445) \u2013 This section involves combining positive and negative samples into a development dataset, including both hard and uniform negatives, which is part of the initial data loading and extraction process.",
          "\"Create driver_oot_uniform_negative_0 table\" (Lines 449-469) \u2013 This section involves creating a new table with out-of-time (OOT) uniform negative samples, which is part of the initial data loading and extraction process.",
          "\"Create driver_oot_uniform_negative table\" (Lines 472-478) \u2013 This section involves removing duplicate OOT uniform negative samples, which is part of the initial data loading and extraction process.",
          "\"Create driver_oot table\" (Lines 481-495) \u2013 This section involves combining positive and uniform negative samples into an OOT dataset, which is part of the initial data loading and extraction process.",
          "\"Create driver_simu table\" (Lines 498-506) \u2013 This section involves combining unique customer and merchant interactions from development and OOT datasets, which is part of the initial data loading and extraction process.",
          "\"Create driver_simu_consumer table\" (Lines 509-512) \u2013 This section involves creating a new table with unique customer and run date pairs from the simulation dataset, which is part of the initial data loading and extraction process."
        ],
        "why_separate": null,
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/00_driver.py"
      }
    },
    {
      "Driver Creation": {
        "line_range": "Lines 1-819",
        "evidence": [
          "\"Creates a new `driver_simu_txn_365d` table with additional date columns and transaction details.\" \u2013 This indicates the creation of a driver dataset with essential transaction details.",
          "\"Creates a new `driver_consumer_base` table with customer IDs and various run dates.\" \u2013 This shows the creation of a base table for consumers, which is a fundamental part of the driver dataset.",
          "\"Creates a new `driver_merchant_base` table with receiver IDs and run dates.\" \u2013 This indicates the creation of a base table for merchants, another essential part of the driver dataset.",
          "\"Creates a new `driver_elig_save_365d_category` table with customer save event details and merchant category information.\" \u2013 This shows the creation of a table with specific event details, contributing to the driver dataset."
        ],
        "why_separate": "The entire file is focused on creating various tables that form the driver dataset. Each table creation step is part of the overall process of assembling the driver dataset, which is the initial and fundamental step in the ML pipeline. There is no overlap with other components such as feature engineering or data preprocessing, as the primary focus here is on creating the foundational dataset.",
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.py"
      }
    },
    {
      "Data Pulling": {
        "line_range": "Lines 34-67",
        "evidence": [
          "\"Initializes a Fetcher object and sets various properties\" \u2013 This indicates the setup of a data fetching process.",
          "\"Configures the Fetcher object with Google Cloud Platform settings and specifies locations for driver and data files\" \u2013 This shows the configuration of the Fetcher to pull data from specified locations.",
          "\"Specifies the variables to be fetched and sets the split ratio for training data\" \u2013 This further confirms the fetching of specific data variables.",
          "\"Executes the Fetcher to fetch the data based on the provided configurations\" \u2013 This is the actual execution of the data pulling process."
        ],
        "why_separate": null,
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.py"
      },
      "Driver Creation": {
        "line_range": "Lines 14-73",
        "evidence": [
          "\"Exports data from a BigQuery table to Google Cloud Storage in Parquet format\" \u2013 This indicates the creation of a driver dataset by exporting data.",
          "\"Creates or replaces an external table in BigQuery using the data stored in Google Cloud Storage in Parquet format\" \u2013 This shows the creation of a driver dataset in BigQuery."
        ],
        "why_separate": "Verification of ZERO overlap with other components' line ranges: The lines for Data Pulling (34-67) do not overlap with the lines for Driver Creation (14-22, 68-73).\nJustification for why this should be split from the other code: The process of exporting data to Google Cloud Storage and creating an external table in BigQuery is distinct from the process of fetching additional data using the Fetcher object. The former is about creating the initial driver dataset, while the latter is about enriching the dataset with additional features.",
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.py"
      }
    },
    {
      "Feature Consolidation": {
        "line_range": "Lines 1-642",
        "evidence": [
          "\"Create and populate driver_dev_features table (Lines 17-216): Constructs a SQL query to drop and create a new table `driver_dev_features`. Selects various features from multiple joined tables, using COALESCE to handle null values.\" \u2013 This indicates the merging of multiple datasets into a unified feature set.",
          "\"Create and populate driver_oot_features table (Lines 219-418): Constructs a SQL query to drop and create a new table `driver_oot_features`. Similar to the previous block, selects various features from multiple joined tables, using COALESCE to handle null values.\" \u2013 This further supports the merging of datasets into a unified feature set.",
          "\"Expand sequence features in driver_oot_features_expand_seq table (Lines 421-624): Constructs a SQL query to drop and create a new table `driver_oot_features_expand_seq`. Splits a comma-separated list into multiple columns for historical receiver IDs and categories.\" \u2013 This step involves further consolidation and transformation of features into a unified dataset."
        ],
        "why_separate": null,
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/02_combine.py"
      }
    },
    {
      "Data Preprocessing": {
        "line_range": "Lines 1-185",
        "evidence": [
          "**[Setup and Initialization] (Lines 1-20):**\n- \"Imports necessary libraries and modules.\"\n- \"Sets up the model version and directory paths.\"\n- \"Creates directories if they do not exist.\"\n- \"Saves the current model version to a file.\"\n- These steps are preparatory but are part of the overall preprocessing setup.",
          "**[Load and Concatenate Data] (Lines 21-30):**\n- \"Reads parquet files from a specified directory.\"\n- \"Concatenates the data from these files into a single DataFrame.\"\n- This is part of data loading and initial transformation.",
          "**[State Abbreviation Mapping] (Lines 31-94):**\n- \"Defines a mapping from state names to their abbreviations.\"\n- \"Creates a function to clean state data by converting full state names to abbreviations.\"\n- \"Applies this function to the state column in the data.\"\n- This involves cleaning and transforming categorical data.",
          "**[Encode Categorical Features] (Lines 95-112):**\n- \"Initializes a LabelEncoder.\"\n- \"Encodes the state column and stores the encoder mappings.\"\n- \"Encodes other specified categorical features and stores their encoder mappings.\"\n- This involves encoding categorical variables.",
          "**[Scale Numerical Features] (Lines 113-134):**\n- \"Defines a list of numerical features to be scaled.\"\n- \"Scales each numerical feature using StandardScaler.\"\n- \"Stores the mean and standard deviation for each feature.\"\n- This involves scaling numerical features.",
          "**[Process Sequential Data] (Lines 135-153):**\n- \"Cleans and processes sequential data related to recent merchant categories and lists.\"\n- \"Pads sequences to a fixed length.\"\n- \"Tokenizes text data and converts it to sequences.\"\n- \"Renames columns and drops unnecessary ones.\"\n- This involves processing and transforming sequential data.",
          "**[Save Transformed Data] (Lines 154-166):**\n- \"Defines a function to write data chunks to parquet files.\"\n- \"Splits the data into chunks and writes each chunk to a parquet file in the output directory.\"\n- This involves saving the transformed data.",
          "**[Export Transformers and Encoders] (Lines 167-175):**\n- \"Saves the tokenizer, categorical feature encoders, and numerical feature scalers to files using pickle.\"\n- This involves saving the preprocessing objects for future use."
        ],
        "why_separate": "There is no overlap with other components' line ranges.\nThis entire section is dedicated to cleaning, transforming, encoding, and scaling data, which are all tasks under the Data Preprocessing category. The steps are sequential and interdependent, making it a cohesive preprocessing workflow node.",
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/03_prepare_training_data.py"
      }
    },
    {
      "Model Training": {
        "line_range": "Lines 140-268",
        "evidence": [
          "Defines the model architecture using the DIN model and compiles it with specified parameters and optimizer. \u2013 This indicates the setup of the model architecture and compilation, which are key steps in model training.",
          "Trains the model using the training dataset and validates it using the validation dataset, with early stopping. \u2013 This confirms the actual training process, including validation and early stopping, which are essential parts of model training."
        ],
        "why_separate": null,
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/04_training.py"
      },
      "Model Packaging": {
        "line_range": "Lines 140-268",
        "evidence": [
          "Saves the trained model in both H5 and TensorFlow SavedModel formats. \u2013 This indicates the saving of the trained model in deployment-ready formats, which is a key aspect of model packaging.",
          "Converts the trained TensorFlow model to ONNX format and saves the ONNX model specification. \u2013 This conversion to ONNX format is part of preparing the model for deployment, further supporting the model packaging classification."
        ],
        "why_separate": "Verification of ZERO overlap with other components' line ranges: The lines for model packaging (171-186) do not overlap with the lines for model training (140-170) or model scoring (230-268). Justification for why this should be split from the other code: Model packaging involves saving and converting the trained model into formats suitable for deployment, which is distinct from the training process itself. This step ensures the model is ready for deployment and use in production environments.",
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/04_training.py"
      },
      "Model Scoring": {
        "line_range": "Lines 140-268",
        "evidence": [
          "Prepares test data for prediction by extracting relevant features and formatting them appropriately. \u2013 This preparation of test data is a precursor to scoring.",
          "Uses the production model to make predictions on the test data. \u2013 This is the core of model scoring, where the trained model is used to make predictions on unseen data.",
          "Loads the ONNX model and runs inference on the test data, comparing results with the TensorFlow model predictions. \u2013 This further confirms the scoring process, including inference and comparison of results."
        ],
        "why_separate": "Verification of ZERO overlap with other components' line ranges: The lines for model scoring (230-268) do not overlap with the lines for model training (140-170) or model packaging (171-186). Justification for why this should be split from the other code: Model scoring involves using the trained and packaged model to make predictions on new data, which is a distinct step from both training and packaging. This step is crucial for evaluating the model's performance on unseen data and is a separate workflow node in the ML pipeline.",
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/04_training.py"
      }
    },
    {
      "Model Scoring": {
        "line_range": "Lines 1-104",
        "evidence": [
          "Defines a function `oot_data_eval` to evaluate out-of-time (OOT) data. Scores the data using the specified models and saves the results. \u2013 This indicates that the primary function of this code is to score the OOT data using the trained models.",
          "Configures and submits a Spark job to Google Cloud Platform (GCP) for scoring the OOT data. \u2013 This further supports that the main task is scoring the data, which is a key part of the Model Scoring component.",
          "Creates or replaces an external table in BigQuery using the scored OOT data stored in GCP. \u2013 This step is part of the process of handling the results of the scoring, which is integral to the Model Scoring component."
        ],
        "why_separate": null,
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/05_scoring_oot.py"
      }
    },
    {
      "Model Evaluation": {
        "line_range": "Lines 1-731",
        "evidence": [
          "\"Calculate recall metrics for different models (Lines 375-430)\" \u2013 This indicates the calculation of performance metrics, which is a key part of model evaluation.",
          "\"Save recall metrics to CSV and plot results (Lines 431-447)\" \u2013 Saving evaluation results and visualizing them is part of the evaluation process.",
          "\"Calculate recall metrics for first-time users (Lines 448-513)\" \u2013 This is another instance of calculating performance metrics for a specific subset of data.",
          "\"Save first-time user recall metrics to CSV and plot results (Lines 514-530)\" \u2013 Again, saving and visualizing evaluation results.",
          "\"Calculate recall metrics excluding specific merchants (Lines 531-590)\" \u2013 More performance metric calculations for another subset.",
          "\"Save recall metrics excluding specific merchants to CSV and plot results (Lines 591-607)\" \u2013 Saving and visualizing these specific evaluation results.",
          "\"Calculate recall metrics for recent transactions (Lines 608-727)\" \u2013 Performance metric calculations for recent transactions.",
          "\"Save recent transaction recall metrics to CSV (Lines 728-731)\" \u2013 Saving the evaluation results for recent transactions."
        ],
        "why_separate": "Verification of ZERO overlap with other components' line ranges: The identified lines (375-731) do not overlap with the earlier lines (4-373) which are primarily focused on data preparation and BigQuery table creation. Justification for why this should be split from the other code: The lines from 375-731 are focused on evaluating the model's performance by calculating recall metrics, saving these metrics, and visualizing them. This is distinct from the earlier sections which are focused on preparing the data for evaluation. This split results in one of the ML component categories defined above: The activities described in lines 375-731 align with the \"Model Evaluation\" category, as they involve calculating performance metrics and analyzing the results.",
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/06_evaluation.py"
      }
    }
  ]
}