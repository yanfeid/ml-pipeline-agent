{
  "summaries": {
    "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/00_driver.py": "**Load YAML configuration file** (Lines 4-13):\n- Defines a function to load a YAML file and returns its content.\n- Loads a configuration file from a specified path.\n\n**Set BigQuery project dataset prefix** (Lines 14-16):\n- Extracts the BigQuery project dataset prefix from the loaded configuration.\n\n**Create live unique merchants training table** (Lines 18-52):\n- Drops the existing table if it exists.\n- Creates a new table with unique merchants by joining multiple tables and filtering based on specific conditions.\n\n**Create driver_00 table** (Lines 54-81):\n- Drops the existing table if it exists.\n- Creates a new table by joining the live unique merchants training table with another table and applying various filters.\n\n**Create driver_0 table** (Lines 84-102):\n- Drops the existing table if it exists.\n- Creates a new table by filtering the driver_00 table based on the number of distinct placements and specific conditions.\n\n**Create driver_1 table** (Lines 105-120):\n- Drops the existing table if it exists.\n- Creates a new table by joining the live unique merchants training table with another table and applying various filters.\n\n**Create driver_positive_train_attributed table** (Lines 124-168):\n- Drops the existing table if it exists.\n- Creates a new table with positive samples by applying various filters and sampling techniques.\n\n**Create driver_positive_train_organic_0 table** (Lines 170-204):\n- Drops the existing table if it exists.\n- Creates a new table with organic positive samples by applying various filters to remove biases.\n\n**Create driver_positive_train_organic table** (Lines 207-244):\n- Drops the existing table if it exists.\n- Creates a new table with organic positive samples by applying various filters and sampling techniques.\n\n**Create driver_positive table** (Lines 247-277):\n- Drops the existing table if it exists.\n- Combines multiple tables to create a final table with positive samples, including attributed transactions, organic transactions, and saves.\n\n**Create driver_positive_training_split_0 table** (Lines 281-307):\n- Drops the existing table if it exists.\n- Creates a new table by splitting the positive samples into training and validation sets and identifying hard negatives.\n\n**Create driver_positive_training_split table** (Lines 310-321):\n- Drops the existing table if it exists.\n- Creates a new table by adding the count of positive samples per day to the training split table.\n\n**Create driver_training_hard_negative table** (Lines 324-336):\n- Drops the existing table if it exists.\n- Creates a new table with hard negative samples by joining the training split table with another table and applying various filters.\n\n**Create driver_training_hard_negative_downsample table** (Lines 339-349):\n- Drops the existing table if it exists.\n- Creates a new table by downsampling the hard negative samples based on specific conditions.\n\n**Create driver_training_uniform_negative table** (Lines 352-364):\n- Drops the existing table if it exists.\n- Creates a new table with uniform negative samples by joining the training split table with the live unique merchants training table.\n\n**Create driver_training_uniform_negative_remove_window_positive table** (Lines 367-376):\n- Drops the existing table if it exists.\n- Creates a new table by removing uniform negative samples that overlap with positive samples within a specific time window.\n\n**Create driver_training_uniform_negative_downsample_0 table** (Lines 379-401):\n- Drops the existing table if it exists.\n- Creates a new table by downsampling the uniform negative samples based on merchant sampling probability.\n\n**Create driver_training_uniform_negative_downsample table** (Lines 404-407):\n- Drops the existing table if it exists.\n- Creates a new table by further downsampling the uniform negative samples based on the ratio of uniform negatives to positives.\n\n**Create driver_dev table** (Lines 410-445):\n- Drops the existing table if it exists.\n- Combines multiple tables to create a final development table with both positive and negative samples, including hard and uniform negatives.\n\n**Create driver_oot_uniform_negative_0 table** (Lines 449-469):\n- Drops the existing table if it exists.\n- Creates a new table with out-of-time (OOT) uniform negative samples by joining the positive samples with the live unique merchants training table.\n\n**Create driver_oot_uniform_negative table** (Lines 472-478):\n- Drops the existing table if it exists.\n- Creates a new table by removing duplicate OOT uniform negative samples.\n\n**Create driver_oot table** (Lines 481-495):\n- Drops the existing table if it exists.\n- Combines multiple tables to create a final OOT table with both positive and negative samples.\n\n**Create driver_simu table** (Lines 498-506):\n- Drops the existing table if it exists.\n- Creates a new table with distinct customer and receiver IDs and run dates from the development and OOT tables.\n\n**Create driver_simu_consumer table** (Lines 509-512):\n- Drops the existing table if it exists.\n- Creates a new table with distinct customer IDs and run dates from the simulation table.",
    "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.py": "**Load YAML configuration file (Lines 4-13):**\n- Defines a function to load a YAML file.\n- Loads a configuration file and assigns it to a variable.\n\n**Extract BigQuery project dataset prefix (Lines 14-16):**\n- Extracts the BigQuery project dataset prefix from the loaded configuration.\n\n**Create driver_simu_txn_365d table (Lines 17-40):**\n- Drops the existing table if it exists.\n- Creates a new table with transaction data joined with payment data, including various date intervals.\n\n**Create driver_simu_txn_365d_agg table (Lines 43-55):**\n- Drops the existing table if it exists.\n- Creates a new aggregated table with transaction counts and amounts over different time intervals.\n\n**Create driver_consumer_base table (Lines 58-66):**\n- Drops the existing table if it exists.\n- Creates a new table with consumer base data including various date intervals.\n\n**Create driver_consumer_base_txn_5k_merch_category table (Lines 69-93):**\n- Drops the existing table if it exists.\n- Creates a new table with consumer base transaction data joined with merchant category data.\n\n**Create driver_consumer_base_last_10_txn table (Lines 96-175):**\n- Drops the existing table if it exists.\n- Creates a new table with the last 10 transactions for each consumer, including average transaction amounts.\n\n**Create driver_consumer_base_all_history_array_0 table (Lines 178-185):**\n- Drops the existing table if it exists.\n- Creates a new table with the most recent 100 transactions for each consumer.\n\n**Create driver_consumer_base_all_history_array table (Lines 188-194):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated lists of the most recent 100 merchants and categories for each consumer.\n\n**Create driver_combine_category table (Lines 198-203):**\n- Drops the existing table if it exists.\n- Creates a new table combining driver simulation data with merchant category data.\n\n**Create driver_combine_category_agg_0 table (Lines 206-219):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated transaction data by category over different time intervals.\n\n**Create driver_combine_category_agg_1 table (Lines 222-231):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated transaction counts over different time intervals.\n\n**Create driver_combine_category_agg_2 table (Lines 234-257):**\n- Drops the existing table if it exists.\n- Creates a new table with average transaction amounts by category over different time intervals.\n\n**Create driver_combine_category_agg_3 table (Lines 260-286):**\n- Drops the existing table if it exists.\n- Creates a new table combining consumer and category transaction data with aggregated category data.\n\n**Create driver_combine_category_agg_4 table (Lines 289-302):**\n- Drops the existing table if it exists.\n- Creates a new table with transaction frequency ranks by category over different time intervals.\n\n**Create driver_combine_category_agg_5 table (Lines 305-464):**\n- Drops the existing table if it exists.\n- Creates a new table with the top 3 frequent merchant categories over different time intervals.\n\n**Create driver_merchant_base table (Lines 468-476):**\n- Drops the existing table if it exists.\n- Creates a new table with merchant base data including a 30-day interval.\n\n**Create driver_merchant_base_txn_30d table (Lines 479-506):**\n- Drops the existing table if it exists.\n- Creates a new table with merchant transaction data over the last 30 days.\n\n**Create driver_merchant_base_txn_30d_filter_sndr table (Lines 509-523):**\n- Drops the existing table if it exists.\n- Creates a new table filtering merchant transactions by sender account type.\n\n**Create driver_merchant_base_price_agg table (Lines 526-551):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated price statistics for merchants over the last 30 days.\n\n**Create driver_merchant_base_sales_agg table (Lines 554-566):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated sales statistics for merchants over the last 30 days.\n\n**Create driver_elig_save_365d_category table (Lines 569-616):**\n- Drops the existing table if it exists.\n- Creates a new table with consumer save event data joined with merchant category data.\n\n**Create driver_elig_save_agg_00 table (Lines 619-630):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated save counts over different time intervals.\n\n**Create driver_elig_save_agg_0 table (Lines 633-643):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated save counts for consumers over different time intervals.\n\n**Create driver_elig_save_agg_1 table (Lines 646-717):**\n- Drops the existing table if it exists.\n- Creates a new table with the last 5 save events for each consumer.\n\n**Create driver_elig_save_agg_2 table (Lines 720-731):**\n- Drops the existing table if it exists.\n- Creates a new table with aggregated save counts by category over different time intervals.\n\n**Create driver_elig_save_agg_3 table (Lines 734-751):**\n- Drops the existing table if it exists.\n- Creates a new table combining consumer and category save data with aggregated save data.\n\n**Create driver_merchant_base_click_save table (Lines 754-793):**\n- Drops the existing table if it exists.\n- Creates a new table with merchant save event statistics over the last 30 days.\n\n**Create driver_consumer_base_gender table (Lines 796-817):**\n- Drops the existing table if it exists.\n- Creates a new table with consumer gender data based on first name predictions.",
    "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.py": "**[Import and authenticate user] (Lines 4-5):**\n- Imports a cloud module and authenticates the user.\n\n**[Set environment variable] (Line 9):**\n- Sets an environment variable to disable a specific development feature.\n\n**[Export data to Google Cloud Storage] (Lines 14-22):**\n- Exports data from a BigQuery table to Google Cloud Storage in Parquet format.\n\n**[Import modules and set up fetcher] (Lines 23-40):**\n- Imports necessary modules and sets up a Fetcher object with various configurations such as stage, sequence number, job name, and other metadata.\n\n**[Configure GCP settings for fetcher] (Lines 43-49):**\n- Configures Google Cloud Platform settings for the Fetcher, including project ID, bucket name, and data locations.\n\n**[Specify variables and split ratio] (Lines 50-63):**\n- Specifies the variables to be fetched and sets the split ratio for training data.\n\n**[Run fetcher] (Line 67):**\n- Executes the fetcher to retrieve the specified data.\n\n**[Create external table in BigQuery] (Lines 68-73):**\n- Creates or replaces an external table in BigQuery using data stored in Google Cloud Storage in Parquet format.",
    "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/02_combine.py": "**[Load YAML configuration file] (Lines 4-13):**\n- Defines a function to load a YAML file.\n- Attempts to load a configuration file and assigns its content to a variable.\n\n**[Extract BigQuery project dataset prefix from configuration] (Lines 14-16):**\n- Checks if the configuration is loaded.\n- Extracts the BigQuery project dataset prefix from the configuration.\n\n**[Create and populate driver_dev_features table] (Lines 17-216):**\n- Constructs a SQL query to drop the existing `driver_dev_features` table if it exists.\n- Creates a new `driver_dev_features` table with various features, using multiple LEFT JOINs to combine data from different tables.\n\n**[Create and populate driver_oot_features table] (Lines 219-418):**\n- Constructs a SQL query to drop the existing `driver_oot_features` table if it exists.\n- Creates a new `driver_oot_features` table with similar features as `driver_dev_features`, using multiple LEFT JOINs to combine data from different tables.\n\n**[Expand sequence features in driver_oot_features_expand_seq table] (Lines 421-625):**\n- Constructs a SQL query to drop the existing `driver_oot_features_expand_seq` table if it exists.\n- Creates a new `driver_oot_features_expand_seq` table by expanding sequence features from `driver_oot_features` table.\n\n**[Export driver_dev_features table to Google Cloud Storage] (Lines 631-639):**\n- Constructs a SQL query to export the `driver_dev_features` table to Google Cloud Storage in Parquet format.",
    "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/03_prepare_training_data.py": "**[Setup and initialization] (Lines 1-20):**\n- Imports necessary libraries and modules.\n- Sets up the model version and directories for saving artifacts.\n- Saves the current model version using pickle.\n\n**[Load and concatenate data] (Lines 21-30):**\n- Reads parquet files from a specified directory.\n- Concatenates the data from these files into a single DataFrame.\n\n**[State abbreviation mapping] (Lines 31-94):**\n- Creates a dictionary to map state names to their abbreviations.\n- Defines a function to clean state data by converting full state names to abbreviations or keeping abbreviations as is.\n- Applies this function to the state column in the DataFrame.\n\n**[Encode categorical features] (Lines 95-112):**\n- Encodes the state column using LabelEncoder and saves the encoder mappings.\n- Encodes other specified categorical features and saves their encoder mappings.\n\n**[Scale numerical features] (Lines 113-134):**\n- Defines a list of numerical features to be scaled.\n- Scales these features using StandardScaler and saves the scaling parameters.\n\n**[Process sequence data] (Lines 135-153):**\n- Cleans and processes sequence data for recent merchant categories and lists.\n- Pads sequences to a fixed length and adds them to the DataFrame.\n- Tokenizes merchant list data and adds the tokenized sequences to the DataFrame.\n- Drops the original sequence columns from the DataFrame.\n\n**[Save processed data in chunks] (Lines 154-166):**\n- Defines a function to write DataFrame chunks to parquet files.\n- Splits the DataFrame into chunks and writes each chunk to a specified directory.\n\n**[Export feature transformers] (Lines 167-175):**\n- Saves the tokenizer, categorical feature encoders, and numerical feature scalers using pickle.",
    "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/04_training.py": "**Import necessary libraries and modules (Lines 2-11):**\n- Imports essential libraries and modules for data processing, machine learning, and model training.\n\n**Configure GPU settings (Lines 12-14):**\n- Lists available GPUs and sets memory growth to avoid memory allocation issues.\n\n**Load YAML configuration file (Lines 15-21):**\n- Defines a function to load a YAML file and handle file not found errors.\n\n**Load configuration and model version (Lines 22-34):**\n- Loads the YAML configuration file and model version, sets up directories for saving models and artifacts.\n\n**Load and concatenate parquet files (Lines 36-43):**\n- Reads multiple parquet files from a directory and concatenates them into a single DataFrame.\n\n**Load feature encoders and tokenizers (Lines 44-47):**\n- Loads pre-trained categorical feature encoders and tokenizers from pickle files.\n\n**Split data into training and validation sets (Lines 48-49):**\n- Splits the data into training and validation sets based on a 'split' column.\n\n**Define numeric, categorical, and ID feature names (Lines 52-74):**\n- Lists the names of numeric, receiver ID, and categorical features used in the model.\n\n**Function to prepare data for model input (Lines 75-106):**\n- Defines a function to create feature columns, prepare input data, and return feature names and target values.\n\n**Prepare training and validation data (Lines 107-109):**\n- Calls the data preparation function to get training and validation data along with feature columns and behavior features.\n\n**Data generator function for batching (Lines 110-120):**\n- Defines a generator function to yield batches of data for training and validation.\n\n**Create TensorFlow datasets for training and validation (Lines 121-137):**\n- Creates TensorFlow datasets from the data generator for training and validation, with shuffling.\n\n**Set up distributed training strategy (Lines 140-141):**\n- Configures a distributed training strategy for using multiple GPUs.\n\n**Compile and configure the DIN model (Lines 142-155):**\n- Compiles the DIN model with specified configurations and an Adam optimizer.\n\n**Early stopping callback (Lines 156-162):**\n- Sets up an early stopping callback to monitor validation loss and restore the best weights.\n\n**Train the model (Lines 163-170):**\n- Trains the model using the training dataset and validates it using the validation dataset, with early stopping.\n\n**Save the trained model (Lines 171-175):**\n- Saves the trained model in both H5 and TensorFlow SavedModel formats.\n\n**Convert TensorFlow model to ONNX format (Lines 176-186):**\n- Converts the TensorFlow model to ONNX format and saves the ONNX specification.\n\n**Prepare and save the ONNX model (Lines 187-229):**\n- Prepares the ONNX model with preprocessing layers and saves it to a specified path.\n\n**Generate test data and save to JSON (Lines 230-248):**\n- Generates test data, prepares it for model input, and saves it to a JSON file.\n\n**Convert and test ONNX model (Lines 251-267):**\n- Converts the TensorFlow model to ONNX format, runs inference with ONNX Runtime, and compares results with TensorFlow predictions.",
    "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/05_scoring_oot.py": "**[Import necessary modules and set up configuration] (Lines 2-18):**\n- Import various modules and libraries required for the script.\n- Set the working directory and check if the configuration is correctly set up.\n\n**[Load model version and set paths] (Lines 19-24):**\n- Load the current model version from a file.\n- Define paths for the model version and exported models.\n\n**[Define function for out-of-time (OOT) data evaluation] (Lines 25-48):**\n- Define a function `oot_data_eval` to evaluate OOT data using a model.\n- Load OOT data and model files, create a scoring DataFrame, and save the results.\n\n**[Set paths and configurations for model scoring] (Lines 49-53):**\n- Define local and GCP paths for model scoring and OOT data.\n- Retrieve model name from the configuration.\n\n**[Prepare model files and scoring list] (Lines 54-69):**\n- Iterate through local model files, load model specifications, and prepare lists of model paths and scoring outputs.\n\n**[Submit Spark job to GCP for scoring] (Lines 70-86):**\n- Create a Spark job on GCP to run the `oot_data_eval` function with necessary packages and configurations.\n- Submit the job and retrieve the job ID.\n\n**[Check job status and save logs] (Lines 87-94):**\n- Wait for the Spark job to complete and check its status.\n- Save the job log to a file.\n\n**[Create external table in BigQuery] (Lines 99-104):**\n- Create or replace an external table in BigQuery using the scored OOT data stored in GCS.",
    "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/06_evaluation.py": "**Load model version and set up directories (Lines 4-12):**\n- Import necessary libraries.\n- Load the current model version from a file.\n- Set up paths for model artifacts and evaluation readouts.\n- Create directories if they do not exist.\n\n**Load configuration file (Lines 13-25):**\n- Define a function to load a YAML file.\n- Load the configuration file and extract the BigQuery project dataset prefix.\n\n**Create driver_oot_txn_365d table (Lines 26-44):**\n- Drop the existing table if it exists.\n- Create a new table by joining driver_oot with transaction data to get the last purchase timestamp.\n\n**Create driver_oot_txn_save_365d table (Lines 47-65):**\n- Drop the existing table if it exists.\n- Create a new table by joining driver_oot_txn_365d with offer data to get the last save date.\n\n**Create mlv2_gpt_similar_map_snapshot table (Lines 68-137):**\n- Drop the existing table if it exists.\n- Create a new table with similar merchant mappings by joining with live_unique_merchants_train.\n\n**Create mlv2_gpt_similar_map_snapshot_1 table (Lines 140-193):**\n- Drop the existing table if it exists.\n- Create a new table by concatenating similar merchant IDs into a single list and splitting them into separate columns.\n\n**Create driver_oot_txn_save_365d_similar table (Lines 196-207):**\n- Drop the existing table if it exists.\n- Create a new table by joining driver_oot_txn_save_365d with mlv2_gpt_similar_map_snapshot_1 and adding a similar_to column.\n\n**Create driver_oot_txn_save_365d_similar_dedup table (Lines 209-247):**\n- Drop the existing table if it exists.\n- Create a new table by deduplicating similar merchants and ranking them.\n\n**Create driver_oot_two_tower_similar_score table (Lines 249-336):**\n- Drop the existing table if it exists.\n- Create a new table by calculating the dot product of customer and merchant embeddings.\n\n**Create driver_oot_hueristic_model_comparison table (Lines 339-372):**\n- Drop the existing table if it exists.\n- Create a new table by comparing heuristic model scores and ranking them.\n\n**Calculate recall metrics and save results (Lines 375-430):**\n- Calculate recall metrics for different models and save the results to a CSV file.\n- Generate bar plots for the recall metrics and save the plots as an image.\n\n**Calculate recall metrics for specific category and save results (Lines 448-530):**\n- Calculate recall metrics for different models for a specific category and save the results to a CSV file.\n- Generate bar plots for the recall metrics and save the plots as an image.\n\n**Calculate recall metrics excluding specific merchants and save results (Lines 532-607):**\n- Calculate recall metrics for different models excluding specific merchants and save the results to a CSV file.\n- Generate bar plots for the recall metrics and save the plots as an image.\n\n**Calculate recall metrics for recent data and save results (Lines 608-731):**\n- Calculate recall metrics for different models for recent data and save the results to a CSV file.\n- Generate bar plots for the recall metrics and save the plots as an image."
  }
}