{
  "verified_components": [
    {
      "Driver Creation": {
        "line_range": "1-513",
        "evidence": [
<<<<<<< HEAD
          "\"Create live unique merchants training table (Lines 18-52): Drops the existing table if it exists. Creates a new table with unique merchants by joining multiple tables and applying filters.\" \u2013 This indicates the creation of a foundational dataset by joining multiple tables and applying filters, which is a key aspect of driver creation.",
          "\"Create driver_00 table (Lines 54-81): Drops the existing table if it exists. Creates a new table with customer and merchant interaction data, applying various filters and transformations.\" \u2013 This step involves creating a new dataset with specific interactions, which is part of the initial data loading and transformation process.",
          "\"Create driver_positive_training_split_0 table (Lines 281-307): Drops the existing table if it exists. Creates a new table by splitting positive training samples into hard and uniform negatives.\" \u2013 This step involves creating a new dataset by splitting existing data, which is part of the driver creation process.",
          "\"Create driver_dev table (Lines 410-445): Drops the existing table if it exists. Combines positive and negative samples into a development dataset, including both hard and uniform negatives.\" \u2013 This step involves combining various samples into a development dataset, which is a key part of creating the driver dataset.",
          "\"Create driver_oot table (Lines 481-495): Drops the existing table if it exists. Combines positive and negative OOT samples into a single table.\" \u2013 This step involves combining out-of-time samples into a single dataset, which is part of the driver creation process."
=======
          "\"Create live unique merchants training table (Lines 18-52): Drops the existing table if it exists. Creates a new table with unique merchant data, joining multiple sources and filtering based on specific criteria.\" \u2013 This indicates the creation of a foundational dataset by joining multiple sources and applying filters.",
          "\"Create driver_00 table (Lines 54-83): Drops the existing table if it exists. Creates a new table with customer and merchant interaction data, applying various filters and transformations.\" \u2013 This shows the creation of another foundational dataset with specific transformations.",
          "\"Create driver_positive_train_attributed table (Lines 124-168): Drops the existing table if it exists. Creates a new table with positive training samples, applying various transformations and sampling techniques.\" \u2013 This involves creating a dataset with positive training samples, which is a key part of the driver dataset.",
          "\"Create driver_dev table (Lines 410-445): Drops the existing table if it exists. Combines positive and negative samples into a development table, assigning target labels.\" \u2013 This step combines various samples into a development table, which is essential for the driver dataset.",
          "\"Create driver_oot table (Lines 481-495): Drops the existing table if it exists. Combines positive and negative OOT samples into a single table, assigning target labels.\" \u2013 This step combines out-of-time samples into a single table, which is part of the driver dataset creation."
>>>>>>> origin/main
        ],
        "why_separate": null,
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/00_driver.py"
      }
    },
    {
<<<<<<< HEAD
      "Feature Engineering": {
        "evidence": [
          "Added manually during verification"
        ],
        "why_separate": "Added manually during verification",
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.py",
        "line_range": "1-819"
=======
      "Driver Creation": {
        "line_range": "1-819",
        "evidence": [
          "\"Create driver_simu_txn_365d table\" (Lines 17-40) \u2013 This section involves creating a table with transaction data and additional date fields, which is a part of the initial data loading and extraction process.",
          "\"Create driver_simu_txn_365d_agg table\" (Lines 43-55) \u2013 This section involves creating an aggregated table with transaction counts and amounts over different time intervals, which is part of the initial data preparation.",
          "\"Create driver_consumer_base table\" (Lines 58-66) \u2013 This section involves creating a table with consumer base data and additional date fields, which is part of the initial data loading and extraction process.",
          "\"Create driver_consumer_base_txn_5k_merch_category table\" (Lines 69-93) \u2013 This section involves creating a table with consumer base transaction data, including merchant category information, which is part of the initial data preparation.",
          "\"Create driver_consumer_base_last_10_txn table\" (Lines 96-175) \u2013 This section involves creating a table with the last 10 transactions for each consumer, including average transaction amounts, which is part of the initial data preparation.",
          "\"Create driver_consumer_base_all_history_array_0 table\" (Lines 178-185) \u2013 This section involves creating a table with the most recent 100 transactions for each consumer, which is part of the initial data preparation.",
          "\"Create driver_consumer_base_all_history_array table\" (Lines 188-194) \u2013 This section involves creating a table with aggregated lists of the most recent 100 merchants and categories for each consumer, which is part of the initial data preparation.",
          "\"Create driver_combine_category table\" (Lines 198-203) \u2013 This section involves creating a table combining driver simulation data with merchant category information, which is part of the initial data preparation.",
          "\"Create driver_combine_category_agg_0 table\" (Lines 206-219) \u2013 This section involves creating a table with aggregated transaction data by category over different time intervals, which is part of the initial data preparation.",
          "\"Create driver_combine_category_agg_1 table\" (Lines 222-231) \u2013 This section involves creating a table with aggregated transaction counts over different time intervals, which is part of the initial data preparation.",
          "\"Create driver_combine_category_agg_2 table\" (Lines 234-257) \u2013 This section involves creating a table with average transaction amounts by category over different time intervals, which is part of the initial data preparation.",
          "\"Create driver_combine_category_agg_3 table\" (Lines 260-286) \u2013 This section involves creating a table combining consumer and category transaction data, which is part of the initial data preparation.",
          "\"Create driver_combine_category_agg_4 table\" (Lines 289-302) \u2013 This section involves creating a table with transaction frequency ranks by category over different time intervals, which is part of the initial data preparation.",
          "\"Create driver_combine_category_agg_5 table\" (Lines 305-464) \u2013 This section involves creating a table with the top 3 frequent merchant categories over different time intervals, which is part of the initial data preparation.",
          "\"Create driver_merchant_base table\" (Lines 468-476) \u2013 This section involves creating a table with merchant base data and additional date fields, which is part of the initial data preparation.",
          "\"Create driver_merchant_base_txn_30d table\" (Lines 479-506) \u2013 This section involves creating a table with merchant transaction data over the last 30 days, which is part of the initial data preparation.",
          "\"Create driver_merchant_base_txn_30d_filter_sndr table\" (Lines 509-523) \u2013 This section involves creating a table filtering merchant transactions by sender account type, which is part of the initial data preparation.",
          "\"Create driver_merchant_base_price_agg table\" (Lines 526-551) \u2013 This section involves creating a table with aggregated price statistics for merchants over the last 30 days, which is part of the initial data preparation.",
          "\"Create driver_merchant_base_sales_agg table\" (Lines 554-566) \u2013 This section involves creating a table with aggregated sales statistics for merchants over the last 30 days, which is part of the initial data preparation.",
          "\"Create driver_elig_save_365d_category table\" (Lines 569-616) \u2013 This section involves creating a table with consumer save event data and merchant category information, which is part of the initial data preparation.",
          "\"Create driver_elig_save_agg_00 table\" (Lines 619-630) \u2013 This section involves creating a table with aggregated save event counts over different time intervals, which is part of the initial data preparation.",
          "\"Create driver_elig_save_agg_0 table\" (Lines 633-643) \u2013 This section involves creating a table with aggregated save event counts for consumers over different time intervals, which is part of the initial data preparation.",
          "\"Create driver_elig_save_agg_1 table\" (Lines 646-717) \u2013 This section involves creating a table with the last 5 save events for each consumer, which is part of the initial data preparation.",
          "\"Create driver_elig_save_agg_2 table\" (Lines 720-731) \u2013 This section involves creating a table with aggregated save event counts by category over different time intervals, which is part of the initial data preparation.",
          "\"Create driver_elig_save_agg_3 table\" (Lines 734-751) \u2013 This section involves creating a table combining consumer and category save event data, which is part of the initial data preparation.",
          "\"Create driver_merchant_base_click_save table\" (Lines 754-793) \u2013 This section involves creating a table with merchant save event statistics over the last 30 days, which is part of the initial data preparation.",
          "\"Create driver_consumer_base_gender table\" (Lines 796-817) \u2013 This section involves creating a table with consumer gender information, which is part of the initial data preparation."
        ],
        "why_separate": null,
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/01_bq_feat.py"
>>>>>>> origin/main
      }
    },
    {
      "Data Pulling": {
        "line_range": "1-73",
        "evidence": [
<<<<<<< HEAD
          "\"Imports necessary modules and sets up a Fetcher object with various configurations such as stage, sequence number, job name, group name, model name, model owner, description, and manager.\" \u2013 This indicates the setup of a Fetcher object, which is typically used for pulling data from a feature store or variable mart.",
          "\"Configures the Fetcher object with Google Cloud Platform settings including project ID, bucket name, BigQuery project and dataset, and data locations.\" \u2013 This further supports the configuration of a Fetcher object for data pulling.",
          "\"Specifies the variables to be fetched and sets the split ratio for training data.\" \u2013 This step involves specifying the variables to be fetched, which is a key part of data pulling.",
          "\"Executes the Fetcher to fetch the data based on the provided configurations.\" \u2013 This confirms the execution of the Fetcher to pull data."
=======
          "Initializes a Fetcher object with specific configurations including job name, group name, model name, and other metadata. \u2013 This indicates the setup of a data fetching process, which is a key part of Data Pulling.",
          "Configures Google Cloud Platform settings and specifies data locations and variables to be fetched. \u2013 This further supports that the code is focused on pulling additional data/features from a feature store or variable mart.",
          "Sets the data split ratio for training. \u2013 This is part of the configuration for data pulling, ensuring the fetched data is appropriately split for subsequent use."
>>>>>>> origin/main
        ],
        "why_separate": null,
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/01_varmart_feat.py"
      }
    },
<<<<<<< HEAD
=======
    {},
>>>>>>> origin/main
    {
      "Feature Consolidation": {
        "line_range": "1-642",
        "evidence": [
<<<<<<< HEAD
          "Creates a new `driver_dev_features` table with a comprehensive SELECT statement. Joins multiple tables to enrich the data with various features, using COALESCE to handle null values. \u2013 This indicates the merging of multiple datasets into a unified feature set.",
          "Creates a new `driver_oot_features` table with a comprehensive SELECT statement. Joins multiple tables to enrich the data with various features, using COALESCE to handle null values. \u2013 This further supports the merging of datasets into a unified feature set.",
          "Creates a new `driver_oot_features_expand_seq` table by expanding sequence features from `sndr_most_recent_100_merch_list` and `sndr_most_recent_100_merch_category` into individual columns. \u2013 This step involves further consolidation by expanding sequence features into individual columns."
        ],
        "why_separate": "The entire section from lines 17 to 624 is dedicated to creating and populating tables by joining multiple datasets, which is a clear indication of feature consolidation. There is no overlap with other components' line ranges as the other sections (loading YAML configuration, extracting dataset prefix, and exporting to GCS) are distinct and do not involve merging datasets.",
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/02_combine.py"
      }
    },
    {
      "Data Preprocessing": {
        "line_range": "1-185",
        "evidence": [
          "Initializes empty dictionaries to store categorical feature encoders and numerical feature scalers. \u2013 This indicates the setup for data preprocessing tasks.",
          "Reads all parquet files from a specified directory and concatenates them into a single DataFrame. \u2013 Loading and concatenating data is a common preprocessing step.",
          "Cleans the state feature by mapping full state names to abbreviations and encoding them using LabelEncoder. \u2013 Cleaning and encoding categorical data is a key part of preprocessing.",
          "Encodes several categorical features using LabelEncoder and stores the encodings in a dictionary. \u2013 Encoding categorical features is a preprocessing task.",
          "Scales a list of numerical features using StandardScaler and stores the scaling parameters (mean and standard deviation) in a dictionary. \u2013 Scaling numerical features is a preprocessing task.",
          "Processes sequence features by replacing missing values, splitting sequences, and padding them to a fixed length. \u2013 Handling sequence features is part of preprocessing.",
          "Defines a function to write DataFrame chunks to parquet files. \u2013 Writing transformed data to files is often the final step in preprocessing.",
          "Saves the trained Tokenizer, categorical feature encoders, and numerical feature scalers to files using pickle. \u2013 Exporting feature transformers is part of the preprocessing workflow."
        ],
        "why_separate": null,
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/03_prepare_training_data.py"
      }
    },
    {
      "Model Training": {
        "line_range": "1-170",
        "evidence": [
          "\"Set up distributed training strategy and compile model\" \u2013 This indicates the setup for training the model using TensorFlow's MirroredStrategy.",
          "\"Train the model using the training and validation datasets\" \u2013 This confirms the actual training process of the model."
        ],
        "why_separate": null,
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/04_training.py"
      },
      "Model Packaging": {
        "line_range": "171-269",
        "evidence": [
          "\"Save the trained model in H5 and TensorFlow SavedModel formats\" \u2013 This indicates the saving of the trained model in deployment-ready formats.",
          "\"Convert TensorFlow model to ONNX format and save the ONNX specification\" \u2013 This shows the conversion of the model to another format for deployment.",
          "\"Create and save production model with preprocessing layers\" \u2013 This involves creating a production-ready model that includes preprocessing steps."
        ],
        "why_separate": "The model packaging steps are distinct from the training process and involve saving the model in various formats and preparing it for deployment, which is a separate workflow node from training.",
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/04_training.py"
      }
    },
    {
      "Model Scoring": {
        "line_range": "1-104",
        "evidence": [
          "\"Defines a function `oot_data_eval` to evaluate out-of-time (OOT) data. Loads OOT data and model files. Scores the data using the loaded models. Saves the scored data to a specified path.\" \u2013 This clearly describes the process of scoring data using a trained model, which fits the Model Scoring category.",
          "\"Configures and submits a Spark job to GCP for scoring the OOT data. Specifies the function to run, packages to install, and other job parameters.\" \u2013 This indicates the execution of the scoring function on a distributed system, which is part of the Model Scoring process.",
          "\"Creates or replaces an external table in BigQuery using the scored data stored in GCS.\" \u2013 This step involves saving the scored data, which is a typical part of the Model Scoring workflow."
        ],
        "why_separate": null,
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/05_scoring_oot.py"
      }
    },
    {},
    {
      "Model Evaluation": {
        "line_range": "1-731",
        "evidence": [
          "Calculate recall metrics and save performance data (Lines 375-446): Calculates recall metrics for different models and saves the results to a CSV file. Plots the performance metrics and saves the plot as an image. \u2013 This indicates the calculation of performance metrics, which is part of model evaluation.",
          "Calculate recall metrics for specific merchant category and save performance data (Lines 448-530): Calculates recall metrics for different models for a specific merchant category and saves the results to a CSV file. Plots the performance metrics and saves the plot as an image. \u2013 This continues the process of calculating performance metrics for a specific category.",
          "Calculate recall metrics excluding specific merchants and save performance data (Lines 531-607): Calculates recall metrics for different models excluding specific merchants and saves the results to a CSV file. Plots the performance metrics and saves the plot as an image. \u2013 This step involves calculating performance metrics excluding specific merchants.",
          "Calculate recall metrics for recent data and save performance data (Lines 608-731): Calculates recall metrics for different models for recent data and saves the results to a CSV file. Plots the performance metrics and saves the plot as an image. \u2013 This step involves calculating performance metrics for recent data."
        ],
        "why_separate": "The model evaluation steps are distinct from the driver creation steps as they involve calculating and saving performance metrics, which is a separate process from creating and transforming the driver dataset. There is no overlap with the driver creation line ranges.",
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/06_evaluation.py"
      }
=======
          "\"Create driver_dev_features table\" (Lines 17-218): This section involves constructing a SQL query to create a new table `driver_dev_features` by selecting and transforming columns from multiple joined tables. This indicates the merging of datasets into a unified feature set.",
          "\"Create driver_oot_features table\" (Lines 219-420): Similar to the previous section, this involves creating another table `driver_oot_features` by selecting and transforming columns from multiple joined tables, further supporting the feature consolidation process.",
          "\"Expand driver_oot_features table with historical receiver IDs and categories\" (Lines 421-626): This section involves expanding the `driver_oot_features` table with additional columns for historical receiver IDs and categories, extracted from a comma-separated list, which is another step in consolidating features into a unified dataset."
        ],
        "why_separate": "Verification of ZERO overlap with other components' line ranges: The identified lines (17-626) do not overlap with the lines for loading the YAML configuration file (4-13) or exporting the table to Google Cloud Storage (630-639). Justification for why this should be split from the other code: The sections identified involve merging multiple datasets and expanding tables with additional features, which are distinct operations from loading configurations or exporting data. Justification for why this split results in one of the ML component categories defined above: The operations described fit the definition of Feature Consolidation, which involves merging multiple datasets into a unified feature set for modeling.",
        "file_name": "rmr_agent/repos/ql-store-recommendation-prod/research/pipeline/02_combine.py"
      }
>>>>>>> origin/main
    }
  ]
}