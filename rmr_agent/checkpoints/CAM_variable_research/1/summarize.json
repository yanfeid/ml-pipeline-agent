{
  "summaries": {
    "rmr_agent/repos/CAM_variable_research/0_cam_etl.py": "**[Import necessary libraries and set up environment] (Lines 4-21):**\n- Import various Python libraries and modules required for data processing and machine learning.\n- Set display options for pandas DataFrames.\n\n**[Start Spark session] (Lines 23-31):**\n- Import functions to start a Spark session.\n- Initialize a Spark session with specific configurations.\n\n**[Load and process data] (Lines 32-79):**\n- Load data from Parquet and Pig files into Spark DataFrames.\n- Filter and select specific columns based on certain conditions.\n- Save processed data back to Pig format.\n\n**[Define and run a function to convert Parquet to Pig format] (Lines 83-97):**\n- Define a function `parquet2pig` to load, deduplicate, and process data.\n- Save the processed data in Pig format.\n\n**[Set up and run a GCP Spark job for data conversion] (Lines 99-143):**\n- Configure GCP project and Spark job settings.\n- Create and run a Spark job on GCP to execute the `parquet2pig` function.\n\n**[Define a function to pull data based on parameters] (Lines 179-212):**\n- Define a function `pull_data` to load and process data based on given parameters.\n- Use Spark to load data, rename columns, and filter data based on conditions.\n\n**[Set up and run a GCP Spark job for data pulling] (Lines 213-230):**\n- Configure GCP project and Spark job settings.\n- Create and run a Spark job on GCP to execute the `pull_data` function.\n\n**[Define a function to convert raw data to WOE (Weight of Evidence)] (Lines 275-320):**\n- Define a function `raw2woe` to load, deduplicate, and process data.\n- Apply scoring models and save the processed data in Parquet format.\n\n**[Set up and run a GCP Spark job for raw data to WOE conversion] (Lines 321-350):**\n- Configure GCP project and Spark job settings.\n- Create and run a Spark job on GCP to execute the `raw2woe` function.\n\n**[Define a function to convert WOE data to TFRecords] (Lines 355-409):**\n- Define a function `woe2tfrecord` to load, deduplicate, and process data.\n- Apply filters and save the processed data in TFRecord format.\n\n**[Set up and run a GCP Spark job for WOE to TFRecords conversion] (Lines 411-464):**\n- Configure GCP project and Spark job settings.\n- Create and run a Spark job on GCP to execute the `woe2tfrecord` function.\n\n**[Define a function to convert flow data to TFRecords] (Lines 471-544):**\n- Define a function `flow2tfrecord` to load, deduplicate, and process data.\n- Apply filters and save the processed data in TFRecord format.\n\n**[Set up and run a GCP Spark job for flow data to TFRecords conversion] (Lines 546-596):**\n- Configure GCP project and Spark job settings.\n- Create and run a Spark job on GCP to execute the `flow2tfrecord` function.\n\n**[Define a function to convert flow data to TFRecords with additional processing] (Lines 598-655):**\n- Define a function `flow2tfrecord_v2` to load, deduplicate, and process data.\n- Apply filters and save the processed data in TFRecord format.\n\n**[Set up and run a GCP Spark job for flow data to TFRecords conversion with additional processing] (Lines 657-715):**\n- Configure GCP project and Spark job settings.\n- Create and run a Spark job on GCP to execute the `flow2tfrecord_v2` function.\n\n**[Define a function to convert flow data to TFRecords with downsampling] (Lines 719-804):**\n- Define a function `flow2tfrecord_v3` to load, deduplicate, and process data.\n- Apply downsampling and save the processed data in TFRecord format.\n\n**[Set up and run a GCP Spark job for flow data to TFRecords conversion with downsampling] (Lines 811-834):**\n- Configure GCP project and Spark job settings.\n- Create and run a Spark job on GCP to execute the `flow2tfrecord_v3` function.",
    "rmr_agent/repos/CAM_variable_research/1_1_cam_precheck.py": "**[Import necessary packages and set up environment] (Lines 3-23):**\n- Import various Python libraries and modules required for data manipulation, date handling, and Spark operations.\n- Set display options for pandas DataFrames to control the output format.\n\n**[Define parameters for date range] (Lines 26-28):**\n- Define start and end dates for the analysis.\n\n**[Reload helper module and perform solcat precheck] (Lines 32-40):**\n- Reload the helper module and perform a solcat precheck using specified file paths and date parameters.\n\n**[Load incremental variables and perform solcat precheck] (Lines 43-51):**\n- Load incremental variables within a specified date range and perform another solcat precheck.\n\n**[Initialize SolCat loaders and define constants] (Lines 54-66):**\n- Initialize SolCatOnlineVarLoader and SolCatSolutionLoader with a specific parameter.\n- Define constants for the QMONITOR_BASE_URL and column types.\n\n**[Define function to get DataFrame by checkpoint and status] (Lines 67-103):**\n- Define a function to retrieve and process online variables based on checkpoint and status, and merge with solcat data.\n\n**[Define function to split DataFrame by variable type] (Lines 104-118):**\n- Define a function to split the DataFrame into continuous, categorical, and unknown variable types.\n\n**[Define helper functions for variable type organization] (Lines 119-142):**\n- Define functions to check if an item is a number and to organize mixed variables into numeric, categorical, and unknown lists.\n\n**[Define function to convert request JSON to DataFrame] (Lines 143-152):**\n- Define a function to extract column names from JSON data and return them as a list.\n\n**[Define function to get QMonitor information] (Lines 153-163):**\n- Define a function to make a request to the QMonitor API and retrieve column information for a variable.\n\n**[Retrieve and process variables by checkpoint and status] (Lines 164-167):**\n- Retrieve numerical, categorical, and unknown variables using the defined function.\n\n**[Define and process a checklist of variables] (Lines 168-208):**\n- Define a checklist of variables and create a dashboard DataFrame with these variables.\n\n**[Load and inspect data sample] (Lines 209-210):**\n- Load a sample data file and inspect its data types.\n\n**[Read candidate and categorical variable lists] (Lines 211-213):**\n- Read lists of candidate and categorical variables from specified files.\n\n**[Identify differences between categorical lists] (Lines 214-222):**\n- Identify differences between two categorical variable lists and create dashboards for the differences.\n\n**[Check value counts for specific variables] (Lines 223-226):**\n- Check the value counts for specific variables in the data.\n\n**[Sample a fraction of the data for specific variables] (Lines 227-227):**\n- Sample a fraction of the data for specific variables.\n\n**[Classify variables into types] (Lines 228-252):**\n- Classify variables into numeric, categorical, and mixed types based on their data types and null values.\n\n**[Load and inspect another data sample] (Lines 255-256):**\n- Load another data sample and inspect its shape.\n\n**[Classify variables into types for another dataset] (Lines 257-272):**\n- Classify variables into numeric, categorical, and mixed types for another dataset.\n\n**[Check data types for specific variables] (Lines 273-275):**\n- Check the data types and unique values for specific variables.\n\n**[Create dashboards for specific variables] (Lines 276-281):**\n- Create dashboards for specific variables and inspect their unique values.\n\n**[Define functions for sanity checks and column configuration checks] (Lines 310-342):**\n- Define functions to perform sanity checks on variable types and to check column configurations.\n\n**[Perform column configuration checks and concatenate results] (Lines 343-351):**\n- Perform column configuration checks on specified JSON files and concatenate the results.\n\n**[Define function to get sample values for variables] (Lines 352-355):**\n- Define a function to get sample values for variables in the DataFrame.\n\n**[Update dashboard with sample values and check data types] (Lines 356-358):**\n- Update the dashboard with sample values for variables and check for discrepancies in data types.\n\n**[Perform sanity checks on specific variables] (Lines 360-374):**\n- Perform sanity checks on specific variables and inspect the results.\n\n**[Perform sanity checks on differences between categorical lists] (Lines 375-376):**\n- Perform sanity checks on the differences between two categorical lists.\n\n**[Inspect unique values and perform sanity checks] (Lines 377-390):**\n- Inspect unique values for specific variables and perform sanity checks.\n\n**[Check data types for specific variables] (Lines 391-398):**\n- Check data types and value counts for specific variables.\n\n**[Read and merge candidate variable lists] (Lines 399-430):**\n- Read candidate variable lists from specified files and merge them into a single list.\n\n**[Write merged candidate and categorical lists to files] (Lines 431-430):**\n- Write the merged candidate and categorical lists to specified files.",
    "rmr_agent/repos/CAM_variable_research/1_2_cam_postcheck.py": "**[Import necessary packages and set up environment] (Lines 3-30):**\n- Import system and OS modules, append utility path to system path.\n- Import pandas and custom Shifu utilities.\n- Set Hadoop job queue and configuration parameters.\n- Define working paths and environment variables.\n- Set pandas display options for better readability.\n\n**[Load and filter column statistics] (Lines 33-50):**\n- Define path to column statistics file and load it into a DataFrame.\n- Apply filters to remove columns based on missing percentage, IV, standard deviation, and distinct count.\n- Identify columns with high missing rates and remove them.\n- Filter out columns with missing flags.\n\n**[Define columns to check and perform PSI and IV checks] (Lines 52-88):**\n- Specify columns to check for further analysis.\n- Perform checks on PSI and IV values to identify columns that meet specific thresholds.\n- Calculate shapes of DataFrames based on PSI and IV conditions.\n\n**[Import helper module and define model names] (Lines 91-110):**\n- Import and reload a helper module.\n- Define a list of fixed model names for further processing.",
    "rmr_agent/repos/CAM_variable_research/2_1_cam_exist_var_shifu_multisegment.py": "**[Import necessary libraries and set up configurations] (Lines 5-31):**\n- Import essential libraries and modules.\n- Set up paths and configurations for the environment.\n- Configure display options for pandas DataFrames.\n\n**[Define parameters and paths] (Lines 38-53):**\n- Define various parameters such as trigger date, queue, sample rate, and correlation cut-off.\n- Set up paths for segment files, configuration files, HDFS folders, and local directories.\n\n**[Create directories if they do not exist] (Lines 54-56):**\n- Create necessary directories for local paths and Shifu paths.\n\n**[Define utility functions] (Lines 58-93):**\n- Define functions for deleting normalized data, rewriting column configurations, checking sanity, and handling correlations.\n\n**[Load segment file and initialize model names] (Lines 95-118):**\n- Read the segment file and initialize lists for model names and segment names.\n- Create a dictionary for top SE values.\n\n**[Load or create new Shifu models] (Lines 119-120):**\n- Initialize the MultiSegmentWoe object and load or create new Shifu models.\n\n**[Update model configurations] (Lines 122-151):**\n- Load common configuration from a JSON file and update paths.\n- Update model configurations in batch and for specific segments based on the segment file.\n\n**[Copy column files] (Lines 153-165):**\n- Copy column files for different column types to the local Shifu path.\n\n**[Initialize and run Shifu commands] (Lines 168-191):**\n- Run initial Shifu commands for model initialization, statistics, and export.\n- Handle additional statistics and export for failed or incorrect models.\n\n**[Filter by correlation] (Lines 193-194):**\n- Run correlation statistics and filter columns based on correlation cut-off.\n\n**[Normalization and variable selection] (Lines 195-223):**\n- Update model configurations for variable selection.\n- Define and run segment commands for normalization and variable selection.\n\n**[Rewrite column configurations] (Lines 224-239):**\n- Rewrite column configurations based on the final SE values.\n\n**[Collect final SE variables] (Lines 240-257):**\n- Define a function to rewrite column configurations and collect final SE variables.\n\n**[Aggregate and save final SE statistics] (Lines 258-287):**\n- Aggregate statistics for the top SE variables and save to a CSV file.\n\n**[Select different candidate lists] (Lines 288-302):**\n- Generate and save a list of candidate variables for different segments.\n\n",
    "rmr_agent/repos/CAM_variable_research/2_2_cam_new_var_selection_shifu.py": "**[Import necessary libraries and set up configurations] (Lines 4-19):**\n- Import essential libraries and modules.\n- Set up paths and configurations for the working environment.\n- Configure Shifu settings and environment variables.\n\n**[Define parameters and paths] (Lines 26-43):**\n- Define various parameters such as trigger date, queue, sample rate, and correlation cut-off.\n- Set up paths for segment files, configuration files, HDFS, and local directories.\n- Create necessary directories if they do not exist.\n\n**[Define utility functions] (Lines 46-82):**\n- Define functions for deleting normalized data, rewriting column configurations, checking sanity, and filtering by correlation.\n- These functions are used for managing and processing model configurations and data.\n\n**[Load segment file and initialize models] (Lines 84-96):**\n- Read the segment file into a DataFrame.\n- Extract model names and segment names from the DataFrame.\n- Initialize the MultiSegmentWoe object and load or create new models based on the segment file.\n\n**[Update model configurations] (Lines 101-128):**\n- Load common configuration from a JSON file and update paths for HDFS and raw data.\n- Update model configurations in batch and for specific segments based on the segment file.\n\n**[Copy column files to models] (Lines 130-142):**\n- Define column types and read corresponding column files.\n- Write column files to each model's configuration.\n\n**[Run Shifu commands] (Lines 145-148):**\n- Initialize models using Shifu.\n- Run Shifu statistics and export column statistics for each model.\n\n**[Prepare and run segment commands] (Lines 195-214):**\n- Define segment commands for normalization, variable selection, and evaluation.\n- Run the segment commands in parallel for all models using Shifu.",
    "rmr_agent/repos/CAM_variable_research/2_3_cam_all_var_shifu_multisegment.py": "**[Configuration and setup] (Lines 5-28):**\n- Imports necessary libraries and modules.\n- Sets up paths and configurations for the working environment.\n- Configures display options for pandas DataFrames.\n- Sets environment variables for memory allocation.\n\n**[Parameter initialization] (Lines 35-53):**\n- Initializes various parameters such as trigger date, queue, sample rate, correlation cut-off, and paths for segment files and model configurations.\n- Creates necessary directories if they do not exist.\n\n**[Function definitions] (Lines 55-88):**\n- Defines `delete_norm_data` function to delete normalized data from HDFS.\n- Defines `rewriteColumnConfig` function to update column configurations based on selected features.\n- Defines `filter_by_corr` function to filter columns based on correlation threshold and update final column lists.\n\n**[Load segment data and initialize models] (Lines 90-118):**\n- Reads segment data from a CSV file.\n- Initializes lists for model names and segment names.\n- Loads or creates new models using the `MultiSegmentWoe` class.\n\n**[Update model configurations] (Lines 121-149):**\n- Loads common model configuration from a JSON file and updates paths.\n- Updates model configurations in batch for all models.\n- Updates specific configurations for each model based on segment data.\n\n**[Copy column files] (Lines 151-163):**\n- Copies column files for different column types (meta, forceremove, categorical, candidate) to the model directories.\n\n**[Run Shifu commands] (Lines 166-169):**\n- Runs Shifu initialization, statistics calculation, and column statistics export commands in parallel for all models.\n\n**[Additional statistics check and update candidate columns] (Lines 171-180):**\n- Reloads helper module and performs additional statistics sanity checks.\n- Updates candidate columns based on the results of the sanity check.\n\n**[Run normalization and correlation commands] (Lines 196-207):**\n- Runs normalization and correlation statistics commands in parallel for all models.\n- Calls `corr_main` function to handle correlation filtering for each model.\n\n**[Variable selection commands] (Lines 217-231):**\n- Prepares and appends variable selection commands for each model, including resetting variable selection, running variable selection rounds, deleting normalized data, rewriting column configurations, and evaluating normalized data.",
    "rmr_agent/repos/CAM_variable_research/2_4_cam_shifu_all_in_one.py": "**[Configuration and setup] (Lines 1-15):**\n- Imports necessary libraries and modules.\n- Sets up paths for utility functions.\n- Configures Shifu settings for Hadoop job queue and split size.\n\n**[Parameter initialization] (Lines 16-45):**\n- Defines working paths and environment variables.\n- Sets up various parameters like trigger date, sample rate, correlation cut-off, and paths for segment files and configurations.\n- Creates necessary directories if they do not exist.\n\n**[Shifu model setup] (Lines 46-57):**\n- Initializes the MultiSegmentWoe object.\n- Loads or creates new models based on the specified names and paths.\n\n**[Segment file processing] (Lines 58-59):**\n- Reads the segment file into a DataFrame.\n\n**[Column configuration] (Lines 60-72):**\n- Iterates over different column types and writes column configurations for each model.\n\n**[Update configuration and segment file] (Lines 73-97):**\n- Reads and updates the segment file to exclude certain segments.\n- Updates the common configuration and specific segment configurations in the model.\n\n**[Model initialization, statistics, and normalization] (Lines 98-104):**\n- Runs initialization, statistics, and normalization commands in parallel for the models.\n\n**[Merge column configurations] (Lines 105-137):**\n- Iterates over segment DataFrame to create sub-model names and segment names.\n- Reads and processes column configurations for each sub-model, updating column names and flags.\n\n**[Save merged column configuration] (Lines 145-148):**\n- Saves the combined column configuration to a specified path.\n\n**[Final column selection] (Lines 149-161):**\n- Retrieves the final column configuration and selects columns marked as 'Candidate' and 'finalSelect'.\n- Writes the final selected columns to a file.\n\n**[Evaluation] (Lines 168):**\n- Runs the evaluation command in parallel for the models.\n\n**[Categorical variable check] (Lines 169-201):**\n- Reads and processes column statistics.\n- Filters and displays statistics for a predefined list of categorical variables.\n\n**[Top IV and SE variable check] (Lines 202-225):**\n- Defines a function to read lines from a file.\n- Reads and processes column statistics for different segments.\n- Compares top variables between segments and displays statistics for selected variables.",
    "rmr_agent/repos/CAM_variable_research/3_1_model_redefine.py": "**[Imports and setup] (Lines 3-22):**\n- Imports necessary libraries and modules for TensorFlow and Keras.\n- Sets up the path for utility functions and appends it to the system path.\n\n**[Load base model] (Lines 30-32):**\n- Loads a pre-trained base model from a specified path.\n- Prints the summary of the base model.\n\n**[Define shared MLP function] (Lines 34-40):**\n- Defines a function to create a shared multi-layer perceptron (MLP) with specified parameters.\n\n**[Create ensemble model] (Lines 42-49):**\n- Creates an ensemble model by combining multiple models.\n- Sets specific layers to be non-trainable.\n\n**[Add new variable input and MLP] (Lines 50-60):**\n- Adds a new input layer for additional variables.\n- Concatenates outputs from the base model with the new input.\n- Passes the concatenated input through a shared MLP and a dense layer to produce the final output.\n\n**[Define and summarize new models] (Lines 61-73):**\n- Defines new models with the specified inputs and outputs.\n- Prints the summary of the new models.\n\n**[Set trainable layers] (Lines 74-79):**\n- Sets specific layers in the new model to be trainable or non-trainable.\n\n**[Save new models] (Lines 85-87):**\n- Saves the newly defined models to specified paths.\n\n**[Define data preparation class] (Lines 95-231):**\n- Defines a class for preparing data for model training.\n- Includes methods for logging, dataset creation, and parsing TFRecord files.\n\n**[Prepare dataset] (Lines 232-256):**\n- Prepares a dataset using the defined data preparation class and specified parameters.\n\n**[Load and summarize additional models] (Lines 259-275):**\n- Loads and summarizes additional pre-trained models.\n- Creates new models by extracting specific layers from the loaded models.\n\n**[Define shared MLP function again] (Lines 286-292):**\n- Re-defines the shared MLP function for use in subsequent models.\n\n**[Define reduce_sum function] (Lines 293-308):**\n- Defines a function to compute the sum of elements across dimensions of a tensor.\n\n**[Create FM part and concatenate layers] (Lines 310-324):**\n- Creates the Factorization Machine (FM) part of the model.\n- Concatenates intermediate outputs to form the final input for subsequent layers.\n\n**[Define linear and MLP parts] (Lines 326-356):**\n- Defines the linear and MLP parts of the model for different tasks.\n- Adds the outputs from these parts to form the final logits.\n\n**[Compile and save model] (Lines 375-387):**\n- Compiles the model with specified optimizer and loss functions.\n- Saves the compiled model to a specified path.\n\n**[Train model] (Lines 388-394):**\n- Trains the model using the prepared dataset.\n- Specifies training parameters such as epochs, verbosity, and validation data.\n\n**[Copy model to GCS] (Lines 399):**\n- Copies the trained model to a specified Google Cloud Storage path.\n\n**[Build and save new model] (Lines 407-514):**\n- Defines a function to build a new model by combining multiple pre-trained models.\n- Sets specific layers to be trainable or non-trainable.\n- Saves the new model to a specified path.\n\n**[Create and save another new model] (Lines 518-550):**\n- Creates another new model by adding new variable inputs and combining them with the base model outputs.\n- Sets specific layers to be trainable or non-trainable.\n- Saves the new model to a specified path.\n\n**[Extract embedding from model] (Lines 552-560):**\n- Extracts embeddings from a specified layer of a pre-trained model.\n- Saves the new model with the extracted embeddings to a specified path.\n\n**[Define attention layer class] (Lines 588-601):**\n- Defines a custom attention layer class for use in subsequent models.\n\n**[Build and save attention model] (Lines 602-726):**\n- Defines a function to build a new model with attention mechanisms.\n- Sets specific layers to be trainable or non-trainable.\n- Saves the new model to a specified path.\n\n**[Build and save gate model] (Lines 733-855):**\n- Defines a function to build a new model with gating mechanisms.\n- Sets specific layers to be trainable or non-trainable.\n- Saves the new model to a specified path.\n\n**[Build and save max pooling model] (Lines 857-975):**\n- Defines a function to build a new model with max pooling mechanisms.\n- Sets specific layers to be trainable or non-trainable.\n- Saves the new model to a specified path.",
    "rmr_agent/repos/CAM_variable_research/3_2_gcp_training.py": "**[Import necessary libraries and set up paths] (Lines 3-15):**\n- Import TensorFlow, system libraries, and custom utilities.\n- Set up paths for utility scripts and data assets.\n\n**[Define the training function] (Lines 19-93):**\n- Define `exp_trainer_all` function to train a model using TensorFlow.\n- Set up logging and distributed strategy.\n- Load a pre-trained model and compile it with custom loss functions and metrics.\n- Prepare training and validation datasets.\n- Define callbacks for early stopping, model checkpointing, and TensorBoard logging.\n- Train the model using the prepared datasets and specified configurations.\n\n**[Set up global configurations] (Lines 94-138):**\n- Define global variables such as GCS bucket paths, number of workers, batch size, sample size, target and weight lists, and training parameters.\n- Read candidate variables from files and adjust their names.\n- Create a dictionary `model_config` to store model configurations.\n\n**[Define specific experiment configurations] (Lines 145-328):**\n- Define segment names and a function to get candidate variables with suffixes.\n- Create a dictionary `se_files` to store candidate variables for different segments.\n- Define specific configurations for multiple experiments, including paths, number of workers, and training steps.\n\n**[Run experiments and manage jobs] (Lines 330-378):**\n- Set up logging and distributed strategy for local debugging.\n- Iterate over a list of experiment IDs to configure and run each experiment.\n- Split data into training and validation sets.\n- Create and submit jobs to the cloud for each experiment.\n- Save job configurations and IDs to JSON files.\n\n**[Define utility functions for job management] (Lines 380-404):**\n- Define functions to pretty-print model configurations, check job states, and cancel jobs.\n- Load and print configurations for specific jobs.\n- Wait for job completion and retrieve job logs.",
    "rmr_agent/repos/CAM_variable_research/3_3_gcp_training.py": "**[Import necessary libraries and modules] (Lines 3-23):**\n- Imports TensorFlow, system and OS modules, cloud client, TensorBoard, and other utilities.\n- Sets up paths and imports custom helper functions and loss functions.\n\n**[Define the training function] (Lines 25-108):**\n- Defines `exp_trainer_all` function to train a model using TensorFlow.\n- Loads a pre-trained model and compiles it with custom loss functions and metrics.\n- Prepares training and validation datasets.\n- Sets up callbacks for early stopping, model checkpointing, and TensorBoard logging.\n- Trains the model using the prepared datasets and specified configurations.\n\n**[Set up model configuration and parameters] (Lines 109-153):**\n- Defines various parameters such as GCS bucket paths, number of workers, batch size, sample size, target and weight lists, and candidate variables.\n- Configures model training parameters including learning rate, loss functions, and training epochs.\n\n**[Define segment-specific configurations] (Lines 154-226):**\n- Lists segment names and defines a function to get candidate variables for each segment.\n- Sets up specific configurations for different experimental setups, including paths to previous models and data.\n\n**[Run training jobs for each experimental setup] (Lines 227-281):**\n- Iterates over experimental setups, deep copies the base model configuration, and updates it with specific configurations.\n- Splits data into training and validation sets.\n- Submits training jobs to the cloud client and saves job IDs and configurations.\n\n**[Define utility functions for tracking and logging] (Lines 282-350):**\n- Defines functions to pretty print model configurations, check job states, and cancel jobs.\n- Loads and prints job configurations and states.\n- Extracts and logs metrics from job logs.",
    "rmr_agent/repos/CAM_variable_research/3_4_gcp_training.py": "**[Import necessary libraries and modules] (Lines 3-22):**\n- Imports TensorFlow, system utilities, cloud client, TensorBoard, and other necessary libraries.\n- Sets up paths for utility functions and imports custom helper functions.\n\n**[Define experimental trainer function] (Lines 25-108):**\n- Defines a function `exp_trainer_all` to train a model using a given configuration.\n- Sets up logging, distributed strategy, and loads a pre-trained model.\n- Compiles the model with custom loss functions and metrics.\n- Prepares training and validation datasets.\n- Sets up callbacks for early stopping, model checkpointing, and TensorBoard.\n- Trains the model using the prepared datasets and callbacks.\n\n**[Set up model configuration] (Lines 109-153):**\n- Defines various parameters such as GCS buckets, number of workers, batch size, sample size, target and weight lists, and training epochs.\n- Reads candidate variables from files and adjusts them with suffixes.\n- Creates a dictionary `model_config` to store all these parameters.\n\n**[Define specific configurations for different experiments] (Lines 160-226):**\n- Lists segment names and defines a function to get candidate variables with suffixes.\n- Creates a dictionary `se_files` to store candidate variables for different segments.\n- Defines specific configurations for multiple experiments, each with its own candidate variables, previous model path, and data path.\n\n**[Set up logging and distributed strategy] (Lines 229-232):**\n- Configures logging and sets up a distributed strategy using multiple GPUs.\n\n**[Run experiments and save configurations] (Lines 233-277):**\n- Iterates over the list of experiments, deep copies the base model configuration, and updates it with specific configurations.\n- Splits data files into training and validation sets.\n- Submits training jobs to the cloud and saves job IDs and configurations to files.\n\n**[Define utility functions for tracking and logging] (Lines 280-346):**\n- Defines functions to pretty print model configurations, check job states, and cancel jobs.\n- Loads and prints job configurations, waits for job completion, and retrieves job logs.\n- Defines a function to extract metrics from log files and writes job logs to a file.",
    "rmr_agent/repos/CAM_variable_research/3_5_gcp_training.py": "**[Import necessary libraries and modules] (Lines 3-22):**\n- Imports TensorFlow, system utilities, cloud client, TensorBoard, data processing, and other utility functions.\n\n**[Define the experimental trainer function] (Lines 26-109):**\n- Sets up logging and distributed training strategy.\n- Loads a pre-trained model and compiles it with custom loss functions and metrics.\n- Prepares training and validation datasets.\n- Defines callbacks for early stopping, model checkpointing, and TensorBoard logging.\n- Trains the model using the prepared datasets and specified configurations.\n\n**[Set up global configurations and variables] (Lines 110-164):**\n- Defines GCS bucket paths, number of workers, batch size, sample size, target and weight lists, and other training parameters.\n- Reads candidate variables from files and processes them.\n\n**[Define specific experiment configurations] (Lines 165-224):**\n- Lists segment names and defines a function to get candidate variables with a suffix.\n- Sets up specific configurations for different experiments, including paths to data and pre-trained models.\n\n**[Initialize logging and distributed strategy] (Lines 227-230):**\n- Configures logging and sets up a mirrored strategy for distributed training on GPUs.\n\n**[Run experiments and manage job configurations] (Lines 231-275):**\n- Iterates over specific experiment configurations, prepares model configurations, splits data into training and validation sets, and submits training jobs to the cloud.\n- Saves job configurations and IDs to JSON files for tracking.\n\n**[Define utility functions for tracking and logging] (Lines 278-342):**\n- Defines functions to pretty-print model configurations, check job states, cancel jobs, and extract metrics from logs.\n- Uses these functions to manage and track the training jobs.\n\n**[Check and log job details] (Lines 343-344):**\n- Retrieves and logs job details and application logs for further analysis.\n\n**[Check TensorBoard logs] (Lines 345-347):**\n- Commands to list and copy TensorBoard logs from GCS for local inspection.",
    "rmr_agent/repos/CAM_variable_research/3_6_gcp_training.py": "**[Imports and setup] (Lines 3-23):**\n- Imports necessary libraries and modules for TensorFlow, data processing, and utility functions.\n- Sets up paths for utility functions and appends them to the system path.\n\n**[Experiment trainer function definition] (Lines 26-109):**\n- Defines a function `exp_trainer_all` that sets up and trains a TensorFlow model.\n- Configures logging, distributed training strategy, and model compilation.\n- Prepares datasets for training and validation.\n- Sets up callbacks for early stopping, model checkpointing, and TensorBoard logging.\n- Trains the model using the prepared datasets and specified configurations.\n\n**[Model configuration setup] (Lines 110-153):**\n- Defines various configuration parameters such as batch size, sample size, target and weight lists, and training epochs.\n- Reads candidate variables from specified files and adjusts their names based on the configuration.\n\n**[Segment-specific configuration] (Lines 154-213):**\n- Defines specific configurations for different segments of the data.\n- Sets paths for candidate variables and previous model paths for each segment.\n\n**[Local debug setup] (Lines 216-221):**\n- Configures logging and sets up a TensorFlow distribution strategy for local debugging.\n- Calls the `exp_trainer_all` function with a temporary configuration for testing.\n\n**[Job submission and tracking] (Lines 226-290):**\n- Loads job IDs from a JSON file if it exists.\n- Iterates over specific configurations, prepares the model configuration, and splits data files into training and validation sets.\n- Sets paths for saving model checkpoints and logs.\n- (Commented out) Submits jobs to a cloud service for training and saves job IDs.\n\n**[Log extraction and metric analysis] (Lines 294-328):**\n- Defines functions to extract and analyze metrics from training logs.\n- Reads logs and extracts metrics using regular expressions.\n\n**[Miscellaneous operations] (Lines 329-336):**\n- (Commented out) Various operations related to checking configurations, logs, and TensorBoard files.",
    "rmr_agent/repos/CAM_variable_research/3_7_gcp_training.py": "**[Import necessary libraries and modules] (Lines 3-22):**\n- Imports TensorFlow, system utilities, cloud client, TensorBoard, and other necessary libraries.\n- Sets up paths for utility functions and imports custom helper functions.\n\n**[Define experimental trainer function] (Lines 26-109):**\n- Defines a function `exp_trainer_all` to train a model using a distributed strategy.\n- Loads a pre-trained model and compiles it with custom loss functions and metrics.\n- Prepares training and validation datasets.\n- Sets up callbacks for early stopping, model checkpointing, and TensorBoard logging.\n- Trains the model using the prepared datasets and specified configurations.\n\n**[Set up model configuration] (Lines 110-154):**\n- Defines various parameters such as batch size, sample size, target and weight lists, and candidate variables.\n- Configures model training parameters including learning rate, loss functions, and paths for data and models.\n\n**[Define specific configurations for different experiments] (Lines 165-244):**\n- Lists segment names and defines a function to get candidate variables.\n- Sets up specific configurations for multiple experiments, including paths to data and models, and custom loss scales.\n\n**[Initialize logging and strategy, and run experiments] (Lines 247-295):**\n- Initializes logging and sets up a distributed strategy.\n- Iterates over specific configurations, prepares model configurations, and submits training jobs to the cloud.\n- Saves job IDs and configurations for tracking.\n\n**[Define utility functions for tracking and logging] (Lines 298-362):**\n- Defines functions to pretty-print model configurations, check job states, and cancel jobs.\n- Extracts metrics from log files and retrieves job logs from the cloud.\n\n**[Check and log TensorBoard data] (Lines 365-367):**\n- Commands to list and copy TensorBoard logs from cloud storage for local inspection.",
    "rmr_agent/repos/CAM_variable_research/4_pack_scoring.py": "**[Import necessary libraries and modules] (Lines 4-41):**\n- Import TensorFlow and Keras libraries for deep learning.\n- Import various utility functions and modules for data processing and model handling.\n- Set display options for pandas DataFrames.\n\n**[Define parameters and paths] (Lines 47-57):**\n- Set local and Google Cloud Storage (GCS) directories.\n- Define experiment ID and list of TensorFlow model directories on GCS.\n\n**[Download TensorFlow models from GCS to local directory] (Lines 61-91):**\n- Create local directories for TensorFlow models.\n- Download models from GCS to local directories using `gsutil` command.\n- Append local paths of downloaded models to a list.\n\n**[Define function to get candidate variables] (Lines 119-121):**\n- Define a function to read candidate variable names from a file and append a suffix to each.\n\n**[Define function to package and save UME models] (Lines 125-174):**\n- Define a function to package TensorFlow models with candidate variables and save the UME models.\n- Load TensorFlow models and create transformation nodes.\n- Generate and save the final UME model specification.\n\n**[Prepare candidate variables and package UME models for different segments] (Lines 175-217):**\n- Define candidate variables for different segments.\n- Call the packaging function for each experiment and segment.\n\n**[Define function to package all flow UME models] (Lines 219-328):**\n- Define a function to package all flow UME models with candidate variables.\n- Load TensorFlow models and create transformation nodes.\n- Generate and save the final UME model specification.\n\n**[Prepare candidate variables and package all flow UME models] (Lines 263-328):**\n- Define candidate variables for all flow segments.\n- Call the packaging function for the all flow segment.\n\n**[Define function for base ESM WOE scoring] (Lines 432-471):**\n- Define a function to load data, apply filters, and score using UME models.\n- Save the scored data to a specified path.\n\n**[Submit and monitor GCP Spark jobs for scoring] (Lines 476-500):**\n- Create and submit Spark jobs on GCP for scoring using the defined function.\n- Monitor job completion and retrieve logs.\n\n**[Define function for score alignment] (Lines 515-538):**\n- Define a function to align scores between different models using SAP.\n- Generate and save the new model specification with aligned scores.\n\n**[Define function to test score alignment with additional conditions] (Lines 547-591):**\n- Define a function to test score alignment and add conditions for specific scenarios.\n- Generate and save the final model specification with conditions.\n\n**[Align scores to RMR and save results] (Lines 616-620):**\n- Call the alignment function to align scores to RMR and save the results.\n\n**[Define function for final UME scoring] (Lines 686-727):**\n- Define a function to load data, deduplicate, and score using UME models.\n- Save the scored data to a specified path.\n\n**[Submit and monitor GCP Spark jobs for final scoring] (Lines 731-770):**\n- Create and submit Spark jobs on GCP for final scoring using the defined function.\n- Monitor job completion and retrieve logs.\n\n**[Copy final scoring results from GCS to local directory] (Lines 789-797):**\n- Copy final scoring results from GCS to a local directory using `gsutil` command.",
    "rmr_agent/repos/CAM_variable_research/5_eval.py": "**Importing necessary libraries and setting display options (Lines 1-8):**\n- Imports various libraries required for data manipulation and evaluation.\n- Sets display options for pandas to show more rows and columns.\n\n**Defining file paths and creating directories if they don't exist (Lines 10-15):**\n- Defines file paths for scoring data and output files.\n- Creates necessary directories if they do not exist.\n\n**Loading and merging data from parquet files (Lines 16-25):**\n- Reads data from two parquet files and merges them on a common column.\n\n**Reading and inspecting the final parquet file (Lines 27-30):**\n- Reads the final parquet file and checks its shape and date range.\n\n**Renaming columns in the dataframe (Lines 34-39):**\n- Renames specific columns in the dataframe for clarity.\n\n**Defining a list of score columns (Lines 40-47):**\n- Creates a list of score columns to be used in the evaluation.\n\n**Defining functions to get month and week cutoffs (Lines 51-74):**\n- Defines functions to categorize dates into specific month and week periods.\n\n**Applying month period function to dataframe (Lines 75-76):**\n- Applies the month period function to a date column in the dataframe and counts the occurrences of each period.\n\n**Converting specific columns to float type (Lines 96-104):**\n- Converts certain columns in the dataframe to float type for further analysis.\n\n**Setting up parameters for model evaluation (Lines 107-130):**\n- Defines a dictionary of parameters to be used in the model evaluation function.\n\n**Running the model evaluation and processing results (Lines 132-138):**\n- Calls the model evaluation function with the defined parameters and processes the results.\n\n**Concatenating and pivoting results for various conditions (Lines 140-188):**\n- Concatenates and pivots the evaluation results based on different conditions and dimensions.\n\n**Querying and grouping data for specific conditions (Lines 224-235):**\n- Queries the dataframe for specific conditions and groups the results for further analysis.\n\n**Concatenating and pivoting results for different flows and segments (Lines 244-343):**\n- Concatenates and pivots the evaluation results for different flows and segments over specified periods.\n\n**Plotting performance metrics (Lines 786-836):**\n- Defines a function to plot performance metrics for different scores and dimensions.\n\n**Plotting specific performance metrics for various flows (Lines 837-844):**\n- Calls the plotting function for different flows and score names to visualize performance metrics.\n\n**Setting up and displaying seaborn plots (Lines 845-875):**\n- Configures seaborn settings and plots score distributions by group.\n\n**Querying and pivoting results for specific evaluation conditions (Lines 876-882):**\n- Queries and pivots the evaluation results for specific conditions to check catch rates.\n\n**Running SQL queries to fetch data (Lines 886-929):**\n- Executes SQL queries to fetch data from a database for further analysis.\n\n**Aggregating and pivoting data for specific conditions (Lines 930-947):**\n- Aggregates and pivots data based on specific conditions to analyze weights and counts.\n\n**Loading and renaming columns in a new dataframe (Lines 954-960):**\n- Loads a new parquet file and renames columns for clarity.\n\n**Calculating precision and recall metrics (Lines 962-990):**\n- Calculates weighted precision and recall metrics for different conditions and periods.",
    "rmr_agent/repos/CAM_variable_research/analysis.py": "**Import necessary libraries and set configurations (Lines 3-18):**\n- Import various libraries including `json`, `warnings`, `os`, `sys`, `pandas`, `matplotlib`, `seaborn`, and `pyspark`.\n- Set display options for pandas.\n\n**Read and preprocess data (Lines 29-50):**\n- Read a parquet file into a DataFrame.\n- Rename columns and change data types of specific columns.\n- Display the range of a date column.\n\n**Driver analysis (Lines 51-104):**\n- Read a CSV file into a DataFrame.\n- Create new columns based on conditions.\n- Perform groupby and aggregation operations.\n- Pivot tables and calculate percentages.\n\n**Model evaluation setup (Lines 105-143):**\n- Define a list of score names.\n- Import a model evaluation function and set parameters for evaluation.\n- Execute the model evaluation function and process the results.\n\n**Visualize score distributions (Lines 214-259):**\n- Plot histograms of score distributions for different segments using seaborn.\n\n**TSNE visualization (Lines 260-277):**\n- Perform TSNE dimensionality reduction on a sample of the data.\n- Add TSNE results to the DataFrame.\n\n**Scatter plots of TSNE results by segment (Lines 283-336):**\n- Create scatter plots of TSNE results, colored by different segments and bad tags.\n\n**False positive case analysis (Lines 337-545):**\n- Create scatter plots of TSNE results for false positive cases, colored by different segments and bad tags.\n\n**Stability vs. confidence analysis (Lines 546-657):**\n- Create scatter plots to analyze the relationship between score gaps and another score, colored by different segments and bad tags.\n\n**Sample time period analysis (Lines 658-714):**\n- Create scatter plots of TSNE results for different time periods, colored by different segments and bad tags.\n\n**Training data visualization (Lines 805-831):**\n- Read a parquet file and filter the data.\n- Perform TSNE dimensionality reduction on a sample of the training data.\n\n**Join with additional tagging data (Lines 1195-1283):**\n- Join the main DataFrame with another DataFrame containing additional tagging information.\n- Perform TSNE dimensionality reduction on the merged data.\n- Create scatter plots of TSNE results, colored by different tags.\n\n**Balance check (Lines 1284-1329):**\n- Query a database to get statistics on different segments and product flows.\n- Calculate and display bad/good ratios for different segments and product flows.\n\n**Learning rate decay visualization (Lines 1331-1352):**\n- Define a function to calculate decayed learning rates.\n- Create a DataFrame with steps and corresponding learning rates.\n- Plot the learning rate decay over steps using seaborn.",
    "rmr_agent/repos/CAM_variable_research/model_redefine.py": "**Import necessary libraries and initialize DataProcClient** (Lines 2-10):\n- Import TensorFlow and other necessary libraries.\n- Initialize a DataProcClient for Google Cloud Platform.\n\n**Load and summarize pre-trained models** (Lines 17-24):\n- Load two pre-trained models from specified Google Cloud Storage paths.\n- Print summaries of the loaded models.\n\n**Extract and redefine adaptive model** (Lines 26-60):\n- Extract specific layers from the adaptive model.\n- Create a new model using these layers.\n- Add new task-specific layers and save the final model.\n\n**Extract and redefine robust model** (Lines 62-94):\n- Extract specific layers from the robust model.\n- Create a new model using these layers.\n- Add new task-specific layers and save the final model.\n\n**Create and save ensemble model combining adaptive and robust models** (Lines 95-116):\n- Extract specific layers from both adaptive and robust models.\n- Combine these layers into an ensemble model.\n- Add new task-specific layers and save the final ensemble model.\n\n**Create and save adaptive model with classification and regression outputs** (Lines 126-145):\n- Extract specific layers for classification and regression from the adaptive model.\n- Combine these layers into a new model.\n- Add new task-specific layers for both classification and regression and save the final model.\n\n**Define shared MLP and reduce_sum functions** (Lines 147-169):\n- Define a function to create a shared MLP with specified parameters.\n- Define a function to perform a reduce_sum operation with compatibility for different TensorFlow versions.\n\n**Create and save DeepFM model** (Lines 170-224):\n- Extract specific layers from adaptive and robust models.\n- Combine these layers into an ensemble model.\n- Add DeepFM layers and save the final model.\n\n**Load and save pre-trained model for fine-tuning** (Lines 226-229):\n- Load a pre-trained model from a specified path.\n- Save the model for further fine-tuning.\n\n**Create and save model with two outputs** (Lines 234-250):\n- Load a pre-trained model.\n- Modify the model to have two outputs.\n- Save the final model.\n\n**Extract and package UME model** (Lines 252-308):\n- Load pre-trained adaptive and robust models.\n- Extract specific layers and package them with UME model.\n- Save the final packaged model.\n\n**Create and save MMOE+PPNet model** (Lines 319-386):\n- Define inputs and embeddings for additional features.\n- Combine adaptive and robust model embeddings with additional feature embeddings.\n- Add interaction layers and save the final model.\n\n**Create and save MMOE+CT/YG tower model** (Lines 393-426):\n- Define inputs for additional tasks.\n- Combine adaptive and robust model embeddings with task-specific inputs.\n- Add task-specific layers and save the final model."
  },
  "cleaned_code": {
    "rmr_agent/repos/CAM_variable_research/0_cam_etl.py": "   1 | # %url -c horton\n   2 | # %ppauth\n   3 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/jyang2/tmp/cam_rsh1/infer_scoring/\n   4 | import json\n   5 | import warnings\n   6 | import os, sys\n   7 | from model_automation.utils.rmr.helper import run_cmd\n   8 | from py_dpu.utils import  save_pig,load_pig, rm_df_alias\n   9 | from pyspark.sql import functions as F\n  10 | from pyspark.sql.functions import col, max, min, when, lit\n  11 | import time\n  12 | from model_automation.gcp import dataproc_config\n  13 | import pandas as pd\n  14 | from pyScoring.model import ModelScorer\n  15 | from pyScoring import UMEModel\n  16 | util_path = '/projects/gds-focus/data/catch/IL/CAM_variable/assets'\n  17 | sys.path.append(util_path)\n  18 | from helper import *\n  19 | pd.set_option('display.max_rows', 500)\n  20 | pd.set_option('display.max_columns', 500)\n  21 | pd.set_option('display.width', 1000)\n  22 | # ## merge data\n  23 | from automation_utils.spark.session import get_spark\n  24 | from model_automation.utils.rmr import spark_config\n  25 | # start spark session\n  26 | spark = get_spark(\n  27 |     app_name=\"new_variable\", \n  28 |     queue = \"risk_gds_focus\",\n  29 |     **spark_config['large']\n  30 | )\n  31 | spark\n  32 | from py_dpu.utils import save_pig,load_pig, rm_df_alias, load_parquet\n  33 | df = load_parquet(spark, \"/apps/risk/madmen/out/model_ops/junrzhang_1739108926\")\n  34 | df.grouBy(\"driver_madmen_\")\n  35 | df = load_pig(spark, \"/apps/risk/det/cchen16/il_new_variable/data/cam_0912_recent_8k_var_pig_w_crc\")\n  36 | meta_cols = [c for c in df.columns if c.startswith(\"driver_\")]\n  37 | len(meta_cols)\n  38 | meta_cols\n  39 | save_pig(spark, df.select(*meta_cols), \"/apps/risk/det/cchen16/il_new_variable/data/cam_0912_recent_meta_pig\")\n  40 | # !hadoop fs -get /apps/risk/det/cchen16/il_new_variable/data/cam_0912_recent_meta_pig ./data/\n  41 | df = load_parquet(spark, \"/apps/risk/det/cchen16/il_new_variable/data/cam_0912_recent_8k_var_parquet\")\n  42 | meta_cols = read_line_file('/projects/gds-focus/data/catch/IL/CAM_research/CAM_new_feature_pipeline/output/20250205/recent/shifu/multi_seg/cam24_il_Tot/columns/meta.column.names')\n  43 | candidate_vars = read_line_file('/projects/gds-focus/data/catch/IL/CAM_research/CAM_new_feature_pipeline/output/20250205/recent/shifu/multi_seg/cam24_il_Tot/columns/candidate.column.names')\n  44 | set(meta_cols)-set(df.columns), set(candidate_vars)-set(df.columns)\n  45 | set(meta_cols)-set(df.columns), set(candidate_vars)-set(df.columns)\n  46 | # %maglev copy gs://pypl-bkt-rsh-row-std-gds-focus/user/makang/CAM24/data/multiseg_data/adp_rbt_woe_0510_0331_all /apps/risk/det/cchen16/il_new_variable/data/\n  47 | # !gsutil du -h -s gs://pypl-bkt-rsh-row-std-gds-focus/user/makang/CAM24/data/multiseg_data/adp_rbt_woe_0510_0331_all\n  48 | # !hadoop fs -du -h -s /apps/risk/det/cchen16/il_new_variable/data/adp_rbt_woe_0510_0331_all\n  49 | df = load_parquet(spark, \"/apps/risk/det/cchen16/il_new_variable/data/adp_rbt_woe_0510_0331_all\")\n  50 | df.groupBy(\"driver_madmen_monthly\").agg({\"driver_trans_id\":\"count\"}).show()\n  51 | len(df.columns)\n  52 | df = df.withColumn(\"driver_product_flow_rollup\", when(col(\"driver_l2_product_flow\").isin(\"MS EC Non Recurring\", \"WPS Non Recurring\"), \"Branded XO\")  \n  53 |                                                     .when(col(\"driver_l2_product_flow\").isin(\"MS EC Recurring\", \"WPS Recurring\", \"Branded_Recurring\"), \"Branded Recurring\")  \n  54 |                                                     .when(col(\"driver_l2_product_flow\").isin(\"P2P-F&F\", \"P2P-G&S\", \"eBay MOR\"), col(\"driver_l2_product_flow\"))  \n  55 |                                                     .otherwise(\"Other\")\n  56 |                   )\n  57 | df.groupBy(\"driver_product_flow_rollup\").agg({\"driver_trans_id\":\"count\"}).show()\n  58 | write_line_file(\"/projects/gds-focus/data/catch/IL/CAM_research/data_drift/files/adaptive_woe_output.txt\", adaptive_cols)\n  59 | write_line_file(\"/projects/gds-focus/data/catch/IL/CAM_research/data_drift/files/adaptive_meta.txt\", meta_cols)\n  60 | adaptive_cols = list(UMEModel(\"/projects/gds-focus/data/catch/IL/CAM24/umes/base/adaptive_woe.m\").get_outputs())\n  61 | len(adaptive_cols)\n  62 | adaptive_cols[-1]\n  63 | len(set(adaptive_cols)&set(df.columns)) == len(adaptive_cols)\n  64 | meta_cols = [c for c in df.columns if c.startswith('driver_')]\n  65 | len(meta_cols)\n  66 | 'driver_product_flow_rollup' in meta_cols\n  67 | used_cols = meta_cols + adaptive_cols\n  68 | len(used_cols)\n  69 | df.select(*used_cols).write.format(\"parquet\").mode(\"overwrite\").save(\"/apps/risk/det/cchen16/il_new_variable/data/adp_woe_0510_0331_20241212\")\n  70 | # !hadoop fs -du -h -s /apps/risk/det/cchen16/il_new_variable/data/adp_woe_0510_0331_20241212\n  71 | df.select['']\n  72 | len([c for c in incre_vars if c in df1.columns])\n  73 | len(set(df1.columns)&set(df2.columns))\n  74 | incre_vars = [c for c in incre_vars if c not in df1.columns]\n  75 | len(incre_vars)\n  76 | df2 = df2.withColumn(\"driver_trans_id_new\", col(\"driver_trans_id\"))\n  77 | df2_used_cols = ['driver_trans_id_new']+incre_vars\n  78 | set(df1.columns)&set(df2_used_cols)\n  79 | final_df = df1.join(df2.select(*df2_used_cols), df1.driver_trans_id == df2.driver_trans_id_new, how=\"inner\").drop(\"driver_trans_id_new\")\n  80 | save_pig(spark, final_df, \"/apps/risk/det/cchen16/il_new_variable/data/cam_0912_recent_all_var_pig_w_crc\", delimiter='\\x07', with_crc=True)\n  81 | # !hadoop fs -du -h -s /apps/risk/det/cchen16/il_new_variable/data/cam_0912_recent_all_var_pig_w_crc\n  82 | # ## parquet2pig\n  83 | def parquet2pig(input_path,\n  84 |         output_path):\n  85 |     from pyScoring.model import ModelScorer\n  86 |     from pyScoring import UMEModel\n  87 |     from py_dpu import load_parquet, load_pig, save_pig\n  88 |     from automation_utils.gcp.GSUtil import GSUtilHelper\n  89 |     from pyspark.sql.functions import array, col, lit, when\n  90 |     from pyspark.sql import functions as F\n  91 |     # load and dedup \n  92 |     data = load_parquet(spark, input_path)\n  93 |     data = data.drop_duplicates(subset=[\"driver_trans_id\"])\n  94 |     data = data.withColumn(\"driver_cam22_score_bin\", when((col(\"driver_cam22_ems_score\") >= lit(600.0))\\\n  95 |                                                          &(col(\"driver_cam22_ems_score\") <= lit(800.0)), \"BT_600_800\")\\\n  96 |                                                          .otherwise(lit(\"other_bin\")))\n  97 |     save_pig(spark, data, output_path, delimiter='\\x07', with_crc=True)\n  98 |     return\n  99 | from model_automation.gcp import dataproc_config\n 100 | from aml import cloud_v1 as cloud\n 101 | gcp_project = \"ccg24-hrzana-gds-focus\"\n 102 | client = cloud.TrainingClient(gcp_project=gcp_project)\n 103 | dataproc_config['medium']['spark_properties']['spark.driver.maxResultSize'] = '24g'\n 104 | dataproc_config['medium']['spark_properties']['spark.yarn.driver.memoryOverhead'] = '4096'\n 105 | dataproc_config['medium']['spark_properties']['spark.yarn.executor.memoryOverhead'] = '4096'\n 106 | from model_automation.gcp import dataproc_config\n 107 | from aml import cloud_v1 as cloud\n 108 | gcp_project = \"ccg24-hrzana-gds-focus\"\n 109 | client = cloud.TrainingClient(gcp_project=gcp_project)\n 110 | job_id = client.create_spark_job(\n 111 |     func= parquet2pig,\n 112 |     packages_to_install = ['PyDPU==1.1.0',\n 113 |                            'pyScoring==0.8.0.1.post1',\n 114 |                            'gcsfs',\n 115 |                            'automation_utils==0.3.0',\n 116 |                            'pyjnius<1.5.0'],\n 117 |     # func kwargs\n 118 |     input_path = \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/pull_data_il_0912_dt_1112/group_all\",\n 119 |     output_path = \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_0912_recent_8k_var_pig_w_crc\",\n 120 |     # dataproc config\n 121 |     **dataproc_config['medium']\n 122 | )\n 123 | !\n 124 | from model_automation.gcp import dataproc_config\n 125 | from aml import cloud_v1 as cloud\n 126 | gcp_project = \"ccg24-hrzana-gds-focus\"\n 127 | client = cloud.TrainingClient(gcp_project=gcp_project)\n 128 | incre_job_id = client.create_spark_job(\n 129 |     func= parquet2pig,\n 130 |     packages_to_install = ['PyDPU==1.1.0',\n 131 |                            'pyScoring==0.8.0.1.post1',\n 132 |                            'gcsfs',\n 133 |                            'automation_utils==0.3.0',\n 134 |                            'pyjnius<1.5.0'],\n 135 |     # func kwargs\n 136 |     input_path = \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/pull_incre_var_il_0912_dt_1113/group_all\",\n 137 |     output_path = \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_0912_recent_incre_var_pig_w_crc\",\n 138 |     # dataproc config\n 139 |     **dataproc_config['medium']\n 140 | )\n 141 | # !gsutil cat gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_0912_recent_incre_var_pig_w_crc/.pig_header | tr '\\a' '\\n' | grep 'crc_flag'\n 142 | client.wait_job_for_completion(incre_job_id, \n 143 |                                time_to_sleep=300)\n 144 | def check_type(variables_to_check, df):\n 145 |     for var in variables_to_check:  \n 146 |         if pd.api.types.is_numeric_dtype(df[var]):  \n 147 |             return 'numeric_var'\n 148 |         elif pd.api.types.is_categorical_dtype(df[var]) or df[var].dtype == 'object':  \n 149 |             return 'categorical_var'\n 150 |         else:  \n 151 |             return 'mixed_var'\n 152 | client.get_job_driver_logs(incre_job_id)\n 153 | # !hadoop fs -text {hdfs_folder}/cam_0912_recent_8k_var_pig/.pig_header | tr '\\a' '\\n' | grep 'driver_'\n 154 | # !echo {hdfs_folder}/cam_0912_recent_8k_var_pig\n 155 | gcs_path = \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_0912_recent_incre_var_pig_w_crc\"\n 156 | hdfs_folder = \"/apps/risk/det/cchen16/il_new_variable/data\"\n 157 | # %maglev copy {gcs_path} {hdfs_folder}/\n 158 | # ## candidate data pull\n 159 | # !gsutil -m rm -r gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/pull_oot_data_il_0812_1012\n 160 | init_pool_list_path = \"/projects/gds-focus/data/catch/IL/CAM_variable/files/oot_1127_2747.txt\"\n 161 | with open(init_pool_list_path, 'r') as f:\n 162 |     init_list = [line.strip() for line in f.readlines()]\n 163 | data_pull_params = {\n 164 |     \"data_type\" : \"oot\",\n 165 |     \"start_date\" : \"2024-08-12\",\n 166 |     \"end_date\" : \"2024-10-12\",\n 167 |     \"checkpoint\": \"ConsolidatedFunding\",\n 168 |     \"model_name\" : \"SF_SL_new_vars\",\n 169 |     \"gcs_driver_path\" : \"gs://pypl-bkt-rsh-row-std-gds-qpull/out/cchen16_1732625181/cchen16_1732625181\",\n 170 |     \"group_size\" : '1',\n 171 |     \"driver_date_column\": \"driver_pmt_start_date\",\n 172 |     \"var_list\": init_list,\n 173 |     \"driver_keys\": [\"driver_trans_id_fix\"],\n 174 |     \"madmen_keys\": ['transaction'],\n 175 |     \"gcs_var_path\": \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/pull_oot_data_il_0812_1012\",\n 176 | }\n 177 | from py_dpu import loadByDriver\n 178 | help(loadByDriver)\n 179 | def pull_data(params\n 180 |              ):\n 181 |     import os, sys, ast\n 182 |     from datetime import datetime, timedelta, date\n 183 |     sys.path.append(\"dpu-latest.jar\")\n 184 |     from pyspark.sql.functions import col, regexp_replace\n 185 |     from py_dpu import load_parquet, load_pig, save_pig\n 186 |     from py_dpu import loadByDriver, Statistics, rename_df\n 187 |     var_list = params.get('var_list')\n 188 |     driver_date_column = params.get('driver_date_column')\n 189 |     driver_keys = params.get('driver_keys')\n 190 |     driverSet = load_parquet(spark, params.get('gcs_driver_path'))\n 191 |     # driverSet = spark.read.format('csv') \\\n 192 |     #     .option('delimiter', '\\x07') \\\n 193 |     #     .option('header', 'true') \\\n 194 |     #     .load(\"{}/*/*\".format(params['gcs_driver_path']))\n 195 |     renamed_df = rename_df(spark, driverSet, prefix='driver_')\n 196 |     # for key in driver_keys:\n 197 |     #     renamed_df = renamed_df.withColumn(key, col(key).cast(\"string\"))\n 198 |     #     renamed_df = renamed_df.withColumn(key, regexp_replace(key, \"\\.\\d+\", \"\"))\n 199 |     time = \"{} to {}\".format(params['start_date'], params['end_date'])\n 200 |     madmen_df = loadByDriver(\n 201 |         spark,\n 202 |         renamed_df,\n 203 |         checkpoint=params.get(\"checkpoint\"),\n 204 |         time=time,\n 205 |         variables=var_list,\n 206 |         dateColumn=driver_date_column,\n 207 |         driverKeys=driver_keys,\n 208 |         madmenKeys=params.get(\"madmen_keys\", [\"transaction\"]),\n 209 |         outputPath=params.get(\"gcs_var_path\"),\n 210 |         groupSize=int(params.get('group_size'))\n 211 |     )\n 212 |     return\n 213 | from model_automation.gcp import dataproc_config\n 214 | from aml import cloud_v1 as cloud\n 215 | gcp_project = \"ccg24-hrzana-gds-focus\"\n 216 | client = cloud.TrainingClient(gcp_project=gcp_project)\n 217 | dataproc_config['large']['spark_properties']['spark.driver.maxResultSize'] = '48g'\n 218 | dataproc_config['large']['spark_properties']['spark.yarn.driver.memoryOverhead'] = '4096'\n 219 | dataproc_config['large']['spark_properties']['spark.yarn.executor.memoryOverhead'] = '4096'\n 220 | data_pull_job_id = client.create_spark_job(\n 221 |         func= pull_data,\n 222 |         packages_to_install = ['automation_utils==0.3.2','gcsfs'],\n 223 |         # function kwargs\n 224 |         params=data_pull_params,\n 225 |         **dataproc_config['large'],\n 226 |     )\n 227 | client.wait_job_for_completion(data_pull_job_id, \n 228 |                                time_to_sleep=300)\n 229 | # client.cancel_job(data_pull_job_id)\n 230 | client.get_job_driver_logs(data_pull_job_id)\n 231 | # ## New var data pull\n 232 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-art/maglev/SF_SL/20240926/data/training/driver/pypl-edw.pp_scratch.il_offsim_all_final_2024_09_26/_ver_20240625_20240923 | head\n 233 | # !gsutil ls gs://pypl-bkt-r-row-std-nbapp-63fad5/mds/22/RESEARCH/SCORING/1672/1/20240924_20241223/ | head\n 234 | incre_pool_list_path = \"/projects/gds-focus/data/catch/IL/CAM_variable/files/oot_0111_3505.txt\"\n 235 | with open(incre_pool_list_path, 'r') as f:\n 236 |     incre_list = [line.strip() for line in f.readlines()]\n 237 | incre_pull_params = {\n 238 |     \"data_type\" : \"training\",\n 239 |     \"start_date\" : \"2024-05-26\",\n 240 |     \"end_date\" : \"2024-09-23\",\n 241 |     \"checkpoint\": \"ConsolidatedFunding\",\n 242 |     \"model_name\" : \"SF_SL_incre_vars\",\n 243 |     \"gcs_driver_path\" : \"gs://pypl-bkt-rsh-row-std-gds-art/maglev/SF_SL/20240926/data/training/driver/pypl-edw.pp_scratch.il_offsim_all_final_2024_09_26\",\n 244 |     \"group_size\" : '1',\n 245 |     \"driver_date_column\": \"driver_pmt_start_date\",\n 246 |     \"var_list\": incre_list,\n 247 |     \"driver_keys\": [\"driver_trans_id\"],\n 248 |     \"madmen_keys\": ['transaction'],\n 249 |     \"gcs_var_path\": \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/pull_incre_var_il_0926_dt_0115\",\n 250 | }\n 251 | from model_automation.gcp import dataproc_config\n 252 | from aml import cloud_v1 as cloud\n 253 | gcp_project = \"ccg24-hrzana-gds-focus\"\n 254 | client = cloud.TrainingClient(gcp_project=gcp_project)\n 255 | dataproc_config['large']['spark_properties']['spark.driver.maxResultSize'] = '48g'\n 256 | dataproc_config['large']['spark_properties']['spark.yarn.driver.memoryOverhead'] = '4096'\n 257 | dataproc_config['large']['spark_properties']['spark.yarn.executor.memoryOverhead'] = '4096'\n 258 | incre_pull_job_id = client.create_spark_job(\n 259 |         func= pull_data,\n 260 |         packages_to_install = ['automation_utils==0.3.2','gcsfs'],\n 261 |         # function kwargs\n 262 |         params=incre_pull_params,\n 263 |         **dataproc_config['large'],\n 264 |     )\n 265 | client.wait_job_for_completion(incre_pull_job_id,\n 266 |                                time_to_sleep=300)\n 267 | client.get_job_driver_logs(incre_pull_job_id)\n 268 | 6000000/(1024*32)\n 269 | data = pd.read_csv(\"/projects/gds-focus/data/catch/IL/CAM_variable/sample.csv\",sep='\\x07')\n 270 | data.shape\n 271 | sum((data.isnull().sum(axis=0)/data.shape[0]) > 0.98)\n 272 | # ## Raw data to woe\n 273 | # %ppauth\n 274 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/CAM_IL/20240926/driver/cam_rmr_refresh_20240625_20240923/pypl-edw.pp_scratch.REAP_cam_rmr_strategy_final_2024_09_26/data/group_all\n 275 | def raw2woe(input_path,\n 276 |          output_path,\n 277 |             model_path_list,\n 278 |          meta_cols=[],\n 279 |          file_number=256,\n 280 |          ):\n 281 |     from py_dpu import load_parquet, load_pig, rm_df_alias\n 282 |     # from automation_utils.spark.tfrecord import store_tfrecord\n 283 |     from automation_utils.gcp.GSUtil import GSUtilHelper\n 284 |     from pyspark.sql.functions import min, max\n 285 |     import pyspark.sql.functions as F\n 286 |     from pyspark.sql.functions import array, col, lit, when\n 287 |     from pyspark.sql.types import FloatType, IntegerType  \n 288 |     import json\n 289 |     from pyScoring.model import ModelScorer\n 290 |     from pyScoring import UMEModel\n 291 |     from py_dpu import load_parquet, load_pig\n 292 |     from automation_utils.gcp.GSUtil import GSUtilHelper\n 293 |     from pyspark.sql.functions import array, col\n 294 |     from pyspark.sql import functions as F\n 295 |     # load and dedup \n 296 |     data = load_parquet(spark, input_path)\n 297 |     data = data.drop_duplicates(subset=[\"driver_trans_id\"])\n 298 |     # data = data.withColumn(\"driver_cam22_ems_score\", col(\"driver_cam22_ems_score\").cast(FloatType()))\n 299 |     # filters = \"\"\"\n 300 |     #     driver_cam22_ems_score > 650.0\n 301 |     # \"\"\"\n 302 |     # data = data.where(filters)\n 303 |     tmp_file_list = []\n 304 |     input_list = []\n 305 |     model_outputs = []\n 306 |     for file_path in model_path_list:\n 307 |         model_name = file_path.split('/')[-1]\n 308 |         GSUtilHelper.cp(file_path, f'/tmp/model_spec/{model_name}')\n 309 |         tmp_file_list.append(f'/tmp/model_spec/{model_name}')\n 310 |         model = UMEModel(f'/tmp/model_spec/{model_name}')\n 311 |         final_outputs = [model.name+'_'+c for c in model.outputs]\n 312 |         model_outputs.extend(final_outputs)\n 313 |     data = data.withColumn(\"index\", F.monotonically_increasing_id()) # fix\n 314 |     scorer = ModelScorer(spark, validate=False)\n 315 |     data = scorer.create_score_df(input_df = data, \n 316 |                                   mfile_paths = tmp_file_list)\n 317 |     meta_cols = [c for c in data.columns if c.startswith(\"driver_\")]\n 318 |     data = data.select(*(col(c).cast(\"float\").alias(c) for c in model_outputs), *meta_cols).repartition(400)\n 319 |     data.write.format(\"parquet\").mode(\"overwrite\").save(output_path)\n 320 |     return\n 321 | from model_automation.gcp import dataproc_config\n 322 | from aml import cloud_v1 as cloud\n 323 | gcp_project = \"ccg24-hrzana-gds-focus\"\n 324 | client = cloud.TrainingClient(gcp_project=gcp_project)\n 325 | dataproc_config['medium']['spark_properties']['spark.driver.maxResultSize'] = '48g'\n 326 | dataproc_config['medium']['spark_properties']['spark.yarn.driver.memoryOverhead'] = '4096'\n 327 | dataproc_config['medium']['spark_properties']['spark.yarn.executor.memoryOverhead'] = '4096'\n 328 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/umes/ | grep 'robust'\n 329 | # !gsutil cp /projects/gds-focus/data/catch/IL/CAM_variable/models/umes/new_var_tot_500_woe.m gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/umes/\n 330 | tfrecords_job_id = client.create_spark_job(\n 331 |     func= raw2woe,\n 332 |     packages_to_install=['PyDPU==1.1.0',\n 333 |                            'pyScoring==0.8.0.1.post1',\n 334 |                            'gcsfs',\n 335 |                            'automation_utils==0.3.0',\n 336 |                            'pyjnius<1.5.0'],\n 337 |     # params\n 338 |     input_path = f\"gs://pypl-bkt-rsh-row-std-gds-qpull/madmen/output/cchen16_1736909438\",\n 339 |     output_path = f\"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_data_drift/data/cam_il_20240926_training_data_new_var_woe\",\n 340 |     model_path_list=['gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/umes/adaptive_woe.m',\n 341 |                      'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/umes/robust_woe.m',\n 342 |                      'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/umes/new_var_tot_500_woe.m',\n 343 |                     ],\n 344 |     file_number = 256,\n 345 |     # gcp config\n 346 |     **dataproc_config['large']\n 347 | )\n 348 | client.wait_job_for_completion(tfrecords_job_id,\n 349 |                                time_to_sleep=300)\n 350 | client.get_job_driver_logs(tfrecords_job_id)\n 351 | # ## woe2tfrecords\n 352 | gcs_data_path = 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data'\n 353 | # !gsutil -m rm -r {gcs_data_path}/EvalNormalized\n 354 | # %maglev copy /apps/risk/det/cchen16/il_new_variable/cam_20240912_all/cam_20240912_all_in_one/evals/Eval1/EvalNormalized {gcs_data_path}/\n 355 | def woe2tfrecord(woe_data_gcp_path1,\n 356 |                  woe_data_gcp_path2,\n 357 |                  output_path,\n 358 |                  feature_list1,\n 359 |                  feature_list2,\n 360 |                  meta_cols=[],\n 361 |                  file_number=256,\n 362 |                  ):\n 363 |     from py_dpu import load_parquet, load_pig, rm_df_alias\n 364 |     # from automation_utils.spark.tfrecord import store_tfrecord\n 365 |     from model_automation.gcp import tfrecord_convert\n 366 |     from automation_utils.gcp.GSUtil import GSUtilHelper\n 367 |     from pyspark.sql.functions import min, max\n 368 |     import pyspark.sql.functions as F\n 369 |     from pyspark.sql.functions import array, col, lit, when\n 370 |     import json\n 371 |     data_1 = load_pig(spark, woe_data_gcp_path1)\n 372 |     data_2 = load_parquet(spark, woe_data_gcp_path2)\n 373 |     data_2 = data_2.drop_duplicates(subset=['driver_trans_id'])\n 374 |     data_2 = data_2.withColumn(\"driver_trans_id_new\", col(\"driver_trans_id\"))\n 375 |     data_1 = data_1.drop_duplicates(subset=['driver_trans_id'])\n 376 |     data_1 = rm_df_alias(spark, data_1, prefix='EvalNorm::')\n 377 |     data_1_used_cols = [c for c in meta_cols if c in data_1.columns]+feature_list1\n 378 |     data_2_used_cols = [\"driver_trans_id_new\"]+feature_list2\n 379 |     left = data_1.select(*data_1_used_cols)\n 380 |     right = data_2.select(*data_2_used_cols)\n 381 |     data = left.join(right, left.driver_trans_id == right.driver_trans_id_new, how=\"inner\").drop(\"driver_trans_id_new\")\n 382 |     data = data.withColumn('rand', F.rand(seed=42)).orderBy('rand').drop('rand') # shuffle\n 383 |     feature_list = feature_list1+feature_list2\n 384 |     used_cols = meta_cols + feature_list\n 385 |     data_filters = \"\"\"\n 386 |         driver_is_brm3_4_11_15_bad != 2 \n 387 |         and driver_is_brm3_4_11_15_bad != '2' \n 388 |         and driver_is_brm3_4_11_15_bad != 2.0 \n 389 |         and driver_is_sndr_id_train_set == 1\n 390 |     \"\"\"\n 391 |     data = data.where(data_filters)\n 392 |                        'driver_is_brm3_4_11_15_bad']).groupby('driver_is_brm3_4_11_15_bad').count().show())\n 393 |     data = data.select(*used_cols)\n 394 |     data = data.withColumn(\"driver_product_flow_rollup\", when(col(\"driver_l2_product_flow\").isin(\"MS EC Non Recurring\", \"WPS Non Recurring\"), \"Branded XO\")  \n 395 |                                                         .when(col(\"driver_l2_product_flow\").isin(\"MS EC Recurring\", \"WPS Recurring\", \"Branded_Recurring\"), \"Branded Recurring\")  \n 396 |                                                         .when(col(\"driver_l2_product_flow\").isin(\"P2P-F&F\", \"P2P-G&S\", \"eBay MOR\"), col(\"driver_l2_product_flow\"))  \n 397 |                                                         .otherwise(\"Other\")\n 398 |                           )\n 399 |     data.write.format(\"parquet\").mode(\"overwrite\").save(output_path+\"_parquet\")\n 400 |     # tfrecord_convert(\n 401 |     #     spark, \n 402 |     #     data_path=\"\", \n 403 |     #     tfrecord_path=output_path+\"_tfrecords\", \n 404 |     #     feature_list=feature_list, \n 405 |     #     drop_cols_list=[],\n 406 |     #     file_number=file_number, \n 407 |     #     fill_cols=[],\n 408 |     #     dataframe = data\n 409 |     # )\n 410 |     return\n 411 | from model_automation.gcp import dataproc_config\n 412 | from aml import cloud_v1 as cloud\n 413 | gcp_project = \"ccg24-hrzana-gds-focus\"\n 414 | client = cloud.TrainingClient(gcp_project=gcp_project)\n 415 | dataproc_config['large']['spark_properties']['spark.driver.maxResultSize'] = '48g'\n 416 | dataproc_config['large']['spark_properties']['spark.yarn.driver.memoryOverhead'] = '4096'\n 417 | dataproc_config['large']['spark_properties']['spark.yarn.executor.memoryOverhead'] = '4096'\n 418 | dataproc_config['large']['spark_properties']\n 419 | from model_automation.gcp import dataproc_config\n 420 | from aml import cloud_v1 as cloud\n 421 | gcp_project = \"ccg24-hrzana-gds-focus\"\n 422 | client = cloud.TrainingClient(gcp_project=gcp_project)\n 423 | dataproc_config['large']['spark_properties']['spark.driver.maxResultSize'] = '48g'\n 424 | dataproc_config['large']['spark_properties']['spark.yarn.driver.memoryOverhead'] = '4096'\n 425 | dataproc_config['large']['spark_properties']['spark.yarn.executor.memoryOverhead'] = '4096'\n 426 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-art/maglev/SF_SL/20240912/data/training/early_arrive_data_woe/ | head\n 427 | 's02_eg_txn_npph' in feature_list1\n 428 | feature_list1[:10]\n 429 | import pandas as pd\n 430 | feature_list = [\n 431 |     list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns), \\\n 432 |     list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/robust_tf_input.txt',sep='|').columns), \\\n 433 |    ]\n 434 | feature_list[0] = [item+\"_adp\" for item in feature_list[0]]\n 435 | feature_list[1] = [item+\"_rob\" for item in feature_list[1]]\n 436 | feature_list2 = feature_list[0] + feature_list[1]\n 437 | feature_list1 = read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse6000.txt\")\n 438 | meta_cols = read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/meta.column.names\")\n 439 | # !echo {gcs_data_path}/EvalNormalized\n 440 | # !gsutil cat gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/EvalNormalized/.pig_header | tr '\\a' '\\n' | wc -l\n 441 | tfrecords_job_id = client.create_spark_job(\n 442 |         func= woe2tfrecord,\n 443 |         packages_to_install=['PyDPU==1.1.0',\n 444 |                          'automation_utils==0.3.2',\n 445 |                          'gcsfs',\n 446 |                          'PySolCat',\n 447 |                          'model_automation==0.4.0.dev8'],\n 448 |         # params\n 449 |         woe_data_gcp_path1 = f\"{gcs_data_path}/EvalNormalized\",\n 450 |         woe_data_gcp_path2 = \"gs://pypl-bkt-rsh-row-std-gds-art/maglev/SF_SL/20240912/data/training/early_arrive_data_woe\",\n 451 |         output_path = f\"{gcs_data_path}/cam_20240912_recent_data_all_seg_parquet_top500\",\n 452 |         file_number = 64,\n 453 |         feature_list1 = feature_list1,\n 454 |         feature_list2 = feature_list2,\n 455 |         meta_cols = meta_cols,\n 456 |         # gcp config\n 457 |         **dataproc_config['large']\n 458 |     )\n 459 | client.wait_job_for_completion(tfrecords_job_id,\n 460 |                                time_to_sleep=300)\n 461 | # !echo {gcs_data_path}/cam_20240912_recent_data_all_seg_parquet_top500\n 462 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_all_seg_parquet_top500\n 463 | # !gsutil ls {gcs_data_path}/cam_20240912_recent_data_all_seg_parquet_top500\n 464 | client.get_job_driver_logs(tfrecords_job_id)\n 465 | f\"{gcs_data_path}/cam_20240912_recent_data_all_seg_parquet\"\n 466 | # !echo {gcs_data_path}/cam_20240912_recent_data\n 467 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data/ | head\n 468 | # !gsutil cat gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data/candidate_list > ./files/tf_candidate_list.txt\n 469 | # !gsutil -m cp -r gs://pypl-bkt-rsh-row-std-gds-qpull/madmen/output/cchen16_1731908487 {gcs_data_path}/\n 470 | # ## flow tfrecords\n 471 | def flow2tfrecord(woe_data_gcp_path1,\n 472 |                  woe_data_gcp_path2,\n 473 |                  output_path,\n 474 |                  feature_list1,\n 475 |                  feature_list2,\n 476 |                  meta_cols=[],\n 477 |                  file_number=256,\n 478 |                  ):\n 479 |     from py_dpu import load_parquet, load_pig, rm_df_alias\n 480 |     # from automation_utils.spark.tfrecord import store_tfrecord\n 481 |     from model_automation.gcp import tfrecord_convert\n 482 |     from automation_utils.gcp.GSUtil import GSUtilHelper\n 483 |     from pyspark.sql.functions import min, max\n 484 |     import pyspark.sql.functions as F\n 485 |     from pyspark.sql.functions import array, col, lit, when\n 486 |     import json\n 487 |     # data = load_parquet(spark, woe_data_gcp_path)\n 488 |     data_1 = load_pig(spark, woe_data_gcp_path1)\n 489 |     data_2 = load_parquet(spark, woe_data_gcp_path2)\n 490 |     data_2 = data_2.drop_duplicates(subset=['driver_trans_id'])\n 491 |     data_2 = data_2.withColumn(\"driver_trans_id_new\", col(\"driver_trans_id\"))\n 492 |     data_1 = data_1.drop_duplicates(subset=['driver_trans_id'])\n 493 |     data_1 = rm_df_alias(spark, data_1, prefix='EvalNorm::')\n 494 |     data_1_used_cols = [c for c in meta_cols if c in data_1.columns]+feature_list1\n 495 |     data_2_used_cols = [\"driver_trans_id_new\"]+feature_list2\n 496 |     left = data_1.select(*data_1_used_cols)\n 497 |     right = data_2.select(*data_2_used_cols)\n 498 |     data = left.join(right, left.driver_trans_id == right.driver_trans_id_new, how=\"inner\").drop(\"driver_trans_id_new\")\n 499 |     data = data.withColumn('rand', F.rand(seed=42)).orderBy('rand').drop('rand') # shuffle\n 500 |     feature_list = feature_list1+feature_list2\n 501 |     used_cols = meta_cols + feature_list\n 502 |     data_filters = \"\"\"\n 503 |         driver_is_brm3_4_11_15_bad != 2 \n 504 |         and driver_is_brm3_4_11_15_bad != '2' \n 505 |         and driver_is_brm3_4_11_15_bad != 2.0 \n 506 |         and driver_is_sndr_id_train_set == 1\n 507 |         and (driver_is_y_g == '1' or driver_is_y_g == '1.0')\n 508 |     \"\"\"\n 509 |     data = data.where(data_filters)\n 510 |                        'driver_is_brm3_4_11_15_bad']).groupby('driver_is_brm3_4_11_15_bad').count().show())\n 511 |     data = data.select(*used_cols)\n 512 |     data = data.withColumn(\"driver_product_flow_rollup\", when(col(\"driver_l2_product_flow\").isin(\"MS EC Non Recurring\", \"WPS Non Recurring\"), \"Branded XO\")  \n 513 |                                                         .when(col(\"driver_l2_product_flow\").isin(\"MS EC Recurring\", \"WPS Recurring\", \"Branded_Recurring\"), \"Branded Recurring\")  \n 514 |                                                         .when(col(\"driver_l2_product_flow\").isin(\"P2P-F&F\", \"P2P-G&S\", \"eBay MOR\"), col(\"driver_l2_product_flow\"))  \n 515 |                                                         .otherwise(\"Other\")\n 516 |                           )\n 517 |     data = data.withColumn('rand', F.rand(seed=42)).orderBy('rand').drop('rand') # shuffle\n 518 |     # data.write.format(\"parquet\").mode(\"overwrite\").save(output_path+\"_parquet\")\n 519 |     tfrecord_convert(\n 520 |         spark, \n 521 |         data_path=\"\", \n 522 |         tfrecord_path=output_path, \n 523 |         feature_list=feature_list, \n 524 |         drop_cols_list=[],\n 525 |         file_number=file_number, \n 526 |         fill_cols=[],\n 527 |         dataframe = data\n 528 |     )\n 529 |     # if len(filters) > 0:\n 530 |     #     data = data.where(filters)\n 531 |     # print(data.select(['driver_trans_id',\n 532 |     #                    'driver_is_brm3_4_11_15_bad']).groupby('driver_is_brm3_4_11_15_bad').count().show())\n 533 | #     used_cols = meta_cols + feature_list + ['driver_product_flow_rollup']\n 534 | #     data = data.select(*used_cols)\n 535 | #     tfrecord_convert(\n 536 | #         spark, \n 537 | #         data_path=\"\", \n 538 | #         tfrecord_path=output_path, \n 539 | #         feature_list=feature_list, \n 540 | #         drop_cols_list=[],\n 541 | #         file_number=file_number, \n 542 | #         fill_cols=[],\n 543 | #         dataframe = data\n 544 | #     )\n 545 |     return\n 546 | # feature_list = feature_list1+feature_list2\n 547 | meta_cols = read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/meta.column.names\")\n 548 | # data_filter = [\n 549 | #     (\"all_seg\", \"\", 32),\n 550 | #     (\"recur\", \" driver_product_flow_rollup == 'Branded Recurring' \", 32),\n 551 | #     (\"xo\", \" driver_product_flow_rollup == 'Branded XO' \", 32),\n 552 | #     (\"other\", \" driver_product_flow_rollup == 'Other' \", 32),\n 553 | #     (\"p2pff\", \" driver_product_flow_rollup == 'P2P-F&F' \", 32),\n 554 | #     (\"p2pgs\", \" driver_product_flow_rollup == 'P2P-G&S' \", 32),\n 555 | #     (\"ebaymor\", \" driver_product_flow_rollup == 'eBay MOR' \", 32), \n 556 | # ]\n 557 | def get_candidate_vars(var_path, suffix):\n 558 |     candidate_ls = read_line_file(var_path)\n 559 |     return [c+suffix for c in candidate_ls]\n 560 | tot_feature_list = get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/candidates_top500.txt',\"\")\n 561 | se_files = {\n 562 |     # 'all_seg':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt',\"\"),\n 563 |     # 'Tot':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/candidates_top500.txt',\"\"),\n 564 |     'recur':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedRecurring/columns/candidates_top500.txt',\"_seg6\"),\n 565 |     'xo':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedXO/columns/candidates_top500.txt',\"_seg5\"),\n 566 |     'other':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Other/columns/candidates_top500.txt',\"_seg10\"),\n 567 |     'p2pff':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PFF/columns/candidates_top500.txt',\"_seg7\"),\n 568 |     'p2pgs':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PGS/columns/candidates_top500.txt',\"_seg8\"),\n 569 |     'ebaymor':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_EbayMor/columns/candidates_top500.txt',\"_seg9\"),\n 570 | }\n 571 | job_ids = {}\n 572 | for name, fl in se_files.items():\n 573 |     feature_list1 = tot_feature_list + fl\n 574 |     job_ids[name] = client.create_spark_job(\n 575 |         func= flow2tfrecord,\n 576 |         packages_to_install=['PyDPU==1.1.0',\n 577 |                          'automation_utils==0.3.2',\n 578 |                          'gcsfs',\n 579 |                          'PySolCat',\n 580 |                          'model_automation==0.4.0.dev8'],\n 581 |         # params\n 582 |         woe_data_gcp_path1 = f\"{gcs_data_path}/EvalNormalized\",\n 583 |         woe_data_gcp_path2 = \"gs://pypl-bkt-rsh-row-std-gds-art/maglev/SF_SL/20240912/data/training/early_arrive_data_woe\",\n 584 |         output_path = f\"{gcs_data_path}/cam_20240912_recent_data_yg_tot_{name}_top500_tf\",\n 585 |         file_number = 256,\n 586 |         feature_list1 = feature_list1,\n 587 |         feature_list2 = feature_list2,\n 588 |         meta_cols = meta_cols,\n 589 |         # gcp config\n 590 |         **dataproc_config['large']\n 591 |     )\n 592 | for k,v in job_ids.items():\n 593 | client.wait_job_for_completion(job_ids['Tot'],\n 594 |                                time_to_sleep=300)\n 595 | client.get_job_driver_logs(job_ids['recur'])\n 596 | # !gsutil ls {gcs_data_path}/ | grep 'cam_20240912_recent_data_yg_tot_'\n 597 | # ### concat all flow\n 598 | def flow2tfrecord_v2(woe_data_gcp_path1,\n 599 |                  woe_data_gcp_path2,\n 600 |                  output_path,\n 601 |                  feature_list1,\n 602 |                  feature_list2,\n 603 |                  meta_cols=[],\n 604 |                  file_number=256,\n 605 |                  ):\n 606 |     from py_dpu import load_parquet, load_pig, rm_df_alias\n 607 |     # from automation_utils.spark.tfrecord import store_tfrecord\n 608 |     from model_automation.gcp import tfrecord_convert\n 609 |     from automation_utils.gcp.GSUtil import GSUtilHelper\n 610 |     from pyspark.sql.functions import min, max\n 611 |     import pyspark.sql.functions as F\n 612 |     from pyspark.sql.functions import array, col, lit, when\n 613 |     import json\n 614 |     # data = load_parquet(spark, woe_data_gcp_path)\n 615 |     data_1 = load_pig(spark, woe_data_gcp_path1)\n 616 |     data_2 = load_parquet(spark, woe_data_gcp_path2)\n 617 |     data_2 = data_2.drop_duplicates(subset=['driver_trans_id'])\n 618 |     data_2 = data_2.withColumn(\"driver_trans_id_new\", col(\"driver_trans_id\"))\n 619 |     data_1 = data_1.drop_duplicates(subset=['driver_trans_id'])\n 620 |     data_1 = rm_df_alias(spark, data_1, prefix='EvalNorm::')\n 621 |     data_1_used_cols = [c for c in meta_cols if c in data_1.columns]+feature_list1\n 622 |     data_2_used_cols = [\"driver_trans_id_new\"]+feature_list2\n 623 |     left = data_1.select(*data_1_used_cols)\n 624 |     right = data_2.select(*data_2_used_cols)\n 625 |     data = left.join(right, left.driver_trans_id == right.driver_trans_id_new, how=\"inner\").drop(\"driver_trans_id_new\")\n 626 |     data = data.withColumn('rand', F.rand(seed=42)).orderBy('rand').drop('rand') # shuffle\n 627 |     feature_list = feature_list1+feature_list2\n 628 |     used_cols = meta_cols + feature_list\n 629 |     data_filters = \"\"\"\n 630 |         driver_is_brm3_4_11_15_bad != 2 \n 631 |         and driver_is_brm3_4_11_15_bad != '2' \n 632 |         and driver_is_brm3_4_11_15_bad != 2.0 \n 633 |         and driver_is_sndr_id_train_set == 1\n 634 |     \"\"\"\n 635 |             # and (driver_is_y_g == '1' or driver_is_y_g == '1.0')\n 636 |     data = data.where(data_filters)\n 637 |                        'driver_is_brm3_4_11_15_bad']).groupby('driver_is_brm3_4_11_15_bad').count().show())\n 638 |     data = data.select(*used_cols)\n 639 |     data = data.withColumn(\"driver_product_flow_rollup\", when(col(\"driver_l2_product_flow\").isin(\"MS EC Non Recurring\", \"WPS Non Recurring\"), \"Branded XO\")  \n 640 |                                                         .when(col(\"driver_l2_product_flow\").isin(\"MS EC Recurring\", \"WPS Recurring\", \"Branded_Recurring\"), \"Branded Recurring\")  \n 641 |                                                         .when(col(\"driver_l2_product_flow\").isin(\"P2P-F&F\", \"P2P-G&S\", \"eBay MOR\"), col(\"driver_l2_product_flow\"))  \n 642 |                                                         .otherwise(\"Other\")\n 643 |                           )\n 644 |     data = data.withColumn('rand', F.rand(seed=42)).orderBy('rand').drop('rand') # shuffle\n 645 |     # data.write.format(\"parquet\").mode(\"overwrite\").save(output_path+\"_parquet\")\n 646 |     tfrecord_convert(\n 647 |         spark, \n 648 |         data_path=\"\", \n 649 |         tfrecord_path=output_path, \n 650 |         feature_list=feature_list, \n 651 |         drop_cols_list=[],\n 652 |         file_number=file_number, \n 653 |         fill_cols=[],\n 654 |         dataframe = data\n 655 |     )\n 656 |     return\n 657 | # feature_list = feature_list1+feature_list2\n 658 | meta_cols = read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/meta.column.names\")\n 659 | # data_filter = [\n 660 | #     (\"all_seg\", \"\", 32),\n 661 | #     (\"recur\", \" driver_product_flow_rollup == 'Branded Recurring' \", 32),\n 662 | #     (\"xo\", \" driver_product_flow_rollup == 'Branded XO' \", 32),\n 663 | #     (\"other\", \" driver_product_flow_rollup == 'Other' \", 32),\n 664 | #     (\"p2pff\", \" driver_product_flow_rollup == 'P2P-F&F' \", 32),\n 665 | #     (\"p2pgs\", \" driver_product_flow_rollup == 'P2P-G&S' \", 32),\n 666 | #     (\"ebaymor\", \" driver_product_flow_rollup == 'eBay MOR' \", 32), \n 667 | # ]\n 668 | def get_candidate_vars(var_path, suffix):\n 669 |     candidate_ls = read_line_file(var_path)\n 670 |     return [c+suffix for c in candidate_ls]\n 671 | tot_feature_list = get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/candidates_top500.txt',\"\")\n 672 | se_files = {\n 673 |     # 'all_seg':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt',\"\"),\n 674 |     # 'Tot':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/candidates_top500.txt',\"\"),\n 675 |     'recur':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedRecurring/columns/candidates_top500.txt',\"_seg6\"),\n 676 |     'xo':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedXO/columns/candidates_top500.txt',\"_seg5\"),\n 677 |     'other':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Other/columns/candidates_top500.txt',\"_seg10\"),\n 678 |     'p2pff':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PFF/columns/candidates_top500.txt',\"_seg7\"),\n 679 |     'p2pgs':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PGS/columns/candidates_top500.txt',\"_seg8\"),\n 680 |     'ebaymor':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_EbayMor/columns/candidates_top500.txt',\"_seg9\"),\n 681 | }\n 682 | flow_fl = []\n 683 | for k,fl in se_files.items():\n 684 |     flow_fl.extend(fl)\n 685 | # feature_list1 = tot_feature_list + flow_fl\n 686 | feature_list1 = tot_feature_list\n 687 | feature_list1 = [item+\"_new_var\" for item in feature_list1]\n 688 | from model_automation.gcp import dataproc_config\n 689 | from aml import cloud_v1 as cloud\n 690 | gcp_project = \"ccg24-hrzana-gds-focus\"\n 691 | client = cloud.TrainingClient(gcp_project=gcp_project)\n 692 | dataproc_config['large']['spark_properties']['spark.driver.maxResultSize'] = '48g'\n 693 | dataproc_config['large']['spark_properties']['spark.yarn.driver.memoryOverhead'] = '4096'\n 694 | dataproc_config['large']['spark_properties']['spark.yarn.executor.memoryOverhead'] = '4096'\n 695 | job_id = client.create_spark_job(\n 696 |     func= flow2tfrecord_v2,\n 697 |     packages_to_install=['PyDPU==1.1.0',\n 698 |                      'automation_utils==0.3.2',\n 699 |                      'gcsfs',\n 700 |                      'PySolCat',\n 701 |                      'model_automation==0.4.0.dev8'],\n 702 |     # params\n 703 |     woe_data_gcp_path1 = f\"{gcs_data_path}/EvalNormalized\",\n 704 |     woe_data_gcp_path2 = \"gs://pypl-bkt-rsh-row-std-gds-art/maglev/SF_SL/20240912/data/training/early_arrive_data_woe\",\n 705 |     output_path = f\"{gcs_data_path}/cam_20240912_recent_data_overall_7217_tf\",\n 706 |     file_number = 256,\n 707 |     feature_list1 = feature_list1,\n 708 |     feature_list2 = feature_list2,\n 709 |     meta_cols = meta_cols,\n 710 |     # gcp config\n 711 |     **dataproc_config['large']\n 712 | )\n 713 | client.wait_job_for_completion(job_id,\n 714 |                                time_to_sleep=300)\n 715 | client.get_job_driver_logs(job_id)\n 716 | f\"{gcs_data_path}/cam_20240912_recent_data_overall_7217_tf\"\n 717 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_overall_7217_tf/\n 718 | # ### YG bad 4 downsample\n 719 | def flow2tfrecord_v3(woe_data_gcp_path1,\n 720 |                  woe_data_gcp_path2,\n 721 |                  output_path,\n 722 |                  feature_list1,\n 723 |                  feature_list2,\n 724 |                  meta_cols=[],\n 725 |                  file_number=256,\n 726 |                      frac=50,\n 727 |                  ):\n 728 |     from py_dpu import load_parquet, load_pig, rm_df_alias\n 729 |     # from automation_utils.spark.tfrecord import store_tfrecord\n 730 |     from model_automation.gcp import tfrecord_convert\n 731 |     from automation_utils.gcp.GSUtil import GSUtilHelper\n 732 |     from pyspark.sql.functions import min, max\n 733 |     import pyspark.sql.functions as F\n 734 |     from pyspark.sql.functions import array, col, lit, when\n 735 |     import json\n 736 |     data = load_parquet(spark, woe_data_gcp_path1)\n 737 |     data = rm_df_alias(spark, data, prefix='adaptive_woe_')\n 738 |     data = rm_df_alias(spark, data, prefix='robust_woe_')\n 739 |     data = rm_df_alias(spark, data, prefix='new_var_tot_500_woe_')\n 740 | #     data_1 = load_pig(spark, woe_data_gcp_path1)\n 741 | #     data_2 = load_parquet(spark, woe_data_gcp_path2)\n 742 | #     data_2 = data_2.drop_duplicates(subset=['driver_trans_id'])\n 743 | #     data_2 = data_2.withColumn(\"driver_trans_id_new\", col(\"driver_trans_id\"))\n 744 | #     data_1 = data_1.drop_duplicates(subset=['driver_trans_id'])\n 745 | #     data_1 = rm_df_alias(spark, data_1, prefix='EvalNorm::')\n 746 | #     data_1_used_cols = [c for c in meta_cols if c in data_1.columns]+feature_list1\n 747 | #     data_2_used_cols = [\"driver_trans_id_new\"]+feature_list2\n 748 | #     left = data_1.select(*data_1_used_cols)\n 749 | #     right = data_2.select(*data_2_used_cols)\n 750 | #     data = left.join(right, left.driver_trans_id == right.driver_trans_id_new, how=\"inner\").drop(\"driver_trans_id_new\")\n 751 | #     data = data.withColumn('rand', F.rand(seed=42)).orderBy('rand').drop('rand') # shuffle\n 752 | #     print('*'*100)\n 753 |     meta_cols = [c for c in data.columns if c.startswith(\"driver_\")]\n 754 |     feature_list = feature_list1+feature_list2\n 755 |     used_cols = meta_cols+feature_list\n 756 |     data_filters = \"\"\"\n 757 |         driver_is_brm3_4_11_15_bad != 2 \n 758 |         and driver_is_brm3_4_11_15_bad != '2' \n 759 |         and driver_is_brm3_4_11_15_bad != 2.0 \n 760 |         and driver_is_sndr_id_train_set == 1\n 761 |     \"\"\"\n 762 |             # and (driver_is_y_g == '1' or driver_is_y_g == '1.0')\n 763 |     data = data.where(data_filters)\n 764 |                        'driver_is_brm3_4_11_15_bad']).groupby('driver_is_brm3_4_11_15_bad').count().show())\n 765 |     data = data.select(*used_cols)\n 766 |     data = data.withColumn(\"driver_product_flow_rollup\", when(col(\"driver_l2_product_flow\").isin(\"MS EC Non Recurring\", \"WPS Non Recurring\"), \"Branded XO\")  \n 767 |                                                         .when(col(\"driver_l2_product_flow\").isin(\"MS EC Recurring\", \"WPS Recurring\", \"Branded_Recurring\"), \"Branded Recurring\")  \n 768 |                                                         .when(col(\"driver_l2_product_flow\").isin(\"P2P-F&F\", \"P2P-G&S\", \"eBay MOR\"), col(\"driver_l2_product_flow\"))  \n 769 |                                                         .otherwise(\"Other\")\n 770 |                           )\n 771 |     # bad4 logic\n 772 |     df_good = data.where(\" driver_brm_bad_tag_assigned in ('~')  \")\n 773 |     df_bad3_11_15 = data.where(\" driver_brm_bad_tag_assigned in ('11_FREE_UNAUTH_CBK', '3_STOLEN_CC_CB') \")\n 774 |     # df_bad4_keep = data.where(\" driver_brm_bad_tag_assigned == '4_UNAUTH_RESTR' and (driver_nloss > 0 or driver_cnsr_seg in ('C','T','G')) \")\n 775 |     # df_bad4_left = data.where(\" driver_brm_bad_tag_assigned == '4_UNAUTH_RESTR' and driver_nloss <= 0 and driver_cnsr_seg in ('Y') \")\\\n 776 |     #                    .sample(withReplacement=False, fraction=frac)\n 777 |     df_bad4_keep = data.where(\" driver_brm_bad_tag_assigned == '4_UNAUTH_RESTR' and driver_nloss > 0 \")\n 778 |     df_bad4_left = data.where(\" driver_brm_bad_tag_assigned == '4_UNAUTH_RESTR' and driver_nloss <= 0 \")\\\n 779 |                        .sample(withReplacement=False, fraction=frac)\n 780 |     data = df_good.union(df_bad3_11_15).union(df_bad4_keep).union(df_bad4_left)\n 781 |     data = data.withColumn('rand', F.rand(seed=42)).orderBy('rand').drop('rand') # shuffle\n 782 |     # data.write.format(\"parquet\").mode(\"overwrite\").save(output_path+\"_parquet\")\n 783 |     tfrecord_convert(\n 784 |         spark, \n 785 |         data_path=\"\", \n 786 |         tfrecord_path=output_path, \n 787 |         feature_list=feature_list, \n 788 |         drop_cols_list=[],\n 789 |         file_number=file_number, \n 790 |         fill_cols=[],\n 791 |         dataframe = data\n 792 |     )\n 793 |     # tfrecord_convert(\n 794 |     #     spark, \n 795 |     #     data_path=\"\", \n 796 |     #     tfrecord_path=output_path+\"_yg\", \n 797 |     #     feature_list=feature_list, \n 798 |     #     drop_cols_list=[],\n 799 |     #     file_number=file_number, \n 800 |     #     fill_cols=[],\n 801 |     #     dataframe = data.where(\" driver_is_y_g == '1' or driver_is_y_g == '1.0' \")\n 802 |     # )\n 803 |     # print(f\"convert partquet file to tf record successfully!\")\n 804 |     # print('*'*100)        \n 805 |     return\n 806 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_data_drift/data/cam_il_20240926_training_data_new_var_woe/ | head\n 807 | # !gsutil cp gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_data_drift/data/cam_il_20240926_training_data_new_var_woe/part-00000-44f2716e-4ced-4bcf-97b4-872cb63c8f5c-c000.snappy.parquet ./data/\n 808 | tmp = pd.read_parquet(\"/projects/gds-focus/data/catch/IL/CAM_variable/data/part-00000-44f2716e-4ced-4bcf-97b4-872cb63c8f5c-c000.snappy.parquet\")\n 809 | tmp.columns[-50:]\n 810 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-art/maglev/SF_SL/20240926/data/training/early_arrive_data_woe\n 811 | job_ids = {}\n 812 | for frac in {25,50,75}:\n 813 |     job_ids[str(frac)] = client.create_spark_job(\n 814 |         func= flow2tfrecord_v3,\n 815 |         packages_to_install=['PyDPU==1.1.0',\n 816 |                          'automation_utils==0.3.2',\n 817 |                          'gcsfs',\n 818 |                          'PySolCat',\n 819 |                          'model_automation==0.4.0.dev8'],\n 820 |         # params\n 821 |         woe_data_gcp_path1 = f\"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_data_drift/data/cam_il_20240926_training_data_new_var_woe\",\n 822 |         woe_data_gcp_path2 = \"gs://pypl-bkt-rsh-row-std-gds-art/maglev/SF_SL/20240926/data/training/early_arrive_data_woe\",\n 823 |         output_path = f\"{gcs_data_path}/cam_20240926_recent_data_overall_7217_tf_bad4_old_{frac}\",\n 824 |         file_number = 80,\n 825 |         feature_list1 = feature_list1,\n 826 |         feature_list2 = feature_list2,\n 827 |         meta_cols = meta_cols,\n 828 |         frac = frac/100,\n 829 |         # gcp config\n 830 |         **dataproc_config['large']\n 831 |     )\n 832 | client.wait_job_for_completion(job_ids['75'],\n 833 |                                time_to_sleep=300)\n 834 | client.get_job_driver_logs(job_ids['75'])\n 835 | # !gsutil ls {gcs_data_path}/ | grep 'cam_20240926_recent_data_overall_7217_tf_bad4_'",
    "rmr_agent/repos/CAM_variable_research/1_1_cam_precheck.py": "   1 | # # import packages\n   2 | # %url -c horton\n   3 | import sys,os\n   4 | username = os.environ['NB_USER']\n   5 | util_path = '/projects/gds-focus/data/catch/IL/CAM_variable/assets'\n   6 | sys.path.append(util_path)\n   7 | from helper import *\n   8 | import pandas as pd\n   9 | import json\n  10 | import numpy as np\n  11 | from automation_utils.spark.session import get_spark\n  12 | from automation_utils.common.time import get_past_month_date\n  13 | from datetime import datetime\n  14 | from py_dpu import CacheReader,save_pig, load_pig, save_pig\n  15 | from pyspark.sql.types import DoubleType\n  16 | import pyspark.sql.functions as F\n  17 | from pyspark.sql.window import Window\n  18 | from pyspark.sql.functions import col, row_number, datediff, to_date, lit, when, broadcast\n  19 | from dateutil.relativedelta import relativedelta\n  20 | pd.set_option('display.max_rows', 2000)\n  21 | pd.set_option('display.max_columns', 500)\n  22 | pd.set_option('display.width', 1000)\n  23 | pd.set_option('display.max_colwidth', 100)\n  24 | # pd.set_option?\n  25 | # # Parameters\n  26 | start_dt = datetime(2024, 6, 11)\n  27 | end_dt = datetime(2024, 9, 9)\n  28 | start_dt\n  29 | # # sanity check\n  30 | # ## solcat check\n  31 | #\n  32 | import importlib  \n  33 | import helper\n  34 | importlib.reload(helper) \n  35 | from helper import *\n  36 | help(solcat_precheck)\n  37 | solcat_precheck(var_path=\"/projects/gds-focus/data/catch/IL/CAM_variable/files/raw_candidate_variables.txt\", \n  38 |                 start_dt=start_dt, \n  39 |                 remove_pattern_path=\"/projects/gds-focus/data/catch/IL/CAM_variable/assets/var_remove_pattern.txt\", \n  40 |                 column_folder=\"/projects/gds-focus/data/catch/IL/CAM_variable/files/candidate_pool_columns\")\n  41 | # ### Incremental vars\n  42 | # !cat /projects/gds-focus/data/catch/IL/CAM_variable/files/candidate_pool_columns/forceremove.column.names\n  43 | load_incre_vars(st='2024-01-01', \n  44 |                 et='2024-06-11', \n  45 |                 target_cp='ConsolidatedFunding', \n  46 |                 incre_var_path=\"/projects/gds-focus/data/catch/IL/CAM_variable/files/incre_new_variables_list.txt\"\n  47 |                )\n  48 | solcat_precheck(var_path=\"/projects/gds-focus/data/catch/IL/CAM_variable/files/incre_new_variables_list.txt\", \n  49 |                 start_dt=start_dt,\n  50 |                 remove_pattern_path=\"/projects/gds-focus/data/catch/IL/CAM_variable/assets/var_remove_pattern.txt\", \n  51 |                 column_folder=\"/projects/gds-focus/data/catch/IL/CAM_variable/files/incre_pool_columns\")\n  52 | # !cat /projects/gds-focus/data/catch/IL/CAM_variable/files/incre_pool_columns/forceremove.column.names\n  53 | # ## sample value check\n  54 | from PySolCat import SolCatOnlineVarLoader, SolCatSolutionLoader\n  55 | from PySolCat import OutputTransform\n  56 | import datetime\n  57 | import json\n  58 | import requests\n  59 | import pandas as pd\n  60 | onlineVarLoader = SolCatOnlineVarLoader(\"PAZ\")\n  61 | solcatLoader = SolCatSolutionLoader(\"PAZ\")\n  62 | QMONITOR_BASE_URL = \"http://lvsriskgrds3.lvs.paypal.com:8097/qmonitor-api/variables/variable_chart/?checkpoint_name={checkpoint_name}&model_name={variable_track_name}&variable_name={variable_name}&from={from_date}&to={to_date}&by_hour=true\"\n  63 | numeric_columns = ['P50', 'P25', 'P75', 'P01', 'P99', 'P95', 'P05', 'Mean', 'Max', 'Min']\n  64 | boolean_columns = ['false', 'true']\n  65 | to_date = (datetime.datetime.now() - datetime.timedelta(days=7)).strftime('%Y-%m-%d')\n  66 | from_date = (datetime.datetime.now() - datetime.timedelta(days=67)).strftime('%Y-%m-%d')\n  67 | def get_df_by_cp_and_status(checkpoint_name, variable_track_name, sample_status):\n  68 |    try:\n  69 |       response = onlineVarLoader.get_online_var_by_cp_status(checkpoint=checkpoint_name, status=sample_status)\n  70 |       responseInDictList = OutputTransform.convertToDict(response)\n  71 |       try:\n  72 |             ovl_df = pd.DataFrame(responseInDictList)\n  73 |       except Exception as e:\n  74 |             raise Exception(\"Exception: Failed to create Pandas DataFrame from onlineVarLoader\")\n  75 |       request = {\n  76 |             'filter': {\n  77 |                'name': ovl_df[\"_variable_name\"].tolist(),\n  78 |                'solution_type': \"variable\",\n  79 |                'sub_type': \"online\",\n  80 |                'source': \"rtcs\",\n  81 |                'metadata': {\n  82 |                   'basic_info': {\n  83 |                         'variable_type': \"component\"\n  84 |                   }\n  85 |                }\n  86 |             },\n  87 |             'output': ['name', 'metadata.impl_info.component_error_codes'],\n  88 |             'scroll_params': {\n  89 |                'scroll_id': ''\n  90 |             }\n  91 |       }\n  92 |       response = solcatLoader.send_list_request(request)\n  93 |       try:\n  94 |             scl_df = pd.DataFrame(response)\n  95 |       except Exception as e:\n  96 |             raise Exception(\"Exception: Failed to create Pandas DataFrame from solcatLoader\")\n  97 |       scl_df_name = scl_df[\"solution\"].apply(pd.Series).drop(columns=[\"metadata\"])\n  98 |       scl_df_error = scl_df[\"solution\"].apply(pd.Series)[\"metadata\"].apply(pd.Series)[\"impl_info\"].apply(pd.Series)[\"component_error_codes\"]\n  99 |       scl_df_fin = pd.concat([scl_df_name, scl_df_error], axis=1)\n 100 |       final = pd.merge(ovl_df, scl_df_fin, left_on='_variable_name', right_on='name', how='left')\n 101 |       return split_df_to_type(final, checkpoint_name, variable_track_name)\n 102 |    except Exception as e:\n 103 |       raise Exception(\"Exception: \" + repr(e))\n 104 | def split_df_to_type(df, checkpoint_name, variable_track_name):\n 105 |    try:\n 106 |       if(df is None or df.empty):\n 107 |             raise Exception(\"Sorry, DF is empty or None\")\n 108 |             return None, None, None\n 109 |       mixed = df.query(\"(value_type == 'continuous' & return_type in ['string', '', 'unknown']) | (value_type == 'categorical' & return_type in ['integer', 'long', 'double', '', 'unknown']) | (value_type == '')\")\n 110 |       mixed_variables = mixed[\"_variable_name\"].tolist()\n 111 |       numeric_variables, categorical_variables, unknown_variables = organize_mixed_variables(mixed_variables, checkpoint_name, variable_track_name)\n 112 |       continuous = df.query(\"(value_type == 'continuous' & return_type in ['double', 'integer', 'long']) | (_variable_name in @numeric_variables)\")\n 113 |       categorical = df.query(\"(value_type == 'categorical' & return_type == 'string') | (_variable_name in @categorical_variables)\")\n 114 |       unknown = df.query(\"_variable_name in @unknown_variables\")\n 115 |       assert(len(df) == len(continuous + categorical + unknown))\n 116 |       return continuous, categorical, unknown\n 117 |    except Exception as e:\n 118 |       raise Exception(\"Exception: \" + repr(e))\n 119 | def is_number(item):\n 120 |    try:\n 121 |       float(item)\n 122 |       return True\n 123 |    except ValueError:\n 124 |       return False\n 125 | def organize_mixed_variables(mixed_variables, checkpoint_name, variable_track_name):\n 126 |    numeric_variables = []\n 127 |    categorical_variables = []\n 128 |    unknown_variables = []\n 129 |    for variable in mixed_variables:\n 130 |       columns = get_qmonitor_info(checkpoint_name, variable_track_name, variable)\n 131 |       if columns is not None:\n 132 |             if len(columns) == 1:\n 133 |                unknown_variables.append(variable)\n 134 |             elif any(i in columns for i in numeric_columns) or all(is_number(i) for i in columns[:-1]):\n 135 |                numeric_variables.append(variable)\n 136 |             elif any(i in columns for i in boolean_columns):\n 137 |                numeric_variables.append(variable)\n 138 |             else:\n 139 |                categorical_variables.append(variable)\n 140 |       else:\n 141 |             unknown_variables.append(variable)\n 142 |    return numeric_variables, categorical_variables, unknown_variables\n 143 | def request_to_dataframe(json_data):\n 144 |    try:\n 145 |       columns = []\n 146 |       json_data = json_data['metrics']\n 147 |       for i in json_data:\n 148 |             for col_name in json_data[i]:\n 149 |                columns.append(col_name)\n 150 |       return columns\n 151 |    except Exception as e:\n 152 |       raise Exception(\"Exception: \" + repr(e))\n 153 | def get_qmonitor_info(checkpoint_name, variable_track_name, variable_name):\n 154 |    try:\n 155 |       url = QMONITOR_BASE_URL.format(checkpoint_name=checkpoint_name,\n 156 |                                        variable_track_name=variable_track_name,\n 157 |                                        variable_name=variable_name,\n 158 |                                        from_date=from_date,\n 159 |                                        to_date=to_date)\n 160 |       request = requests.get(url)\n 161 |       return request_to_dataframe(request.json())\n 162 |    except:\n 163 |       return None\n 164 | checkpoint_name = \"EvalAssetRisk\"\n 165 | variable_track_name = \"EvalAssetRiskAllInOneTrack\"\n 166 | sample_status = [\"audit clean\", \"implemented\"]\n 167 | numerical, categorical, unknown = get_df_by_cp_and_status(checkpoint_name, variable_track_name, sample_status)\n 168 | check_list = [\n 169 | 'acct_acct_rmphndcsn_cnt_sw90_30d',\n 170 | 'xai_rc_account_profile_richness_cnt_7d',\n 171 | 'ucc_eml_email_patmptn_sum_abs_amt_sw72_72h',\n 172 | 'session_ip_ip_s_patmptn_sum_abs_amt_sw72_5m',\n 173 | 'ucc_eml_email_patmptn_last10_num_eml_domain_30m',\n 174 | 'acct_addrchg_remove_cnt_sw90_30d',\n 175 | 'acct_addrchg_add_cnt_sw90_30d',\n 176 | 'acct_addrchg_add_cnt_sw90_7d',\n 177 | 'acct_addrchg_add_cnt_sw72_6h',\n 178 | 'rcvr_egonet_txn_grph_rbb_txn_weight_power_law',\n 179 | 'xai_rc_account_geographic_riskiness_cnt_7d',\n 180 | 'xai_rc_bank_account_riskiness_cnt_7d',\n 181 | 'ucc_phn_ctry_phone_patmptn_last10_num_eml_domain_60d',\n 182 | 'xai_rc_collusion_cnt_7d',\n 183 | 'xai_rc_payment_amount_cnt_7d',\n 184 | 'xai_rc_payment_velocity_cnt_7d',\n 185 | 'xai_rc_suspicious_address_cnt_7d',\n 186 | 'xai_rc_vid_riskiness_and_activeness_cnt_7d',\n 187 | 'xai_rc_ato_score_7d',\n 188 | 'xai_rc_login_velocity_score_7d',\n 189 | 'xai_rc_multiple_txn_failure_score_7d',\n 190 | 'xai_rc_payment_amount_score_7d',\n 191 | 'ucc_eml_email_patmptn_last10_num_ctry_phone_60d',\n 192 | 'ucc_phn_ctry_phone_patmptn_last10_num_ip_30m',\n 193 | 'cnt_ntwrk_30d',\n 194 | 'cc_hash_cc_hash_patmptn_last10_num_ip_9d',\n 195 | 'ucc_eml_email_patmptn_last10_num_sfl_name_5m',\n 196 | 'dyson_appguid_rda_dev_id_patmptn_cnt_sw72_48h',\n 197 | 'ucc_eml_email_patmptn_last10_num_lastcc_90d',\n 198 | 'cc_hash_cc_hash_patmptn_last10_num_ctry_phone_60d',\n 199 | 'ucc_phn_ctry_phone_patmptn_last10_num_ip_90d',\n 200 | 'fn_cookie_rda_dev_id_patmptn_sum_abs_amt_sw72_30m',\n 201 | 'dyson_appguid_rda_dev_id_patmptn_cnt_sw72_72h',\n 202 | 'cc_hash_cc_hash_patmptn_cnt_sw72_30m',\n 203 | 'ucc_eml_email_patmptn_last10_num_lastcc_60m',\n 204 | 'cc_hash_cc_hash_patmptn_last10_num_ship_addr_24h',\n 205 | 'ucc_phn_ctry_phone_patmptn_last10_num_rda_devid_5m',\n 206 | ]\n 207 | dashboard = (check_list, [\"ConsolidatedFunding\"]).reset_index()\n 208 | dashboard\n 209 | data = pd.read_csv(\"/projects/gds-focus/data/catch/IL/CAM_variable/incre_data_sample.csv\", sep='\\x07', low_memory=False)\n 210 | data.dtypes\n 211 | candidate_list = read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/incre_pool_columns/candidate.column.names\")\n 212 | categorical_list = read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/incre_pool_columns/categorical.column.names\")\n 213 | len(categorical_list)\n 214 | diff_1 = list(set(cate_list)-set(categorical_list))\n 215 | diff_2 = list(set(categorical_list)-set(cate_list))\n 216 | dashboard = get_data_info(diff_1, [\"ConsolidatedFunding\"]).reset_index()\n 217 | dashboard\n 218 | dashboard = get_data_info(diff_2, [\"ConsolidatedFunding\"]).reset_index()\n 219 | dashboard\n 220 | diff_1\n 221 | diff_2\n 222 | set(diff_2)-set(none_list)\n 223 | data['BinHmac5thLastKey_2'].value_counts()\n 224 | data['v2403_ks_total_dk16_shift_cnt'].value_counts()\n 225 | 'v2403_ks_total_dk16_shift_cnt' in categorical_list\n 226 | data['v2403_ks_total_dk16_shift_cnt'].value_counts()\n 227 | data[diff_list].sample(frac=0.05)\n 228 | def check_type(candidate_list):\n 229 |     var_type = \n 230 |     for var in candidate_list:  \n 231 |         elif pd.api.types.is_numeric_dtype(df[var]):  \n 232 |             return 'num'\n 233 |         elif pd.api.types.is_categorical_dtype(df[var]) or df[var].dtype == 'object':  \n 234 |             return 'cate'\n 235 |         else:  \n 236 |             return 'mix'\n 237 | desc.T.reset_index()\n 238 | data['acct_acct_bankcmp_s_ach_cnt_sw90_30d']\n 239 | data.shape\n 240 | none_list = []\n 241 | cate_list = []\n 242 | mix_list = []\n 243 | for var in candidate_list:  \n 244 |     if data[var].isnull().all():\n 245 |         none_list.append(var)\n 246 |     elif pd.api.types.is_numeric_dtype(data[var]):  \n 247 |         pass\n 248 |     elif pd.api.types.is_categorical_dtype(data[var]) or data[var].dtype == 'object':  \n 249 |         cate_list.append(var)\n 250 |     else:  \n 251 |         mix_list.append(var)\n 252 | len(cate_list)\n 253 | len(mix_list)\n 254 | categorical_list = read_line_file(\"\")\n 255 | df = pd.read_csv(\"/projects/gds-focus/data/catch/IL/CAM_variable/sample.csv\", sep='\\x07', low_memory=False)\n 256 | df.shape\n 257 | candidate_list = read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/candidate_pool_columns/candidate.column.names\")\n 258 | categorical_list = read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/candidate_pool_columns/categorical.column.names\")\n 259 | none_list = []\n 260 | cate_list = []\n 261 | mix_list = []\n 262 | for var in candidate_list:  \n 263 |     if df[var].isnull().all():\n 264 |         none_list.append(var)\n 265 |     elif pd.api.types.is_numeric_dtype(df[var]):  \n 266 |         pass\n 267 |     elif pd.api.types.is_categorical_dtype(df[var]) or df[var].dtype == 'object':  \n 268 |         cate_list.append(var)\n 269 |     else:  \n 270 |         mix_list.append(var)\n 271 | diff_1 = list(set(cate_list)-set(categorical_list))\n 272 | diff_2 = list(set(categorical_list)-set(cate_list))\n 273 | pd.api.types.is_numeric_dtype(df['txnSntPaths3h_curr_txn_amt_dist_from_avg'])\n 274 | df['hrzn_rdi_mg_co_txn_script_prb'].unique()\n 275 | len(diff_1)\n 276 | for col in diff_1:\n 277 |     if df[col].nunique() <= 10:\n 278 | for col in diff_2:\n 279 |     if df[col].nunique() > 10:\n 280 | dashboard = get_data_info(['intp_last_sent_pmt_txn_status','dyson_acct_conf_version'], [\"ConsolidatedFunding\"]).reset_index()\n 281 | dashboard\n 282 | df['intp_last_sent_pmt_txn_status'].unique()\n 283 | df['dyson_acct_conf_version'].unique()\n 284 | df['hrzn_rdi_mg_co_lgn_emulator_prb'].unique()\n 285 | pd.api.types.is_object_dtype(df['intp_last_sent_pmt_txn_status'])\n 286 | pd.api.types.is_cate_dtype(df['v73_max_cnt_z_score'])\n 287 | df['isTrusted20_vid_exc'].nunique()\n 288 | 'icp_wda_s_nach_last_return_age' in categorical_list\n 289 | type(df['v73_max_cnt_z_score'].sort_values().tail(2).values[0])\n 290 | df['v52_is_bill_name_trust_for_cc'].value_counts()\n 291 | 'default_billing_addr_cntry_code' in categorical_list\n 292 | df[diff_1].sample(frac=0.01)\n 293 | df['idi_dispute_contact_buyer_vars_pypl_intrnl_cr_assmt'].value_counts()\n 294 | df[diff_2].sample(frac=0.01)\n 295 | # ### false case 1: solcat CONTINUOUS, actually categorical\n 296 | get_data_info(['transaction_type'], [\"ConsolidatedFunding\"]).reset_index()\n 297 | df['transaction_type'].value_counts()\n 298 | df['transaction_type'].unique()\n 299 | get_data_info(['intp_last_sent_pmt_txn_status'], [\"ConsolidatedFunding\"]).reset_index()\n 300 | df['intp_last_sent_pmt_txn_status'].value_counts()\n 301 | # ### false case 2: solcat CATEGORICAL, actually continuous\n 302 | icp_wda_s_txn_ach_dof\n 303 | get_data_info(['v2108_travelling_cookies_rawdata_client_screen_outer','icp_wda_s_txn_ach_dof'], [\"ConsolidatedFunding\"]).reset_index()\n 304 | df['v2108_travelling_cookies_rawdata_client_screen_outer'].value_counts()\n 305 | get_data_info(['mg_latitude_1d'], [\"ConsolidatedFunding\"]).reset_index()\n 306 | df['mg_latitude_1d'].value_counts().sort_index()\n 307 | # ### false case 3: solcat categorical, actually mixed\n 308 | get_data_info(['hrzn_rdi_mg_co_lgn_emulator_prb'], [\"ConsolidatedFunding\"]).reset_index()\n 309 | df['hrzn_rdi_mg_co_lgn_emulator_prb'].value_counts()\n 310 | def is_float_string(value):  \n 311 |     try:  \n 312 |         float(value)  \n 313 |         return True  \n 314 |     except (ValueError, TypeError):  \n 315 |         return False  \n 316 | def all_integers(float_values):  \n 317 |     return all(float(value).is_integer() for value in float_values if not pd.isna(value))  \n 318 | def sanity_check(sample_value, distinctCount, old_type):   \n 319 |     unique_values_set = set(sample_value)\n 320 |     versions_cnt = sum([1 for val in unique_values_set if isinstance(val, str) and val.count('.') > 1])\n 321 |     if versions_cnt > 0: return 'C'\n 322 |     float_values = [val for val in unique_values_set if isinstance(val, float) or (isinstance(val, str) and is_float_string(val))]\n 323 |     float_count = len(float_values)        \n 324 |     if float_count >= 1:\n 325 |         if all_integers(float_values) and old_type == 'N': return 'N'\n 326 |         elif all_integers(float_values) and old_type == 'C': return 'C' \n 327 |         else: return 'N'\n 328 |     else:\n 329 |         return 'C'\n 330 | def column_config_check(config_path):\n 331 |     import json\n 332 |     tps = []\n 333 |     conf = json.load(open(config_path, \"r\"))\n 334 |     for var in conf:\n 335 |         if var['columnFlag'] in ('Candidate','ForceRemove'):\n 336 |             var_name = var['columnName']\n 337 |             old_type = var['columnType']\n 338 |             new_type = sanity_check(var['sampleValues'], var['columnStats']['distinctCount'], old_type)\n 339 |             tps.append([var_name, old_type, new_type, \"::\".join(str(v) for v in sorted(var['sampleValues'][:15]))])\n 340 |     columns = ['var_name', 'old_type', 'new_type', 'sample_value']  \n 341 |     check_df = pd.DataFrame(tps, columns=columns)  \n 342 |     return check_df\n 343 | check_df_old = column_config_check('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/ColumnConfig.json')\n 344 | check_df_incre = column_config_check('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912_incre/shifu/cam24_il_Tot/ColumnConfig.json')\n 345 | check_df = pd.concat([check_df_old,\n 346 |                       check_df_incre\n 347 |                      ])\n 348 | check_df[(check_df.old_type == 'C')&(check_df.new_type == 'N')&(~check_df.sample_value.isna())].shape,\\\n 349 | check_df[(check_df.old_type == 'N')&(check_df.new_type == 'C')&(~check_df.sample_value.isna())].shape\n 350 | check_df[(check_df.old_type == 'C')&(check_df.new_type == 'N')&(~check_df.sample_value.isna())]\n 351 | check_df[(check_df.old_type == 'N')&(check_df.new_type == 'C')&(~check_df.sample_value.isna())]\n 352 | def get_sample_value(var):\n 353 |     col_data = df[var]\n 354 |     unique_values = col_data.unique()  \n 355 |     return \"::\".join(str(v) for v in unique_values[:5])\n 356 | dashboard['sample_value'] = dashboard['variable_name'].apply(lambda x:get_sample_value(x))\n 357 | dashboard['data_type_fixed']\n 358 | dashboard[dashboard.value_type != dashboard.data_type_fixed].shape\n 359 | df['bcn_cookie_addbank_num_country_6h'].unique()\n 360 | sanity_check(df, ['hrzn_rdi_mg_co_lgn_emulator_prb',\n 361 |                  'transaction_type',\n 362 |                  'v2108_travelling_cookies_rawdata_client_screen_outer',\n 363 |                  'intp_last_sent_pmt_txn_status',\n 364 |                   'mg_latitude_1d',\n 365 |                   'default_billing_addr_cntry_code',\n 366 |                   'ReceiverAccountNumber'\n 367 |                  ])\n 368 | check_df[check_df.var_name.isin(['hrzn_rdi_mg_co_lgn_emulator_prb',\n 369 |                  'transaction_type',\n 370 |                  'v2108_travelling_cookies_rawdata_client_screen_outer',\n 371 |                  'intp_last_sent_pmt_txn_status',\n 372 |                   'mg_latitude_1d',\n 373 |                   'default_billing_addr_cntry_code',\n 374 |                   'ReceiverAccountNumber'])]\n 375 | len(diff_1), len(diff_2)\n 376 | sanity_check(df, diff_1)\n 377 | col_data = df['ctx_cchash_cccrt_add_last10_num_acct_90d']\n 378 | unique_values = col_data.unique()  \n 379 | unique_values_set = set(unique_values)  \n 380 | versions_cnt = sum([1 for val in unique_values_set if isinstance(val, str) and val.count('.') > 1])\n 381 | float_values = [val for val in unique_values_set if isinstance(val, float) or (isinstance(val, str) and is_float_string(val))]\n 382 | float_count = len(float_values) \n 383 | all_integers(float_values)\n 384 | float_count\n 385 | float_values\n 386 | all(float(value).is_i() for value in float_values if not pd.isna(value))\n 387 | isinstance('16.0.3', float)\n 388 | float(np.nan).is_integer()\n 389 | pd.isna(value)\n 390 | float('1.0').is_integer(), float('1.1').is_integer(), float('2.814750e+14').is_integer()\n 391 | df['hrzn_rdi_mg_co_lgn_emulator_prb'].unique()\n 392 | float_count, non_float_count, total_values\n 393 | all(float(value).is_integer() for value in []) \n 394 | df['hrzn_rdi_mg_co_lgn_emulator_prb'].unique()\n 395 | df['intp_last_sent_pmt_txn_status'].value_counts()\n 396 | pd.api.types.is_categorical_dtype(df['intp_last_sent_pmt_txn_status'])\n 397 | df['intp_last_sent_pmt_txn_status'].apply(lambda x: isinstance(x, str) or isinstance(x, pd.Categorical)).sum()  \n 398 | df['intp_last_sent_pmt_txn_status'].dtype.kind\n 399 | candidate_list = check_df['var_name'].values.tolist()\n 400 | categorical_list = check_df[check_df.new_type == 'C']['var_name'].values.tolist()\n 401 | len(candidate_list), len(categorical_list)\n 402 | # !ls /projects/gds-focus/data/catch/IL/CAM_variable/files\n 403 | force_rm1 = read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/candidate_pool_columns/forceremove.column.names\")\n 404 | force_rm2 = read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/check_pool_columns/forceremove.column.names\")\n 405 | force_rm = list(set(force_rm1)|set(force_rm2))\n 406 | len(force_rm)\n 407 | write_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/check_pool_columns/candidate.column.names\", \n 408 |                 candidate_list)\n 409 | write_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/check_pool_columns/categorical.column.names\", \n 410 |                 categorical_list)\n 411 | fix_model_names = [\n 412 |   'cam24_il_Tot',\n 413 |   'cam24_il_Young',\n 414 |   'cam24_il_Guest',\n 415 |   'cam24_il_Top',\n 416 |   'cam24_il_Casual',\n 417 |   'cam24_il_BrandedXO',\n 418 |   'cam24_il_BrandedRecurring',\n 419 |   'cam24_il_P2PFF',\n 420 |   'cam24_il_P2PGS',\n 421 |   'cam24_il_EbayMor',\n 422 |   'cam24_il_Other',\n 423 |   'cam24_il_MidCAM22Score'\n 424 | ]\n 425 | local_shifu_path = '/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912_all/shifu'\n 426 | candidate_ls = []\n 427 | for model_name in fix_model_names:\n 428 |     tmp_ls = read_line_file(os.path.join(local_shifu_path, model_name,'columns/candidate.column.names'))\n 429 |     candidate_ls.extend(tmp_ls)\n 430 | candidate_ls = list(set(candidate_ls))\n 431 | write_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/check_pool_columns/oot_pull_vars.txt\", candidate_ls)",
    "rmr_agent/repos/CAM_variable_research/1_2_cam_postcheck.py": "   1 | # # Packages\n   2 | # %url -c horton\n   3 | import sys,os\n   4 | utils_path = \"/projects/gds-focus/data/makang/CAM24/source_code/cam_utils\"\n   5 | sys.path.append(utils_path)\n   6 | import pandas as pd\n   7 | from ShifuUtils import *\n   8 | from pyshifu.ShifuConf import shifuConf\n   9 | queue = \"risk_gds_focus\"\n  10 | shifuConf.set('hadoopJobQueue', queue)\n  11 | shifuConf.set('guagua.split.maxCombinedSplitSize', '3000000000')\n  12 | # shifuConf.set('guagua.split.maxCombinedSplitSize', '30000000')\n  13 | working_path = \"/projects/gds-focus/data/catch/IL/CAM_variable\"\n  14 | username = os.environ['NB_USER']\n  15 | os.chdir(working_path)\n  16 | os.environ['SHIFU_OPTS'] = '-Xms2G -Xmx4G'\n  17 | trigger_dt = \"20240912_all\"\n  18 | queue = \"risk_gds_focus\"\n  19 | segment_file = f\"{working_path}/files/cam_il_segment.file\"\n  20 | common_conf_path = f\"{working_path}/files/cam_il_common_model_config.json\"\n  21 | hdfs_base_folder = \"/apps/risk/det/cchen16/il_new_variable\"\n  22 | hdfsModelSetPath = f\"{hdfs_base_folder}/cam_{trigger_dt}\" \n  23 | local_path = f\"{working_path}/cam_{trigger_dt}\"\n  24 | local_shifu_path = f\"{working_path}/cam_{trigger_dt}/shifu\" \n  25 | local_cols_path = f\"{local_shifu_path}/columns\"\n  26 | raw_data_path = \"/apps/risk/det/cchen16/il_new_variable/data/cam_0912_recent_8k_var_pig_w_crc\"\n  27 | pd.set_option('display.max_rows', 2000)\n  28 | pd.set_option('display.max_columns', 500)\n  29 | pd.set_option('display.width', 1000)\n  30 | pd.set_option('display.max_colwidth', 100)\n  31 | # # stats check\n  32 | # ## basic check\n  33 | tot_shifu_path = f\"{local_shifu_path}/cam24_il_Tot\"\n  34 | tot_stats = pd.read_csv(os.path.join(tot_shifu_path, 'columns/ColumnStats.csv'))\n  35 | tot_stats[(tot_stats['columnType'] == 'C' )&(tot_stats.distinctCount > 256)&(tot_stats.iv > 0.05)&(tot_stats.columnFlag=='Candidate')]\\\n  36 | missing_cond = tot_stats['missingPercentage'] > 0.98\n  37 | iv_cond = tot_stats['iv'] < 1e-2\n  38 | stdDev_cond = tot_stats['stdDev'].isnull()\n  39 | disc_cond = tot_stats['distinctCount'] < 2\n  40 | disc_cond_2 = (tot_stats['columnType'] == 'C' )&( tot_stats['distinctCount'] > 256)\n  41 | basic_filterout_cond = (missing_cond | iv_cond | stdDev_cond | disc_cond | disc_cond_2 )\n  42 | basic_remove_ls = tot_stats.loc[basic_filterout_cond]\n  43 | len(basic_remove_ls)\n  44 | tot_stats = tot_stats.loc[~basic_filterout_cond]\n  45 | tot_stats.shape\n  46 | tot_stats['columnFlag'].unique()\n  47 | ms_cols = [i for i in tot_stats.columns if i.endswith('_missing_rate')]\n  48 | ms_remove_ls = tot_stats[tot_stats[tot_stats[ms_cols] > 0.98].any(axis=1)]['columnName'].values.tolist()\n  49 | len(ms_remove_ls) \n  50 | tot_stats = tot_stats[~tot_stats.columnFlag.isna()]\n  51 | list(tot_stats.columns)\n  52 | check_cols = [\n  53 |  'dataSet',\n  54 |  'columnFlag',\n  55 |  'columnName',\n  56 |  'columnNum',\n  57 |  'iv',\n  58 | '2024-06-01_iv',\n  59 |  '2024-07-01_iv',\n  60 |  '2024-08-01_iv',\n  61 |  '2024-09-01_iv',\n  62 |  'max',\n  63 |  'min',\n  64 |  'missingPercentage',\n  65 |  'weightedIv',\n  66 |  'columnType',\n  67 |  'psi',\n  68 |  'psiStd',\n  69 |  '2024-06-01_psi',\n  70 |  '2024-07-01_psi',\n  71 |  '2024-08-01_psi',\n  72 |  '2024-09-01_psi'\n  73 | ]\n  74 | tot_stats[(tot_stats.psi < 1.5)&(tot_stats.psiStd < 0.25)].shape[0],\\\n  75 | tot_stats[(tot_stats.psi > 5)&(tot_stats.psiStd > 1.15)].shape[0],\\\n  76 | tot_stats[(tot_stats.psi > 5)|(tot_stats.psiStd > 1.15)].shape[0],\n  77 | tot_stats[(tot_stats.psi > 5)&(tot_stats.psiStd > 1.15)&(tot_stats.iv < 0.05)].shape,\\\n  78 | tot_stats[(tot_stats.psi > 5)&(tot_stats.psiStd > 1.15)&(tot_stats.iv >= 0.05)].shape\n  79 | tot_stats[((tot_stats.psi > 5)|(tot_stats.psiStd > 1.15))&(tot_stats.iv < 0.05)].shape,\\\n  80 | tot_stats[((tot_stats.psi > 5)|(tot_stats.psiStd > 1.15))&(tot_stats.iv >= 0.05)].shape\n  81 | psi_cols = [i for i in tot_stats.columns if i.endswith('_psi')]\n  82 | # ## psi&iv check\n  83 | psi_thres = 5\n  84 | psiStd_thres = 1.15\n  85 | iv_thres = 0.02\n  86 | # psi_iv_remove_ls = tot_stats[((tot_stats.psi > psi_thres)|(tot_stats.psiStd > psiStd_thres))&(tot_stats.iv < iv_thres)]\n  87 | tot_stats[((tot_stats.psi > psi_thres)|(tot_stats.psiStd > psiStd_thres))&(tot_stats.iv < iv_thres)].shape\n  88 | tot_stats[((tot_stats.psi > psi_thres)|(tot_stats.psiStd > psiStd_thres))&(tot_stats.iv < iv_thres)].shape\n  89 | tot_stats[(tot_stats.missingPercentage > 0.95)&(tot_stats.iv < iv_thres)].shape\n  90 | # ## run check\n  91 | import importlib  \n  92 | import helper\n  93 | importlib.reload(helper) \n  94 | from helper import *\n  95 | fix_model_names = [\n  96 |   'cam24_il_Tot',\n  97 |   'cam24_il_Young',\n  98 |   'cam24_il_Guest',\n  99 |   'cam24_il_Top',\n 100 |   'cam24_il_Casual',\n 101 |   'cam24_il_BrandedXO',\n 102 |   'cam24_il_BrandedRecurring',\n 103 |   'cam24_il_P2PFF',\n 104 |   'cam24_il_P2PGS',\n 105 |   'cam24_il_EbayMor',\n 106 |   'cam24_il_Other',\n 107 |   'cam24_il_MidCAM22Score'\n 108 | ]\n 109 | candidate_ls = []\n 110 | for model_name in fix_model_names:\n 111 |     tmp_ls = read_l",
    "rmr_agent/repos/CAM_variable_research/2_1_cam_exist_var_shifu_multisegment.py": "   1 | # # Config preparation\n   2 | # %url -c horton\n   3 | # %ppauth\n   4 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-art/maglev/SF_SL/20240829/data/training/driver/\n   5 | import sys,os\n   6 | utils_path = \"/projects/gds-focus/data/makang/CAM24/source_code/cam_utils\"\n   7 | sys.path.append(utils_path)\n   8 | from model_automation.woe import MultiSegmentWoe\n   9 | import pandas as pd\n  10 | from ShifuUtils import *\n  11 | import json\n  12 | from pyshifu.ShifuConf import shifuConf\n  13 | queue = \"risk_gds_focus\"\n  14 | shifuConf.set('hadoopJobQueue', queue)\n  15 | shifuConf.set('guagua.split.maxCombinedSplitSize', '3000000000')\n  16 | shifuConf.set('mapreduce.map.memory.mb', '8192')\n  17 | shifuConf.set('mapreduce.map.java.opts', '-Xms4096m -Xmx20G')\n  18 | # shifuConf.set('guagua.split.maxCombinedSplitSize', '30000000')\n  19 | 4096*2\n  20 | shifuConf.print_envs()\n  21 | pd.set_option('display.max_rows', 2000)\n  22 | pd.set_option('display.max_columns', 500)\n  23 | pd.set_option('display.width', 1000)\n  24 | pd.set_option('display.max_colwidth', 100)\n  25 | util_path = '/projects/gds-focus/data/catch/IL/CAM_variable/assets'\n  26 | sys.path.append(util_path)\n  27 | from helper import *\n  28 | working_path = \"/projects/gds-focus/data/catch/IL/CAM_variable\"\n  29 | username = os.environ['NB_USER']\n  30 | os.chdir(working_path)\n  31 | os.environ['SHIFU_OPTS'] = '-Xms2G -Xmx40G'\n  32 | # # Parameters\n  33 | # Training data time window: IL 0912 trigger\n  34 | # Raw data\n  35 | # gs://pypl-bkt-rsh-row-std-gds-art/maglev/SF_SL/20240912/data/training/data/group_all\n  36 | #\n  37 | #\n  38 | trigger_dt = '20240912'\n  39 | queue = \"risk_gds_focus\"\n  40 | se_round = 5\n  41 | se_round = int(se_round)\n  42 | sampleRate = 1.0\n  43 | sampleRate = float(sampleRate)\n  44 | corrCut = 0.8\n  45 | corrCut = float(corrCut)\n  46 | segment_file = f\"{working_path}/files/cam_il_segment.file\"\n  47 | common_conf_path = f\"{working_path}/files/cam_il_common_model_config.json\"\n  48 | hdfs_base_folder = \"/apps/risk/det/cchen16/il_new_variable\"\n  49 | hdfsModelSetPath = f\"{hdfs_base_folder}/cam_{trigger_dt}\" \n  50 | local_path = f\"{working_path}/cam_{trigger_dt}\"\n  51 | local_shifu_path = f\"{working_path}/cam_{trigger_dt}/shifu\" \n  52 | local_cols_path = f\"{local_shifu_path}/columns\"\n  53 | raw_data_path = \"/apps/risk/det/cchen16/il_new_variable/data/cam_0912_recent_8k_var_pig_w_crc\"\n  54 | os.makedirs(local_path, exist_ok=True)\n  55 | os.makedirs(local_shifu_path, exist_ok=True)\n  56 | os.makedirs(local_cols_path, exist_ok=True)\n  57 | # # customized Parameters\n  58 | def delete_norm_data(model_name):\n  59 |     from model_automation.utils.rmr import run_cmd\n  60 |     shifu_hdfs_path = woe.get_model_config(model_name).get('basic').get('customPaths').get('hdfsModelSetPath')\n  61 |     cmd =  \"hadoop fs -rm -r -skipTrash {}\".format(os.path.join(shifu_hdfs_path,model_name,'tmp/NormalizedData'))\n  62 |     run_cmd(cmd)\n  63 | def rewriteColumnConfig(model_name):\n  64 |     segPath = woe.model_path.get(model_name)\n  65 |     final_se = pd.read_csv(os.path.join(segPath, 'varsel/se.{}'.format(int(se_round)-1)),sep='\\t',header=None, names=['index','name','max','rms','std']) \n  66 |     final_se.sort_values(by = 'rms', ascending=False).reset_index(drop=True, inplace=True)\n  67 |     # candidates_se = final_se.loc[final_se.index< top_ses.get(model_name) ,'name'].tolist()\n  68 |     candidates_se = final_se.loc[final_se.index < 400 ,'name'].tolist()\n  69 |     # rms: root-mean-square\n  70 |     local_candidate_file = os.path.join(segPath, 'columns/candidates_top{0}.txt'.format(400))\n  71 |     # local_candidate_file = os.path.join(segPath, 'columns/candidates_top{0}.txt'.format(top_ses.get(model_name)))\n  72 |     with open(local_candidate_file, 'w') as f:\n  73 |         f.write(\"\\n\".join(candidates_se))\n  74 |     col_conf = woe.get_column_config(model_name)\n  75 |     for config in col_conf:\n  76 |         config['finalSelect'] = False\n  77 |         if config['columnName'] in candidates_se:\n  78 |             config['finalSelect'] = True\n  79 |     woe.write_column_config(model_name, col_conf)\n  80 | def check_sanity(model_name, drop_psi=1):\n  81 |     gpu_path = os.path.join(folder, model_name)\n  82 |     _, _ = sanity_check(drop_psi=drop_psi,gpu_path=gpu_path)\n  83 | def corr_main(model_name, corrCut=0.9):\n  84 |     seg_path = woe.model_path.get(model_name)\n  85 |     candidate = pd.read_csv(os.path.join(seg_path, 'columns/candidate.column.names'), header=None)[0].tolist()\n  86 |     corr_pd = pd.read_csv(os.path.join(seg_path,'correlation.csv'),index_col=1,header=1,low_memory=False)\\\n  87 |                 .loc[candidate, candidate]\n  88 |     meta_columns = pd.read_csv(os.path.join(seg_path, 'columns/meta.column.names'),header=None)[0].tolist() \n  89 |     df_out = get_corr_pair_speed(corr_pd=corr_pd,seg_path=seg_path, corrCut=corrCut)\n  90 |     drop_corr_columns = df_out['rightVar'].tolist()\n  91 |     final_keep = [i for i in corr_pd.columns if i not in drop_corr_columns and i not in meta_columns]\n  92 |     write_line_file(os.path.join(seg_path, 'columns/candidate.column.names'), final_keep)\n  93 |     return\n  94 | # # load or new shifu model\n  95 | df_seg = pd.read_csv(segment_file, dtype = 'str')\n  96 | model_names = []\n  97 | seg_names = []\n  98 | top_ses = {}\n  99 | for r_index, row in df_seg.iterrows():\n 100 |     model_name = row['job_type']+ '_' + row['seg_name']\n 101 |     model_names.append(model_name)\n 102 |     seg_names.append(row['seg_name'])\n 103 |     top_ses[model_name] = int(row['top_se'])\n 104 | df_seg\n 105 | model_names = [\n 106 |     'cam24_il_Tot',\n 107 |   'cam24_il_Young',\n 108 |   'cam24_il_Guest',\n 109 |   'cam24_il_Top',\n 110 |   'cam24_il_Casual',\n 111 |   'cam24_il_BrandedXO',\n 112 |   'cam24_il_BrandedRecurring',\n 113 |   'cam24_il_P2PFF',\n 114 |   'cam24_il_P2PGS',\n 115 |   'cam24_il_EbayMor',\n 116 |   'cam24_il_Other',\n 117 |   'cam24_il_MidCAM22Score'\n 118 | ]\n 119 | woe = MultiSegmentWoe()\n 120 | woe.load_or_new(model_names, local_shifu_path)\n 121 | # # Before Running Shifu\n 122 | # ## update model config\n 123 | common_conf_path\n 124 | with open(common_conf_path,'r') as f:\n 125 |     common_conf = json.load(f)\n 126 | common_conf.get('basic').get('customPaths')['hdfsModelSetPath'] = hdfsModelSetPath\n 127 | common_conf.get('dataSet')['dataPath'] = raw_data_path\n 128 | common_conf.get('dataSet')['headerPath'] = raw_data_path+'/.pig_header'\n 129 | common_conf.get('evals').get('dataSet')['dataPath']=raw_data_path\n 130 | common_conf.get('evals').get('dataSet')['headerPath']=raw_data_path+'/.pig_header'\n 131 | with open(common_conf_path, 'w') as f:\n 132 |     json.dump(common_conf, f, indent=1)\n 133 | # update model config in batch\n 134 | for key in common_conf.keys():\n 135 |     woe.update_model_config(model_names=model_names, field=key, update_config=common_conf.get(key))\n 136 | # update specific config\n 137 | for r_index, row in df_seg.iterrows():\n 138 |     job_dict = {}\n 139 |     job_dict['dataSet'] = {'posTags':row['bad'].split('|'),\n 140 |                         'negTags':row['good'].split('|'),\n 141 |                         'filterExpressions':row['data_filter'],\n 142 |                         'weightColumnName':row['weight_col'],\n 143 |                         'targetColumnName':row['target_col']}\n 144 |     job_dict['stats'] = {'maxNumBin':int(row['bin'].split('|')[0]), \n 145 |                          'cateMaxNumBin':int(row['bin'].split('|')[1]),\n 146 |                          'sampleRate':float(row['sample_rate'])\n 147 |                         }\n 148 |     job_dict['normalize'] = {'sampleRate':float(row['sample_rate'])}\n 149 |     model_name = [row['job_type']+ '_' + row['seg_name']]\n 150 |     for key in job_dict.keys():\n 151 |         woe.update_model_config(model_names=model_name, field=key, update_config=job_dict.get(key))\n 152 | # # ## copy columns files\n 153 | len(model_names), local_cols_path\n 154 | cols_type = [\n 155 |     'meta'\n 156 |     # ,'forceremove'\n 157 |     # ,'forceselect'\n 158 |     # ,'categorical'\n 159 |     # ,'candidate'\n 160 | ]\n 161 | for col_t in cols_type:\n 162 |     path = os.path.join('/projects/gds-focus/data/catch/IL/CAM_variable/files/candidate_pool_columns','{}.column.names'.format(col_t))\n 163 |     varlist = pd.read_csv(path,header=None).iloc[:,0].tolist()\n 164 |     for model_name in model_names:\n 165 |         woe.write_columns(model_name, col_t, varlist)\n 166 | # # Running Shifu\n 167 | # ## init\n 168 | woe.parallel_run(model_names=model_names, commands=\"init\", parallel_num=5, hide_log=False)\n 169 | # ## shifu stats and export\n 170 | woe.parallel_run(model_names=model_names, commands=\"stats\", parallel_num=5, hide_log=True)\n 171 | woe.parallel_run(model_names=model_names, commands=\"export -t columnstats\", parallel_num=5, hide_log=False)\n 172 | # ## additional shifu stats and export for fail / wrong models\n 173 | import importlib  \n 174 | import helper\n 175 | importlib.reload(helper) \n 176 | from helper import *\n 177 | for model_name in model_names:\n 178 |     df_stats = pd.read_csv(os.path.join(local_shifu_path, model_name, 'columns/ColumnStats.csv'))\n 179 |     rm_list = stats_sanity_check(df_stats)\n 180 |     varlist = df_stats[(df_stats.columnFlag.isin(['Candidate', 'ForceSelect']))&(~df_stats.columnName.isin(rm_list))]\\\n 181 |                      .sort_values(by='iv', ascending=False).reset_index(drop=True)['columnName'].values.tolist()\n 182 |     woe.write_columns(model_name, 'candidate', varlist)\n 183 | # ## correlation\n 184 | import importlib  \n 185 | import helper\n 186 | importlib.reload(helper) \n 187 | from helper import *\n 188 | # woe.parallel_run(model_names=model_names, commands=\\\n 189 | #                  \"stats -correlation -Dshifu.stats.corr.reuse=true\", parallel_num=5, hide_log=False)\n 190 | woe.parallel_run(model_names=model_names, commands=\\\n 191 |                  \"stats -correlation\", parallel_num=5, hide_log=False)\n 192 | # ### filter by correlation\n 193 | for model_name in model_names:\n 194 |     corr_main(model_name, corrCut=0.8)\n 195 | # ## Norm and Varsel\n 196 | se_round = 3\n 197 | se_dict = {}\n 198 | se_dict['varSelect'] = {'filterBy':'SE',\n 199 |                         'filterOutRatio': 0.5\n 200 |                         }\n 201 | for key in se_dict.keys():\n 202 |     woe.update_model_config(model_names=model_names, field=key, update_config=se_dict.get(key))\n 203 | se_round = 3\n 204 | len(model_names)\n 205 | model_names\n 206 | se_round = 3\n 207 | segment_command = {}\n 208 | for model_name in model_names:\n 209 |     segment_command[model_name] = []\n 210 |     # segment_command[model_name].append(\"norm\")\n 211 |     # segment_command[model_name].append(\"varsel -reset\")\n 212 |     # segment_command[model_name].append(\"varsel -r {}\".format(se_round))\n 213 |     # segment_command[model_name].append({\n 214 |     #                                     \"func\": delete_norm_data,\n 215 |     #                                     \"args\": [model_name]\n 216 |     #                                 })\n 217 |     segment_command[model_name].append({\n 218 |                                         \"func\": rewriteColumnConfig,\n 219 |                                         \"args\": [model_name]\n 220 |                                     })\n 221 |     # segment_command[model_name].append(\"eval -norm Eval1\")\n 222 |     #segment_command[model_name].append(\"eval -norm EarlyTagging\")\n 223 | woe.parallel_run_segment(model_names, segment_command, parallel_num=5, auto_restart=0,hide_log=False)\n 224 | model_name = \"cam24_il_Tot\"\n 225 | segPath = woe.model_path.get(model_name)\n 226 | final_se = pd.read_csv(os.path.join(segPath, 'varsel/se.{}'.format(int(se_round)-1)),sep='\\t',header=None, names=['index','name','max','rms','std']) \n 227 | final_se.sort_values(by = 'rms', ascending=False).reset_index(drop=True, inplace=True)\n 228 | # candidates_se = final_se.loc[final_se.index< top_ses.get(model_name) ,'name'].tolist()\n 229 | candidates_se = final_se.loc[final_se.index < 400 ,'name'].tolist()\n 230 | # rms: root-mean-square\n 231 | local_candidate_file = os.path.join(segPath, 'columns/candidates_top{0}.txt'.format(400))\n 232 | # local_candidate_file = os.path.join(segPath, 'columns/candidates_top{0}.txt'.format(top_ses.get(model_name)))\n 233 | with open(local_candidate_file, 'w') as f:\n 234 |     f.write(\"\\n\".join(candidates_se))\n 235 | col_conf = woe.get_column_config(model_name)\n 236 | for config in col_conf:\n 237 |     config['finalSelect'] = False\n 238 |     if config['columnName'] in candidates_se:\n 239 |         config['finalSelect'] = True\n 240 | # ## Collect final se variables\n 241 | def rewriteColumnConfig(model_name):\n 242 |     segPath = woe.model_path.get(model_name)\n 243 |     final_se = pd.read_csv(os.path.join(segPath, 'varsel/se.{}'.format(int(se_round)-1)),sep='\\t',header=None, names=['index','name','max','rms','std']) \n 244 |     final_se.sort_values(by = 'rms', ascending=False).reset_index(drop=True, inplace=True)\n 245 |     # candidates_se = final_se.loc[final_se.index< top_ses.get(model_name) ,'name'].tolist()\n 246 |     candidates_se = final_se.loc[final_se.index < 500 ,'name'].tolist()\n 247 |     # rms: root-mean-square\n 248 |     local_candidate_file = os.path.join(segPath, 'columns/candidates_top{0}.txt'.format(500))\n 249 |     # local_candidate_file = os.path.join(segPath, 'columns/candidates_top{0}.txt'.format(top_ses.get(model_name)))\n 250 |     with open(local_candidate_file, 'w') as f:\n 251 |         f.write(\"\\n\".join(candidates_se))\n 252 |     col_conf = woe.get_column_config(model_name)\n 253 |     for config in col_conf:\n 254 |         config['finalSelect'] = False\n 255 |         if config['columnName'] in candidates_se:\n 256 |             config['finalSelect'] = True\n 257 |     woe.write_column_config(model_name, col_conf)\n 258 | df = pd.DataFrame([])\n 259 | for model_name in model_names:\n 260 |     segPath = woe.model_path.get(model_name)\n 261 |     local_candidate_file = os.path.join(segPath, 'columns/candidates_top{0}.txt'.format(500))\n 262 |     candidate_ls = read_line_file(local_candidate_file)\n 263 |     df_stats = pd.read_csv(os.path.join(segPath, 'columns/ColumnStats.csv'))\n 264 |     df = df.append(df_stats[df_stats.columnName.isin(candidate_ls)])\n 265 | df.shape\n 266 | df.to_csv(\"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912_all_in_one/shifu/cam_20240912_all_in_one/top500_se_stats.csv\", index=False, header=True)\n 267 | df_seg = pd.read_csv(\"/projects/gds-focus/data/makang/CAM24/source_code/files/adaptive_segment.file\")\n 268 | # df_seg = pd.read_csv(segment_file, dtype = 'str')\n 269 | model_names = []\n 270 | seg_names = []\n 271 | top_ses = {}\n 272 | for r_index, row in df_seg.iterrows():\n 273 |     model_name = row['job_type']+ '_' + row['seg_name']\n 274 |     model_names.append(model_name)\n 275 |     seg_names.append(row['seg_name'])\n 276 |     top_ses[model_name] = int(row['top_se'])\n 277 | df_seg\n 278 | df = pd.DataFrame([])\n 279 | for model_name in model_names:\n 280 |     segPath = os.path.join(\"/projects/gds-focus/data/makang/CAM24/source_code/adaptive/shifu/multi_seg\",model_name)\n 281 |     local_candidate_file = os.path.join(segPath, 'columns/candidates_top{0}.txt'.format(top_ses.get(model_name)))\n 282 |     candidate_ls = read_line_file(local_candidate_file)\n 283 |     df_stats = pd.read_csv(os.path.join(segPath, 'columns/ColumnStats.csv'))\n 284 |     df = df.append(df_stats[df_stats.columnName.isin(candidate_ls)])\n 285 | df.shape\n 286 | df\n 287 | df.to_csv(\"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam_20240912_all_in_one/cam24_top_se_stats.csv\", index=False, header=True)\n 288 | # ## select different candidate list\n 289 | model_names\n 290 | for i, name in enumerate(model_names):\n 291 | candidate_list = []\n 292 | for i, model_name in enumerate(model_names):\n 293 |     segPath = woe.model_path.get(model_name)\n 294 |     final_se = pd.read_csv(os.path.join(segPath, 'varsel/se.{}'.format(int(se_round)-1)),sep='\\t',header=None, names=['index','name','max','rms','std']) \n 295 |     final_se.sort_values(by = 'rms', ascending=False).reset_index(drop=True, inplace=True)\n 296 |     candidates_se = final_se.loc[final_se.index< top_ses.get(model_name) ,'name'].tolist()\n 297 |     if i > 0:\n 298 |         candidates_se = [c+'_seg' + str(i) for c in candidates_se]\n 299 |     candidate_list.extend(candidates_se)\n 300 | len(candidate_list)\n 301 | candidate_list\n 302 | write_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt\", candidate_list)\n 303 | # # Final data paths\n 304 | # final_norm_path = {}\n 305 | # for model_name in model_names:\n 306 | #     shifu_hdfs_path = woe.get_model_config(model_name).get('basic').get('customPaths').get('hdfsModelSetPath')\n 307 | #     eval_norm_path = os.path.join(shifu_hdfs_path,model_name,'/evals/Eval1/EvalScore')\n 308 | #     final_norm_path[model_name] = eval_norm_path\n 309 | # print(final_norm_path)",
    "rmr_agent/repos/CAM_variable_research/2_2_cam_new_var_selection_shifu.py": "   1 | # # Config preparation\n   2 | # %url -c horton\n   3 | # %ppauth\n   4 | import sys,os\n   5 | utils_path = \"/projects/gds-focus/data/makang/CAM24/source_code/cam_utils\"\n   6 | sys.path.append(utils_path)\n   7 | from model_automation.woe import MultiSegmentWoe\n   8 | import pandas as pd\n   9 | from ShifuUtils import *\n  10 | import json\n  11 | from pyshifu.ShifuConf import shifuConf\n  12 | queue = \"risk_gds_focus\"\n  13 | shifuConf.set('hadoopJobQueue', queue)\n  14 | shifuConf.set('guagua.split.maxCombinedSplitSize', '3000000000')\n  15 | # shifuConf.set('guagua.split.maxCombinedSplitSize', '30000000')\n  16 | working_path = \"/projects/gds-focus/data/catch/IL/CAM_variable\"\n  17 | username = os.environ['NB_USER']\n  18 | os.chdir(working_path)\n  19 | os.environ['SHIFU_OPTS'] = '-Xms2G -Xmx4G'\n  20 | # # Parameters\n  21 | # Training data time window: IL 0912 trigger\n  22 | # Raw data\n  23 | # gs://pypl-bkt-rsh-row-std-gds-art/maglev/SF_SL/20240912/data/training/data/group_all\n  24 | #\n  25 | #\n  26 | trigger_dt = '20240912_incre'\n  27 | queue = \"risk_gds_focus\"\n  28 | se_round = 3\n  29 | se_round = int(se_round)\n  30 | sampleRate = 1.0\n  31 | sampleRate = float(sampleRate)\n  32 | corrCut = 0.8\n  33 | corrCut = float(corrCut)\n  34 | segment_file = f\"{working_path}/files/cam_il_segment.file\"\n  35 | common_conf_path = f\"{working_path}/files/cam_il_common_model_config.json\"\n  36 | hdfs_base_folder = \"/apps/risk/det/cchen16/il_new_variable\"\n  37 | hdfsModelSetPath = f\"{hdfs_base_folder}/cam_{trigger_dt}\" \n  38 | local_path = f\"{working_path}/cam_{trigger_dt}\"\n  39 | local_shifu_path = f\"{working_path}/cam_{trigger_dt}/shifu\" \n  40 | local_cols_path = f\"{local_shifu_path}/columns\"\n  41 | raw_data_path = \"/apps/risk/det/cchen16/il_new_variable/data/cam_0912_recent_incre_var_pig_w_crc\"\n  42 | os.makedirs(local_path, exist_ok=True)\n  43 | os.makedirs(local_shifu_path, exist_ok=True)\n  44 | os.makedirs(local_cols_path, exist_ok=True)\n  45 | # # customized Parameters\n  46 | def delete_norm_data(model_name):\n  47 |     from model_automation.utils.rmr import run_cmd\n  48 |     shifu_hdfs_path = woe.get_model_config(model_name).get('basic').get('customPaths').get('hdfsModelSetPath')\n  49 |     cmd =  \"hadoop fs -rm -r -skipTrash {}\".format(os.path.join(shifu_hdfs_path,model_name,'tmp/NormalizedData'))\n  50 |     run_cmd(cmd)\n  51 | def rewriteColumnConfig(model_name):\n  52 |     segPath = woe.model_path.get(model_name)\n  53 |     final_se = pd.read_csv(os.path.join(segPath, 'varsel/se.{}'.format(int(se_round)-1)),sep='\\t',header=None, names=['index','name','max','rms','std']) \n  54 |     final_se.sort_values(by = 'rms', ascending=False).reset_index(drop=True, inplace=True)\n  55 |     candidates_se = final_se.loc[final_se.index< top_ses.get(model_name) ,'name'].tolist()\n  56 |     # rms: root-mean-square\n  57 |     local_candidate_file = os.path.join(segPath, 'columns/candidates_top{0}.txt'.format(top_ses.get(model_name)))\n  58 |     with open(local_candidate_file, 'w') as f:\n  59 |         f.write(\"\\n\".join(candidates_se))\n  60 |     col_conf = woe.get_column_config(model_name)\n  61 |     for config in col_conf:\n  62 |         config['finalSelect'] = False\n  63 |         if config['columnName'] in candidates_se:\n  64 |             config['finalSelect'] = True\n  65 |     woe.write_column_config(model_name, col_conf)\n  66 | def check_sanity(model_name, drop_psi=1):\n  67 |     gpu_path = os.path.join(folder, model_name)\n  68 |     _, _ = sanity_check(drop_psi=drop_psi,gpu_path=gpu_path)\n  69 | def filter_by_corr( model_name, corrCut=0.9):\n  70 |     segPath = woe.model_path.get(model_name)\n  71 |     candidate = pd.read_csv(os.path.join(segPath, 'columns/candidate.column.names'), header=None)[0].tolist()\n  72 |     corr_pd = pd.read_csv(os.path.join(segPath, 'correlation.csv'),index_col=1,header=1,low_memory=False).loc[candidate, candidate]\n  73 |     meta_columns = pd.read_csv(os.path.join(segPath, 'columns/meta.column.names'),header=None)[0].tolist() \n  74 |     columnList, df_out = get_corr_pair_speed(corr_pd, segPath, corrCut)\n  75 |     drop_corr_columns = df_out['rightVar'].tolist()\n  76 |     final_keep = [i for i in corr_pd.columns if i not in drop_corr_columns]\n  77 |     with open(os.path.join(segPath, 'columns/final_used_header.txt'),'w') as f:\n  78 |         f.writelines('|'.join(final_keep))\n  79 |     final_dl_inputs = [col  for col in final_keep if col not in meta_columns]\n  80 |     with open(os.path.join(segPath, 'columns/final_dl_inputs.txt'),'w') as f:\n  81 |         f.writelines('|'.join(final_dl_inputs))\n  82 |     pd.DataFrame(final_dl_inputs,columns=['name']).to_csv( os.path.join(segPath, 'columns/candidate.column.names'),index=None,header=None) \n  83 | # # load or new shifu model\n  84 | df_seg = pd.read_csv(segment_file, dtype = 'str')\n  85 | model_names = []\n  86 | seg_names = []\n  87 | top_ses = {}\n  88 | for r_index, row in df_seg.iterrows():\n  89 |     model_name = row['job_type']+ '_' + row['seg_name']\n  90 |     model_names.append(model_name)\n  91 |     seg_names.append(row['seg_name'])\n  92 |     top_ses[model_name] = int(row['top_se'])\n  93 | df_seg\n  94 | local_shifu_path\n  95 | woe = MultiSegmentWoe()\n  96 | woe.load_or_new(model_names, local_shifu_path)\n  97 | # # Before Running Shifu\n  98 | # ## update model config\n  99 | common_conf_path\n 100 | # %whos str\n 101 | with open(common_conf_path,'r') as f:\n 102 |     common_conf = json.load(f)\n 103 | common_conf.get('basic').get('customPaths')['hdfsModelSetPath'] = hdfsModelSetPath\n 104 | common_conf.get('dataSet')['dataPath'] = raw_data_path\n 105 | common_conf.get('dataSet')['headerPath'] = raw_data_path+'/.pig_header'\n 106 | common_conf.get('evals').get('dataSet')['dataPath']=raw_data_path\n 107 | common_conf.get('evals').get('dataSet')['headerPath']=raw_data_path+'/.pig_header'\n 108 | with open(common_conf_path, 'w') as f:\n 109 |     json.dump(common_conf, f, indent=1)\n 110 | # update model config in batch\n 111 | for key in common_conf.keys():\n 112 |     woe.update_model_config(model_names=model_names, field=key, update_config=common_conf.get(key))\n 113 | # update specific config\n 114 | for r_index, row in df_seg.iterrows():\n 115 |     job_dict = {}\n 116 |     job_dict['dataSet'] = {'posTags':row['bad'].split('|'),\n 117 |                         'negTags':row['good'].split('|'),\n 118 |                         'filterExpressions':row['data_filter'],\n 119 |                         'weightColumnName':row['weight_col'],\n 120 |                         'targetColumnName':row['target_col']}\n 121 |     job_dict['stats'] = {'maxNumBin':int(row['bin'].split('|')[0]), \n 122 |                          'cateMaxNumBin':int(row['bin'].split('|')[1]),\n 123 |                          'sampleRate':float(row['sample_rate'])\n 124 |                         }\n 125 |     job_dict['normalize'] = {'sampleRate':float(row['sample_rate'])}\n 126 |     model_name = [row['job_type']+ '_' + row['seg_name']]\n 127 |     for key in job_dict.keys():\n 128 |         woe.update_model_config(model_names=model_name, field=key, update_config=job_dict.get(key))\n 129 | # # ## copy columns files\n 130 | len(model_names),local_cols_path\n 131 | cols_type = [\n 132 |     'meta'\n 133 |     ,'forceremove'\n 134 |     # ,'forceselect'\n 135 |     ,'categorical'\n 136 |     ,'candidate'\n 137 | ]\n 138 | for col_t in cols_type:\n 139 |     path = os.path.join('/projects/gds-focus/data/catch/IL/CAM_variable/files/incre_pool_columns','{}.column.names'.format(col_t))\n 140 |     varlist = pd.read_csv(path,header=None).iloc[:,0].tolist()\n 141 |     for model_name in model_names:\n 142 |         woe.write_columns(model_name, col_t, varlist)\n 143 | # # Running Shifu\n 144 | # ## init\n 145 | woe.parallel_run(model_names=model_names, commands=\"init\", parallel_num=5, hide_log=False)\n 146 | # ## shifu stats and export\n 147 | woe.parallel_run(model_names=model_names, commands=\"stats\", parallel_num=5, hide_log=True)\n 148 | woe.parallel_run(model_names=model_names, commands=\"export -t columnstats\", parallel_num=5, hide_log=False)\n 149 | # ## additional shifu stats and export for fail / wrong models\n 150 | # add_model_names = ['cam24_adaptive_BA']\n 151 | # woe.parallel_run(model_names=add_model_names, commands=\"init\", parallel_num=5, hide_log=False)\n 152 | # woe.parallel_run(model_names=add_model_names, commands=\"stats\", parallel_num=5, hide_log=False)\n 153 | # woe.parallel_run(model_names=add_model_names, commands=\"export -t columnstats\", parallel_num=5, hide_log=False)\n 154 | # ##  stats sanity check \n 155 | # fail_jobs=[]\n 156 | # for model_name in model_names:\n 157 | #     gpu_path = os.path.join(folder,'{}/columns/ColumnStats.csv'.format(model_name))\n 158 | #     data = pd.read_csv(gpu_path)\n 159 | #     var = [var for var in data.columns if '_psi' in var]\n 160 | #     if len(var)==0:\n 161 | # #         print(model_name,' stats failed')\n 162 | #         fail_jobs.append(model_name)\n 163 | # if len(fail_jobs) > 0:\n 164 | #     raise ValueError(\"stats fail for segment: \", fail_jobs)\n 165 | # else:\n 166 | #     print('All segments shifu stats successful')\n 167 | #     total_cnts = []\n 168 | #     for model_name in model_names:\n 169 | #         gpu_path = os.path.join(folder, model_name)\n 170 | #         print('sanity check: ',model_name)\n 171 | #         total_cnt, _ = sanity_check(drop_psi='1',gpu_path=gpu_path)\n 172 | #         total_cnts.append(total_cnt)\n 173 | # candidate_lenth = []\n 174 | # for model_name in model_names:\n 175 | #     path = os.path.join(folder,'{}/columns/candidate.column.names'.format(model_name))\n 176 | #     varlist = pd.read_csv(path,header=None).iloc[:,0].tolist()\n 177 | #     candidate_lenth.append(len(varlist))\n 178 | # total_cnts\n 179 | # candidate_lenth\n 180 | # ## correlation\n 181 | # ## run incomplete models\n 182 | # incomplete_models = list(set(model_names) - set(['cam24_adaptive_Guest', 'cam24_adaptive_Guest1', 'cam24_adaptive_Young1st']))\n 183 | # woe.parallel_run(model_names=incomplete_models, commands=\"stats -correlation\", parallel_num=5, hide_log=False)\n 184 | # ## re-generate correlation file\n 185 | # # !hdfs dfs -ls /apps/risk/madmen/maglev/CAM24/shifu/adaptive/cam24_adaptive_NonSPP/tmp/CorrelationPath\n 186 | # # !hdfs dfs -ls /apps/risk/madmen/maglev/CAM24/shifu/adaptive/cam24_adaptive_Tot\n 187 | # woe.get_model('cam24_adaptive_Mule').stats('-correlation')\n 188 | # woe.get_model('cam24_adaptive_Mule').stats('-correlation', '-Dshifu.stats.corr.reuse=true')\n 189 | # woe.parallel_run(model_names=['cam24_adaptive_Mule'], commands=\"stats -correlation -Dshifu.stats.corr.reuse=true\", parallel_num=1, hide_log=False)\n 190 | # ## filter correlation\n 191 | for model_name in model_names:\n 192 | # for model_name in model_names:\n 193 | #     filter_by_corr( model_name, corrCut)\n 194 | # # Norm and Varsel\n 195 | segment_command = {}\n 196 | for model_name in model_names:\n 197 |     segment_command[model_name] = []\n 198 |     segment_command[model_name].append(\"norm\")\n 199 |     segment_command[model_name].append(\"varsel -reset\")\n 200 |     segment_command[model_name].append(\"varsel -r {}\".format(se_round))\n 201 |     segment_command[model_name].append({\n 202 |                                         \"func\": delete_norm_data,\n 203 |                                         \"args\": [model_name]\n 204 |                                     })\n 205 |     segment_command[model_name].append({\n 206 |                                         \"func\": rewriteColumnConfig,\n 207 |                                         \"args\": [model_name]\n 208 |                                     })\n 209 |     segment_command[model_name].append(\"eval -norm Eval1\")\n 210 |     #segment_command[model_name].append(\"eval -norm EarlyTagging\")\n 211 | # test_model_names = ['cam24_adaptive_Mule']\n 212 | # test_segment_command = {}\n 213 | # test_segment_command['cam24_adaptive_Mule'] = segment_command['cam24_adaptive_Mule']\n 214 | woe.parallel_run_segment(model_names, segment_command, parallel_num=5, auto_restart=1,hide_log=False)\n 215 | # test_model_names = ['cam24_adaptive_Mule']\n 216 | # woe.parallel_run_segment(test_model_names, test_segment_command, parallel_num=5, auto_restart=1,hide_log=False)\n 217 | # # Final data paths\n 218 | # final_norm_path = {}\n 219 | # for model_name in model_names:\n 220 | #     shifu_hdfs_path = woe.get_model_config(model_name).get('basic').get('customPaths').get('hdfsModelSetPath')\n 221 | #     eval_norm_path = os.path.join(shifu_hdfs_path,model_name,'/evals/Eval1/EvalScore')\n 222 | #     final_norm_path[model_name] = eval_norm_path\n 223 | # print(final_norm_path)\n 224 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_0912_recent_8k_var_pig_w_crc/ | head \n 225 | # !gsutil du -h -s {gcs_path}\n 226 | # !hadoop fs -du -h -s {hdfs_folder}/cam_0912_recent_8k_var_pig_w_crc\n 227 | gcs_path = \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_0912_recent_8k_var_pig_w_crc\"\n 228 | hdfs_folder = \"/apps/risk/det/cchen16/il_new_variable/data\"\n 229 | # %maglev copy {gcs_path} {hdfs_folder}/",
    "rmr_agent/repos/CAM_variable_research/2_3_cam_all_var_shifu_multisegment.py": "   1 | # # Config preparation\n   2 | # !kinit cchen16 -l 3d -kt /notebooks/projects/cchen16/config/cchen16.keytab\n   3 | # %url -c horton\n   4 | # %ppauth\n   5 | import sys,os\n   6 | utils_path = \"/projects/gds-focus/data/makang/CAM24/source_code/cam_utils\"\n   7 | sys.path.append(utils_path)\n   8 | from model_automation.woe import MultiSegmentWoe\n   9 | import pandas as pd\n  10 | from ShifuUtils import *\n  11 | import json\n  12 | from pyshifu.ShifuConf import shifuConf\n  13 | queue = \"risk_gds_focus\"\n  14 | shifuConf.set('hadoopJobQueue', queue)\n  15 | shifuConf.set('guagua.split.maxCombinedSplitSize', '3000000000')\n  16 | # shifuConf.set('guagua.split.maxCombinedSplitSize', '30000000')\n  17 | pd.set_option('display.max_rows', 2000)\n  18 | pd.set_option('display.max_columns', 500)\n  19 | pd.set_option('display.width', 1000)\n  20 | pd.set_option('display.max_colwidth', 100)\n  21 | util_path = '/projects/gds-focus/data/catch/IL/CAM_variable/assets'\n  22 | sys.path.append(util_path)\n  23 | from helper import *\n  24 | working_path = \"/projects/gds-focus/data/catch/IL/CAM_variable\"\n  25 | username = os.environ['NB_USER']\n  26 | os.chdir(working_path)\n  27 | # os.environ['SHIFU_OPTS'] = '-Xms2G -Xmx4G'\n  28 | os.environ['SHIFU_OPTS'] = '-Xms2G -Xmx40G'\n  29 | # # Parameters\n  30 | # Training data time window: IL 0912 trigger\n  31 | # Raw data\n  32 | # gs://pypl-bkt-rsh-row-std-gds-art/maglev/SF_SL/20240912/data/training/data/group_all\n  33 | #\n  34 | #\n  35 | trigger_dt = '20240912_all'\n  36 | queue = \"risk_gds_focus\"\n  37 | se_round = 3\n  38 | se_round = int(se_round)\n  39 | sampleRate = 1.0\n  40 | sampleRate = float(sampleRate)\n  41 | corrCut = 0.8\n  42 | corrCut = float(corrCut)\n  43 | segment_file = f\"{working_path}/files/cam_il_segment.file\"\n  44 | common_conf_path = f\"{working_path}/files/cam_il_common_model_config.json\"\n  45 | hdfs_base_folder = \"/apps/risk/det/cchen16/il_new_variable\"\n  46 | hdfsModelSetPath = f\"{hdfs_base_folder}/cam_{trigger_dt}\" \n  47 | local_path = f\"{working_path}/cam_{trigger_dt}\"\n  48 | local_shifu_path = f\"{working_path}/cam_{trigger_dt}/shifu\" \n  49 | local_cols_path = f\"{local_shifu_path}/columns\"\n  50 | raw_data_path = \"/apps/risk/det/cchen16/il_new_variable/data/cam_0912_recent_8k_var_pig_w_crc\"\n  51 | os.makedirs(local_path, exist_ok=True)\n  52 | os.makedirs(local_shifu_path, exist_ok=True)\n  53 | os.makedirs(local_cols_path, exist_ok=True)\n  54 | # # customized Parameters\n  55 | def delete_norm_data(model_name):\n  56 |     from model_automation.utils.rmr import run_cmd\n  57 |     shifu_hdfs_path = woe.get_model_config(model_name).get('basic').get('customPaths').get('hdfsModelSetPath')\n  58 |     cmd =  \"hadoop fs -rm -r -skipTrash {}\".format(os.path.join(shifu_hdfs_path,model_name,'tmp/NormalizedData'))\n  59 |     run_cmd(cmd)\n  60 | def rewriteColumnConfig(model_name):\n  61 |     segPath = woe.model_path.get(model_name)\n  62 |     final_se = pd.read_csv(os.path.join(segPath, 'varsel/se.{}'.format(int(se_round)-1)),sep='\\t',header=None, names=['index','name','max','rms','std']) \n  63 |     final_se.sort_values(by = 'rms', ascending=False).reset_index(drop=True, inplace=True)\n  64 |     candidates_se = final_se.loc[final_se.index< top_ses.get(model_name) ,'name'].tolist()\n  65 |     # rms: root-mean-square\n  66 |     local_candidate_file = os.path.join(segPath, 'columns/candidates_top{0}.txt'.format(top_ses.get(model_name)))\n  67 |     with open(local_candidate_file, 'w') as f:\n  68 |         f.write(\"\\n\".join(candidates_se))\n  69 |     col_conf = woe.get_column_config(model_name)\n  70 |     for config in col_conf:\n  71 |         config['finalSelect'] = False\n  72 |         if config['columnName'] in candidates_se:\n  73 |             config['finalSelect'] = True\n  74 |     woe.write_column_config(model_name, col_conf)\n  75 | def filter_by_corr( model_name, corrCut=0.9):\n  76 |     segPath = woe.model_path.get(model_name)\n  77 |     candidate = pd.read_csv(os.path.join(segPath, 'columns/candidate.column.names'), header=None)[0].tolist()\n  78 |     corr_pd = pd.read_csv(os.path.join(segPath, 'correlation.csv'),index_col=1,header=1,low_memory=False).loc[candidate, candidate]\n  79 |     meta_columns = pd.read_csv(os.path.join(segPath, 'columns/meta.column.names'),header=None)[0].tolist() \n  80 |     columnList, df_out = get_corr_pair_speed(corr_pd, segPath, corrCut)\n  81 |     drop_corr_columns = df_out['rightVar'].tolist()\n  82 |     final_keep = [i for i in corr_pd.columns if i not in drop_corr_columns]\n  83 |     with open(os.path.join(segPath, 'columns/final_used_header.txt'),'w') as f:\n  84 |         f.writelines('|'.join(final_keep))\n  85 |     final_dl_inputs = [col  for col in final_keep if col not in meta_columns]\n  86 |     with open(os.path.join(segPath, 'columns/final_dl_inputs.txt'),'w') as f:\n  87 |         f.writelines('|'.join(final_dl_inputs))\n  88 |     pd.DataFrame(final_dl_inputs,columns=['name']).to_csv( os.path.join(segPath, 'columns/candidate.column.names'),index=None,header=None) \n  89 | # # load or new shifu model\n  90 | df_seg = pd.read_csv(segment_file, dtype = 'str')\n  91 | model_names = []\n  92 | seg_names = []\n  93 | top_ses = {}\n  94 | for r_index, row in df_seg.iterrows():\n  95 |     model_name = row['job_type']+ '_' + row['seg_name']\n  96 |     model_names.append(model_name)\n  97 |     seg_names.append(row['seg_name'])\n  98 |     top_ses[model_name] = int(row['top_se'])\n  99 | df_seg\n 100 | df_seg[df_seg.seg_name == 'Guest1']['data_filter']\n 101 | tot_stats[(tot_stats.psi > 5)&(tot_stats.psiStd > 1.15)&(tot_stats.iv < 0.05)].shape,\\\n 102 | tot_stats[(tot_stats.psi > 5)&(tot_stats.psiStd > 1.15)&(tot_stats.iv >= 0.05)].shape\n 103 | model_names = [\n 104 |     'cam24_il_Tot',\n 105 |   'cam24_il_Young',\n 106 |   'cam24_il_Guest',\n 107 |   'cam24_il_Top',\n 108 |   'cam24_il_Casual',\n 109 |   'cam24_il_BrandedXO',\n 110 |   'cam24_il_BrandedRecurring',\n 111 |   'cam24_il_P2PFF',\n 112 |   'cam24_il_P2PGS',\n 113 |   'cam24_il_EbayMor',\n 114 |   'cam24_il_Other',\n 115 |   'cam24_il_MidCAM22Score'\n 116 | ]\n 117 | woe = MultiSegmentWoe()\n 118 | woe.load_or_new(model_names, local_shifu_path)\n 119 | # # Before Running Shifu\n 120 | # ## update model config\n 121 | common_conf_path\n 122 | with open(common_conf_path,'r') as f:\n 123 |     common_conf = json.load(f)\n 124 | common_conf.get('basic').get('customPaths')['hdfsModelSetPath'] = hdfsModelSetPath\n 125 | common_conf.get('dataSet')['dataPath'] = raw_data_path\n 126 | common_conf.get('dataSet')['headerPath'] = raw_data_path+'/.pig_header'\n 127 | common_conf.get('evals').get('dataSet')['dataPath']=raw_data_path\n 128 | common_conf.get('evals').get('dataSet')['headerPath']=raw_data_path+'/.pig_header'\n 129 | with open(common_conf_path, 'w') as f:\n 130 |     json.dump(common_conf, f, indent=1)\n 131 | # update model config in batch\n 132 | for key in common_conf.keys():\n 133 |     woe.update_model_config(model_names=model_names, field=key, update_config=common_conf.get(key))\n 134 | # update specific config\n 135 | for r_index, row in df_seg.iterrows():\n 136 |     job_dict = {}\n 137 |     job_dict['dataSet'] = {'posTags':row['bad'].split('|'),\n 138 |                         'negTags':row['good'].split('|'),\n 139 |                         'filterExpressions':row['data_filter'],\n 140 |                         'weightColumnName':row['weight_col'],\n 141 |                         'targetColumnName':row['target_col']}\n 142 |     job_dict['stats'] = {'maxNumBin':int(row['bin'].split('|')[0]), \n 143 |                          'cateMaxNumBin':int(row['bin'].split('|')[1]),\n 144 |                          'sampleRate':float(row['sample_rate'])\n 145 |                         }\n 146 |     job_dict['normalize'] = {'sampleRate':float(row['sample_rate'])}\n 147 |     model_name = [row['job_type']+ '_' + row['seg_name']]\n 148 |     for key in job_dict.keys():\n 149 |         woe.update_model_config(model_names=model_name, field=key, update_config=job_dict.get(key))\n 150 | # # ## copy columns files\n 151 | len(model_names),local_cols_path\n 152 | cols_type = [\n 153 |     'meta'\n 154 |     ,'forceremove'\n 155 |     # ,'forceselect'\n 156 |     ,'categorical'\n 157 |     ,'candidate'\n 158 | ]\n 159 | for col_t in cols_type:\n 160 |     path = os.path.join('/projects/gds-focus/data/catch/IL/CAM_variable/files/check_pool_columns','{}.column.names'.format(col_t))\n 161 |     varlist = pd.read_csv(path,header=None).iloc[:,0].tolist()\n 162 |     for model_name in model_names:\n 163 |         woe.write_columns(model_name, col_t, varlist)\n 164 | # # Running Shifu\n 165 | # ## init\n 166 | woe.parallel_run(model_names=model_names, commands=\"init\", parallel_num=5, hide_log=False)\n 167 | # ## shifu stats and export\n 168 | woe.parallel_run(model_names=model_names, commands=\"stats\", parallel_num=5, hide_log=True)\n 169 | woe.parallel_run(model_names=model_names, commands=\"export -t columnstats\", parallel_num=5, hide_log=False)\n 170 | # ## additional shifu stats check\n 171 | import importlib  \n 172 | import helper\n 173 | importlib.reload(helper) \n 174 | from helper import *\n 175 | for model_name in fix_model_names:\n 176 |     df_stats = pd.read_csv(os.path.join(local_shifu_path, model_name, 'columns/ColumnStats.csv'))\n 177 |     rm_list = stats_sanity_check(df_stats)\n 178 |     varlist = df_stats[(df_stats.columnFlag.isin(['Candidate', 'ForceSelect']))&(~df_stats.columnName.isin(rm_list))]\\\n 179 |                      .sort_values(by='iv', ascending=False).reset_index(drop=True)['columnName'].values.tolist()\n 180 |     woe.write_columns(model_name, 'candidate', varlist)\n 181 | fix_model_names = [\n 182 |   'cam24_il_Tot',\n 183 |   'cam24_il_Young',\n 184 |   'cam24_il_Guest',\n 185 |   'cam24_il_Top',\n 186 |   'cam24_il_Casual',\n 187 |   'cam24_il_BrandedXO',\n 188 |   'cam24_il_BrandedRecurring',\n 189 |   'cam24_il_P2PFF',\n 190 |   'cam24_il_P2PGS',\n 191 |   'cam24_il_EbayMor',\n 192 |   'cam24_il_Other',\n 193 |   'cam24_il_MidCAM22Score'\n 194 | ]\n 195 | # ## Norm\n 196 | woe.parallel_run(model_names=fix_model_names, \n 197 |                  commands=\"norm\", parallel_num=5, hide_log=False)\n 198 | # ## correlation\n 199 | import importlib  \n 200 | import helper\n 201 | importlib.reload(helper) \n 202 | from helper import *\n 203 | # !kinit cchen16 -l 3d -kt /notebooks/projects/cchen16/config/cchen16.keytab\n 204 | woe.parallel_run(model_names=fix_model_names, \n 205 |                  commands=\"stats -correlation -Dshifu.stats.corr.reuse=false\", parallel_num=5, hide_log=False)\n 206 | for model_name in fix_model_names:\n 207 |     corr_main(model_name, corrCut=0.8)\n 208 | # conf = woe.get_column_config(\"cam24_il_Tot\")\n 209 | # cnt = 0\n 210 | # for c in conf:\n 211 | #     if c['columnFlag'] == 'Candidate':\n 212 | #         cnt += 1\n 213 | #         if c['columnName'] == '_crc_flag_':\n 214 | #             print(c)\n 215 | # print(cnt)\n 216 | # ## Varsel\n 217 | segment_command = {}\n 218 | for model_name in model_names:\n 219 |     segment_command[model_name] = []\n 220 |     # segment_command[model_name].append(\"norm\")\n 221 |     segment_command[model_name].append(\"varsel -reset\")\n 222 |     segment_command[model_name].append(\"varsel -r {}\".format(se_round))\n 223 |     segment_command[model_name].append({\n 224 |                                         \"func\": delete_norm_data,\n 225 |                                         \"args\": [model_name]\n 226 |                                     })\n 227 |     segment_command[model_name].append({\n 228 |                                         \"func\": rewriteColumnConfig,\n 229 |                                         \"args\": [model_name]\n 230 |                                     })\n 231 |     segment_command[model_name].append(\"eval -norm Eval1\")   \n 232 | # # Final data paths\n 233 | # final_norm_path = {}\n 234 | # for model_name in model_names:\n 235 | #     shifu_hdfs_path = woe.get_model_config(model_name).get('basic').get('customPaths').get('hdfsModelSetPath')\n 236 | #     eval_norm_path = os.path.join(shifu_hdfs_path,model_name,'/evals/Eval1/EvalScore')\n 237 | #     final_norm_path[model_name] = eval_norm_path\n 238 | # print(final_norm_path)\n 239 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_0912_recent_8k_var_pig_w_crc/ | head \n 240 | # !gsutil du -h -s {gcs_path}\n 241 | # !hadoop fs -du -h -s {hdfs_folder}/cam_0912_recent_8k_var_pig_w_crc\n 242 | gcs_path = \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_0912_recent_8k_var_pig_w_crc\"\n 243 | hdfs_folder = \"/apps/risk/det/cchen16/il_new_variable/data\"\n 244 | # %maglev copy {gcs_path} {hdfs_folder}/",
    "rmr_agent/repos/CAM_variable_research/2_4_cam_shifu_all_in_one.py": "   1 | # # Config preparation\n   2 | # %url -c horton\n   3 | import sys,os\n   4 | utils_path = \"/projects/gds-focus/data/makang/CAM24/source_code/cam_utils\"\n   5 | sys.path.append(utils_path)\n   6 | utils_path = \"/projects/gds-focus/data/catch/IL/CAM_variable/assets\"\n   7 | sys.path.append(utils_path)\n   8 | from model_automation.woe import MultiSegmentWoe\n   9 | import pandas as pd\n  10 | import json\n  11 | from pyshifu.ShifuConf import shifuConf\n  12 | queue = \"risk_gds_focus\"\n  13 | shifuConf.set('hadoopJobQueue', queue)\n  14 | shifuConf.set('guagua.split.maxCombinedSplitSize', '3000000000')\n  15 | # shifuConf.set('guagua.split.maxCombinedSplitSize', '30000000')\n  16 | # # Parameters\n  17 | #\n  18 | working_path = \"/projects/gds-focus/data/catch/IL/CAM_variable\"\n  19 | username = os.environ['NB_USER']\n  20 | os.chdir(working_path)\n  21 | # os.environ['SHIFU_OPTS'] = '-Xms2G -Xmx4G'\n  22 | os.environ['SHIFU_OPTS'] = '-Xms2G -Xmx40G'\n  23 | from model_automation.woe import MultiSegmentWoe\n  24 | import sys, os\n  25 | import json\n  26 | import pandas as pd\n  27 | trigger_dt = '20240912'\n  28 | queue = \"risk_gds_focus\"\n  29 | se_round = 3\n  30 | se_round = int(se_round)\n  31 | sampleRate = 1.0\n  32 | sampleRate = float(sampleRate)\n  33 | corrCut = 0.8\n  34 | corrCut = float(corrCut)\n  35 | segment_file = f\"{working_path}/files/cam_il_segment.file\"\n  36 | common_conf_path = f\"{working_path}/files/cam_il_common_model_config.json\"\n  37 | hdfs_base_folder = \"/apps/risk/det/cchen16/il_new_variable\"\n  38 | hdfsModelSetPath = f\"{hdfs_base_folder}/cam_{trigger_dt}\" \n  39 | local_path = f\"{working_path}/cam_{trigger_dt}\"\n  40 | local_shifu_path = f\"{working_path}/cam_{trigger_dt}/shifu\" \n  41 | local_cols_path = f\"{local_shifu_path}/columns\"\n  42 | raw_data_path = \"/apps/risk/det/cchen16/il_new_variable/data/cam_0912_recent_8k_var_pig_w_crc\"\n  43 | os.makedirs(local_path, exist_ok=True)\n  44 | os.makedirs(local_shifu_path, exist_ok=True)\n  45 | os.makedirs(local_cols_path, exist_ok=True)\n  46 | # # Shifu - All in One\n  47 | local_shifu_path\n  48 | data_type = 'il'\n  49 | all_in_one_name = f\"cam_{trigger_dt}\"\n  50 | model_names = [all_in_one_name]\n  51 | local_shifu_path\n  52 | model_names\n  53 | data_type = 'il'\n  54 | all_in_one_name = f\"cam_{trigger_dt}_all_in_one\"\n  55 | model_names = [all_in_one_name]\n  56 | woe = MultiSegmentWoe()\n  57 | woe.load_or_new(model_names, local_shifu_path)\n  58 | df_seg = pd.read_csv(segment_file, dtype = 'str')\n  59 | df_seg\n  60 | # # ## copy columns folder\n  61 | cols_type = [\n  62 |     'meta'\n  63 |     ,'forceremove'\n  64 |     # ,'forceselect'\n  65 |     ,'categorical'\n  66 |     ,'candidate'\n  67 | ]\n  68 | for col_t in cols_type:\n  69 |     path = os.path.join('/projects/gds-focus/data/catch/IL/CAM_variable/files/candidate_pool_columns','{}.column.names'.format(col_t))\n  70 |     varlist = pd.read_csv(path,header=None).iloc[:,0].tolist()\n  71 |     for model_name in model_names:\n  72 |         woe.write_columns(model_name, col_t, varlist)\n  73 | # ## update config and segment.file\n  74 | df_seg = pd.read_csv(segment_file, dtype = 'str')\n  75 | df_seg\n  76 | # write segment.file\n  77 | df_seg = df_seg[~df_seg.seg_name.isin(['Guest1', 'Young1st'])]\n  78 | # path = os.path.join(woe.model_path[all_in_one_name],  'segment.file')\n  79 | # df_seg['data_filter'].to_csv(path, header=None, index=None)\n  80 | # print('segment.file saved at {}'.format(path))\n  81 | # update common config\n  82 | with open(common_conf_path,'r') as f:\n  83 |     common_conf = json.load(f)\n  84 | for key in common_conf.keys():\n  85 |     woe.update_model_config(model_names=model_names, field=key, update_config=common_conf.get(key))\n  86 | # update specific config - segment.file\n  87 | row = df_seg.iloc[0,:]\n  88 | job_dict = {}\n  89 | job_dict['dataSet'] = {'posTags':row['bad'].split('|'),\n  90 |                     'negTags':row['good'].split('|'),\n  91 |                     'filterExpressions':row['data_filter'],\n  92 |                     'weightColumnName':row['weight_col'],\n  93 |                     'targetColumnName':row['target_col'],\n  94 |                     'segExpressionFile':'segment.file'\n  95 |                     }\n  96 | for key in job_dict.keys():\n  97 |     woe.update_model_config(model_names=model_names, field=key, update_config=job_dict.get(key))\n  98 | # ## Init,stats and norm\n  99 | woe.parallel_run(model_names=model_names, commands=\"init\", parallel_num=5, hide_log=False)\n 100 | woe.parallel_run(model_names=model_names, commands=\"stats\", parallel_num=5, hide_log=False)\n 101 | # woe.parallel_run(model_names=model_names, commands=\"export -t columnstats\", parallel_num=5, hide_log=False)\n 102 | # woe.parallel_run(model_names=model_names, commands=\"norm\", parallel_num=5, hide_log=False)\n 103 | woe.parallel_run(model_names=model_names, commands=\"export -t columnstats\", parallel_num=5, hide_log=False)\n 104 | woe.parallel_run(model_names=model_names, commands=\"norm\", parallel_num=5, hide_log=False)\n 105 | # ## merge columns config\n 106 | sub_model_names = []\n 107 | seg_names = []\n 108 | top_ses = {}\n 109 | for r_index, row in df_seg.iterrows():\n 110 |     model_name = row['job_type']+ '_' + row['seg_name']\n 111 |     sub_model_names.append(model_name)\n 112 |     seg_names.append(row['seg_name'])\n 113 |     # top_ses[model_name] = int(row['top_se'])\n 114 |     top_ses[model_name] = 500\n 115 | local_shifu_path\n 116 | # df_seg = pd.read_csv('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam_20240912_all_in_one/segment.file',header=None)\n 117 | df_seg = df_seg.reset_index(drop=True)\n 118 | df_seg\n 119 | config_selected = []\n 120 | cnt = 0\n 121 | for i,row in df_seg.iterrows():\n 122 |     # print(i, row)\n 123 |     config_path = os.path.join(\"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu\", sub_model_names[i], 'ColumnConfig.json')\n 124 |     with open(config_path,'r') as f:\n 125 |         config = json.load(f)\n 126 |     seg_cnt = 0\n 127 |     for conf in config:\n 128 |         if '_crc_flag_' in conf['columnName']:\n 129 |             continue\n 130 |         if i > 0:\n 131 |             conf['columnName'] = conf['columnName'] + '_seg' + str(i)\n 132 |         if conf['columnFlag'] is None:\n 133 |             conf['columnFlag'] = 'Candidate'\n 134 |         conf['columnNum'] = cnt\n 135 |         config_selected.append(conf)\n 136 |         cnt = cnt + 1\n 137 |         seg_cnt += 1\n 138 |         # if 'acct_multi_evt_num_vid_720h' in conf['columnName']:\n 139 |         #     print(conf)\n 140 | # print('='*100)\n 141 | # print('select {} vars'.format(len(config_selected)))\n 142 | # print('last cnt is {}'.format(cnt))\n 143 | seg_cnt\n 144 | 7697\n 145 | # ### save all in one column config\n 146 | path = os.path.join('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam_20240912_all_in_one','ColumnConfig.json')\n 147 | with open(path, 'w') as f:\n 148 |     json.dump(config_selected, f, indent=4)\n 149 | all_in_one_name\n 150 | configs = woe.get_column_config(all_in_one_name)\n 151 | final_ls = []\n 152 | for conf in configs:\n 153 |     if conf['finalSelect'] and conf['columnFlag'] == 'Candidate':\n 154 |         final_ls.append(conf['columnName'])\n 155 | len(final_ls)\n 156 | len(configs)\n 157 | 110016/12\n 158 | with open(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse6000.txt\", 'w') as f:\n 159 |     f.write(\"\\n\".join(final_ls))\n 160 | with open(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse2400.txt\", 'w') as f:\n 161 |     f.write(\"\\n\".join(final_ls))\n 162 | util_path = '/projects/gds-focus/data/catch/IL/CAM_variable/assets'\n 163 | sys.path.append(util_path)\n 164 | # # !kinit cchen16 -l 3d -kt /notebooks/projects/cchen16/config/cchen16.keytab\n 165 | model_names\n 166 | with open(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse2400.txt\", 'w') as f:\n 167 |     f.write(\"\\n\".join(final_ls))\n 168 | woe.parallel_run(model_names=model_names, commands=\"eval -norm Eval1\", parallel_num=5, hide_log=False)\n 169 | # ## TOP CATEGORICAL check\n 170 | import json\n 171 | import os, sys\n 172 | import pandas as pd\n 173 | import pandas as pd\n 174 | pd.set_option('display.max_rows', 500)\n 175 | pd.set_option('display.max_columns', 500)\n 176 | pd.set_option('display.width', 1000)\n 177 | stats = pd.read_csv(\"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/ColumnStats.csv\")\n 178 | stats['columnFlag'].value_counts()\n 179 | cate_list = [\n 180 |     'billing_cntry',\n 181 | 'bin_country_sub_region',\n 182 | 'buyer_region_bu',\n 183 | 'c_dof_c',\n 184 | 'c_primary_residence_1',\n 185 | 'c_primary_timezone',\n 186 | 'c_region',\n 187 | 'country_code_cnsr_seg',\n 188 | 'dimension_region',\n 189 | 'fn_timezone_lower',\n 190 | 'lst_added_phone_country_code',\n 191 | 'mercury_industry_code',\n 192 | 'primary_residence',\n 193 | 'primarycc_country',\n 194 | 'primarycc_currency_code',\n 195 | 'rda_pq_channel_flow_auth_method',\n 196 | 's_dof_c',\n 197 | 'session_login_flow',\n 198 | 'v71_emc_mcc_code',\n 199 | 'v91_flow',\n 200 | ]\n 201 | stats[stats.columnName.isin(cate_list)]\n 202 | # ## Top IV and SE variable check\n 203 | def read_line_file(filepath):\n 204 |     if not os.path.exists(filepath):\n 205 |         return []\n 206 |     with open(filepath, 'r') as f:\n 207 |         lines = [line.strip() for line in f.readlines()]\n 208 |     return lines\n 209 | tot_stats = pd.read_csv(\"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/ColumnStats.csv\")\n 210 | pspgs_stats = pd.read_csv(\"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PGS/columns/ColumnStats.csv\")\n 211 | ebaymor_stats = pd.read_csv(\"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PGS/columns/ColumnStats.csv\")\n 212 | tot_top500_vars = read_line_file('./cam_20240912/shifu/cam24_il_Tot/columns/candidates_top500.txt')\n 213 | p2pgs_top500_vars = read_line_file('./cam_20240912/shifu/cam24_il_P2PGS/columns/candidates_top500.txt')\n 214 | extra = list(set(p2pgs_top500_vars)-set(tot_top500_vars))\n 215 | extra\n 216 | set(p2pgs_top500_vars)-set(tot_top500_vars)\n 217 | p2pgs_top500_vars\n 218 | tmp_stats = tot_stats[tot_stats.columnName.isin(tot_top500_vars)]\n 219 | tmp_stats['rank'] = tmp_stats['columnName'].apply(lambda x:tot_top500_vars.index(x))\n 220 | tmp_stats.sort_values(by='rank')\n 221 | pspgs_stats[pspgs_stats.columnName.isin(extra)].sort_values(by='iv', ascending=False)\n 222 | tot_stats[tot_stats.columnName.isin(tot_top500_vars)]\n 223 | tot_stats[(tot_stats.columnName.isin(tot_top500_vars))&(tot_stats.iv > 0.1)].shape\n 224 | pspgs_stats[(pspgs_stats.columnName.isin(p2pgs_top500_vars))&(~pspgs_stats.columnName.isin(tot_top500_vars))&(pspgs_stats.iv > 0.25)].shape\n 225 | pspgs_stats[(pspgs_stats.iv > 0.2)].shape",
    "rmr_agent/repos/CAM_variable_research/3_1_model_redefine.py": "   1 | # %url -c horton\n   2 | # %ppauth\n   3 | from tensorflow.keras import backend as K, Sequential, Model, layers\n   4 | from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation, Dropout, Flatten, Concatenate, Lambda, Add, Embedding, Reshape,LeakyReLU\n   5 | from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler, ReduceLROnPlateau, CSVLogger\n   6 | from tensorflow.keras import optimizers,regularizers\n   7 | from tensorflow.keras.regularizers import L2\n   8 | utils_path = \"/projects/gds-focus/data/makang/CAM24/source_code/cam_utils\"\n   9 | import sys\n  10 | sys.path.append(utils_path)\n  11 | import logging\n  12 | import random    \n  13 | import os\n  14 | import sys    \n  15 | import tensorflow as tf\n  16 | from tensorflow.keras.models import Sequential\n  17 | from tensorflow.keras.layers import Dense, Dropout\n  18 | from tensorflow.keras import optimizers\n  19 | from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n  20 | from tensorflow.keras import backend as K, Sequential, Model, layers\n  21 | from dl_pipeline_gcp import data_prepare, cr_fpr_Callback, MlflowLogCallback,_run_cmd,get_nn_config,random_split_train\n  22 | from debug import data_prepare_with_filter\n  23 | from dl_nn_set import get_multi, get_multi_1, get_mmoe_nn\n  24 | # ## Base model as logit inputs\n  25 | # ### basic as logit direct output\n  26 | from pyScoring import UMEModel\n  27 | from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate,CategoryEncoding,Multiply,Lambda\n  28 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/IL_CAM24/tf_models/keep_path/ | grep '20240926'\n  29 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/IL_CAM24/tf_models/keep_path/20240926005650/\n  30 | base_model_path = \"gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/IL_CAM24/tf_models/keep_path/20240912032747/best_model\"\n  31 | base_model = tf.keras.models.load_model(base_model_path)\n  32 | base_model.summary()\n  33 | base_model.output[1]\n  34 | def share_mlp_for3(shared_layers_length,activate_func,drop_out,kernel_initializer,regular=None,share_name='Shared_mlp'):\n  35 |     shared_mlp = tf.keras.models.Sequential(name=share_name)\n  36 |     for nb in shared_layers_length:\n  37 |         shared_mlp.add(Dense(nb,activation=activate_func, kernel_initializer=kernel_initializer, kernel_regularizer=regular))\n  38 |         shared_mlp.add(BatchNormalization())\n  39 |         shared_mlp.add(Dropout(rate = drop_out))\n  40 |     return shared_mlp\n  41 | base_model.input\n  42 | from model_automation.deep_learning_pipeline import package_model\n  43 | ensemble_model = tf.keras.models.Model(inputs, outputs, name='adp_rob_ems')\n  44 | for layer in ensemble_model.layers:\n  45 |     layer.trainable = False\n  46 |     if (layer.name == 'new_adp_part' or layer.name == 'new_rob_part'):\n  47 |         for t in layer.layers:\n  48 |             if (t.name == 'MMoE_Layer'):\n  49 |                 t.trainable = False\n  50 | new_var_input = tf.keras.layers.Input(shape=(200,), name='new_var')\n  51 | tc_output = base_model.output[0]\n  52 | yg_output = base_model.output[1]\n  53 | new_var_concat_input = Concatenate()([tc_output, yg_output, new_var_input])\n  54 | new_var_mlp_emb = share_mlp_for3([64, 32], \n  55 |                                  tf.nn.relu, \n  56 |                                  drop_out=0.5, \n  57 |                                  kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), \n  58 |                                  regular=None, \n  59 |                                  share_name='bottom_mlp_new_var')(new_var_concat_input)\n  60 | new_var_output = tf.keras.layers.Dense(1, name='cls',activation='sigmoid')(new_var_mlp_emb)\n  61 | inputs = [base_model.input, new_var_input]\n  62 | outputs = [new_var_output]\n  63 | # logit_v1_model = tf.keras.Model(inputs=inputs, \n  64 | #                        outputs=outputs,\n  65 | #                        name='CAM24_IL_New_Var_add_logit_V1')\n  66 | logit_v2_model = tf.keras.Model(inputs=inputs, \n  67 |                        outputs=outputs,\n  68 |                        name='CAM24_IL_New_Var_add_logit_V2')\n  69 | logit_v3_model = tf.keras.Model(inputs=inputs, \n  70 |                        outputs=outputs,\n  71 |                        name='CAM24_IL_New_Var_add_logit_V3')\n  72 | logit_v3_model.summary()\n  73 | logit_v2_model.summary()\n  74 | logit_v1_model.inputs\n  75 | for layer in logit_v3_model.layers:\n  76 |     if layer.name in {'new_var','bottom_mlp_new_var','cls'}:\n  77 |         layer.trainable = True\n  78 |     else:\n  79 |         layer.trainable = False\n  80 |     # if (layer.name == 'adp_rob_ems'):\n  81 |     #     for t in layer.layers:\n  82 |     #         # if (t.name == 'MMoE_Layer'):\n  83 |     #         #     t.trainable = False\n  84 |     #         print(t.name)\n  85 | logit_v1_model.save(\"/projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/cam_il_new_var_logit_v1\")\n  86 | logit_v2_model.save(\"/projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/cam_il_new_var_logit_v2\")\n  87 | logit_v3_model.save(\"/projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/cam_il_new_var_logit_v3\")\n  88 | gcs_model_path = \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models\"\n  89 | # !gsutil ls {gcs_model_path}\n  90 | # !gsutil -m cp -r /projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/cam_il_new_var_logit_v5 {gcs_model_path}/\n  91 | import logging\n  92 | import random\n  93 | import tensorflow as tf\n  94 | import os\n  95 | class global_data_prepare_for_distill(object):\n  96 |     def __init__(self):\n  97 |         self.logger = self._build_logger()\n  98 |         self._feature_list = None\n  99 |         self._target_list = None\n 100 |         self._teacher_scores = None\n 101 |         self._teacher_scores_range = None\n 102 |         self._weight_list = None\n 103 |         self._target_refine = None\n 104 |         self._f = None\n 105 |         self._sample = None\n 106 |         self._weight_fun = None\n 107 |         self._filter_list = None\n 108 |     def _build_logger(self):\n 109 |         # build logger class\n 110 |         logger = logging.getLogger(__name__)\n 111 |         logger.setLevel(logging.INFO)\n 112 |         ch = logging.StreamHandler()\n 113 |         ch.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n 114 |         logger.addHandler(ch)\n 115 |         return logger\n 116 |     def make_dataset(self, filenames, feature_list, target_list, teacher_scores, teacher_scores_range, batch_size, buffer_size = None, \\\n 117 |                      weight_list = None, compress = 'GZIP', weight_fun = lambda x: x,  \\\n 118 |                      shuffle = True, multi_task=False, filter_list=None , filter_raw_value=None, filter_new_value=None,target_refine={}):\n 119 |         if shuffle:\n 120 |             random.shuffle(filenames)\n 121 |         dataset = tf.data.TFRecordDataset(filenames = filenames,  \\\n 122 |                                           compression_type = compress, num_parallel_reads = tf.data.experimental.AUTOTUNE)\n 123 |         # fetch structure & feature name from tfrecord sample\n 124 |         for elem in dataset:\n 125 |             sample = elem\n 126 |             break\n 127 |         self._sample = sample\n 128 |         example = tf.train.SequenceExample()\n 129 |         example.ParseFromString(sample.numpy())\n 130 |         whole_list = []\n 131 |         target_list = target_list\n 132 |         for key, value in example.context.feature.items():\n 133 |             whole_list.append([key, value.WhichOneof('kind')])\n 134 |             if key in target_list:\n 135 |         # create parse mapping according to original data type\n 136 |         maps = {'float_list': tf.float32, 'bytes_list': tf.string, 'int64_list': tf.int64}\n 137 |         _f = {}\n 138 |         for elem in whole_list:\n 139 |             default = 0.0 if elem[1]=='float_list' else None\n 140 |             _f[elem[0]] = tf.io.FixedLenSequenceFeature([], dtype = maps[elem[1]], allow_missing=True, default_value=default)\n 141 |         self._f = _f\n 142 |         self._weight_fun = weight_fun\n 143 |         self.multi_task = multi_task\n 144 |         # generate parse function\n 145 |         self._feature_list = feature_list\n 146 |         self._filter_list = filter_list\n 147 |         self._filter_raw_value = filter_raw_value\n 148 |         self._filter_new_value = filter_new_value\n 149 |         self._target_list = target_list\n 150 |         self._teacher_scores = teacher_scores\n 151 |         self._teacher_scores_range = teacher_scores_range\n 152 |         self._target_refine = target_refine\n 153 |         self._weight_list = weight_list\n 154 |         # complete tf dataset\n 155 |         if buffer_size is None:\n 156 |             buffer_size = batch_size * 100\n 157 |         if shuffle:\n 158 |             dataset = dataset.shuffle(buffer_size, reshuffle_each_iteration=True)\n 159 |         dataset = dataset.repeat()\n 160 |         dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n 161 |         dataset = dataset.map(self._parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n 162 |         return dataset\n 163 |     def _parse_function(self, proto):\n 164 |         parsed_features = tf.io.parse_example(proto, self._f)\n 165 |         feature_vecs = []\n 166 |         for f_list in self._feature_list:\n 167 |             feature_vec = tf.cast(tf.stack([parsed_features[x] for x in f_list], 1), dtype = tf.float32)\n 168 |             feature_vec = tf.reshape(feature_vec, tf.shape(feature_vec)[0:2])\n 169 |             feature_vecs.append(feature_vec)\n 170 |         teacher_score_fillna = -99999.0\n 171 |         if self._target_list is not None:\n 172 |             target_vec = []\n 173 |             for x in self._target_list:\n 174 |                 elem = parsed_features[x]\n 175 |                 if elem.dtype == tf.string:\n 176 |                     t = tf.strings.to_number(elem, out_type=tf.float32)\n 177 |                 else:\n 178 |                     t = tf.cast(elem, tf.float32)\n 179 |                 target_vec.append(t)\n 180 |             # extract teacher score vector if exists, cast all string to float and then stack\n 181 |             for i, x in enumerate(self._teacher_scores):\n 182 |                 elem = parsed_features[x]\n 183 |                 if elem.dtype == tf.string:\n 184 |                     t = tf.strings.to_number(elem, out_type=tf.float32)\n 185 |                 else:\n 186 |                     t = tf.cast(elem, tf.float32)\n 187 |                 # teacher score scale to [0., 1.]\n 188 |                 if len(self._teacher_scores_range)>0 and len(self._teacher_scores_range)==len(self._teacher_scores):\n 189 |                     min_scr, max_scr = self._teacher_scores_range[i][0], self._teacher_scores_range[i][1]\n 190 |                     t = (t-min_scr)/(max_scr-min_scr)\n 191 |                 # if teacher score is nan, fill nan with teacher_score_fillna\n 192 |                 t = tf.where(tf.math.is_nan(t), teacher_score_fillna, t)\n 193 |                 target_vec.append(t)\n 194 |             target_vec = tf.stack(target_vec, 1)\n 195 |             target_vec = tf.reshape(target_vec, tf.shape(target_vec)[0:2])\n 196 |         else:\n 197 |             target_vec = tf.constant(0.0, shape=(1, 1)) # use dummy tensor\n 198 |         # extract weight vector, cast all string to float and then stack\n 199 |         if self._weight_list is not None:\n 200 |             weight_vec = []\n 201 |             for i, x in enumerate(self._weight_list):\n 202 |                 elem = parsed_features[x]\n 203 |                 if elem.dtype == tf.string:\n 204 |                     w = tf.strings.to_number(elem, out_type=tf.float32)\n 205 |                 else:\n 206 |                     w = tf.cast(elem, tf.float32)\n 207 |                 tmp_t = tf.gather(target_vec, [i], axis=1)\n 208 |                 # for target wgt\n 209 |                 if i<len(self._target_list):\n 210 |                     # only train on 0/1 target, if target is None or not in (0., 1.), sample wgt set to 0.\n 211 |                     w = tf.where((tmp_t>1.0)|(tmp_t<0.0)|(tf.math.is_nan(tmp_t)),0.0,w)\n 212 |                 # for teacher score wgt\n 213 |                 else:\n 214 |                     # if teacher score is nan, sample wgt set to 0.\n 215 |                     w = tf.where(tmp_t==teacher_score_fillna, 0.0, w)\n 216 |                 weight_vec.append(w)\n 217 |             weight_vec = tf.stack(weight_vec, 1)\n 218 |             weight_vec = tf.reshape(weight_vec, tf.shape(weight_vec)[0:2])\n 219 |             # multi task situation, split target and weight into tuples\n 220 |             if len(self._weight_list) > 1:\n 221 |                 # target_vec = tuple(tf.split(target_vec, len(self._target_list), axis=1))\n 222 |                 target_vec = tuple(tf.split(target_vec, len(self._target_list)+len(self._teacher_scores), axis=1))\n 223 |                 weight_vec = tuple(tf.split(weight_vec, len(self._weight_list), axis=1))\n 224 |         # without weight (unit weight)\n 225 |         else:\n 226 |             weight_vec = tf.ones(tf.stack([tf.shape(target_vec)[0], tf.constant(1)]), tf.float32)\n 227 |         if (len(feature_vecs) > 1):\n 228 |             return_tuple = (tuple(feature_vecs), target_vec, self._weight_fun(weight_vec))\n 229 |         else:\n 230 |             return_tuple = (feature_vecs[0], target_vec, self._weight_fun(weight_vec))\n 231 |         return return_tuple\n 232 | new_data_prepare = global_data_prepare_for_distill()\n 233 | gcs_tf_record_path = 'gs://pypl-bkt-rsh-row-std-gds-focus/user/junrzhang/IL/simulation/CAM_IL_data_replay_recent_woe_0601_20240815/part-00000-ba0b157b-79f0-42be-bf98-91ad494aabeb-c000.tfrecord.gz'\n 234 | local_tf_record_path = '/projects/junrzhang/IL/data/tf_record/part-00000-ba0b157b-79f0-42be-bf98-91ad494aabeb-c000.tfrecord.gz'\n 235 | dev_files_tot = tf.io.gfile.glob(local_tf_record_path)\n 236 | prefix_path = '/projects/gds-focus/data/makang/CAM24/source_code/model_ems/submodel/adaptive/'\n 237 | ume_list = [\n 238 | prefix_path + 'adaptive_woe.m',\n 239 | prefix_path + 'robust_woe.m',\n 240 | prefix_path + 'adaptive_woe.m'\n 241 | ]\n 242 | new_feature_list = []\n 243 | for ume_path in ume_list:\n 244 |     m = UMEModel(ume_path)\n 245 |     new_feature_list.append(m.outputs)\n 246 | len(new_feature_list)\n 247 | new_data_set = new_data_prepare.make_dataset(filenames=dev_files_tot, \n 248 |                                                     feature_list=new_feature_list, \n 249 |                                                     target_list=['driver_is_brm3_4_11_15_bad'],\n 250 |                                                     teacher_scores=['cnsr_ems_score_aligned'],\n 251 |                                                     teacher_scores_range=[[0.0, 1000.0]],\n 252 |                                                     weight_list=['driver_train_wgt_3', 'driver_train_wgt_3'],\n 253 |                                                     batch_size=16, buffer_size=16, multi_task=True, filter_list=[], filter_raw_value=[], \n 254 |                                                     filter_new_value=[]\n 255 |                                                    , compress='GZIP')\n 256 | for ele in new_data_set:\n 257 |     break\n 258 | def extract_embed(tf_model, layer_name, input_name,new_name):\n 259 | adp_tf_model_path = '/projects/gds-focus/data/makang/CAM24/source_code/adaptive/model/large_bottom_super/final_model/TagFineTune_best'\n 260 | rob_tf_model_path = '/projects/gds-focus/data/makang/CAM24/source_code/robust/model/large_bottom_super/final_model/robust_large_bottom_super_mmoe_id_0_transfer_learning_10expert_2task_best'\n 261 | ems_tf_model_path = '/projects/gds-focus/data/makang/CAM24/source_code/ems/model/DeepFM_transfer_new_1task_202305_202403/final_model/DeepFM_transfer_new_1task_202305_202403_best'\n 262 | ems_model = tf.keras.models.load_model(ems_tf_model_path)\n 263 | ems_model.summary()\n 264 | adp_tf_model = tf.keras.models.load_model(adp_tf_model_path)\n 265 | rob_tf_model = tf.keras.models.load_model(rob_tf_model_path)\n 266 | adp_input = adp_tf_model.input\n 267 | rob_input = rob_tf_model.input\n 268 | adp_tf_model_new = tf.keras.models.Model(adp_input, adp_tf_model.get_layer('dropout_14').output, name='new_adp_part') \n 269 | rob_tf_model_new = tf.keras.models.Model(rob_input, rob_tf_model.get_layer('dropout_14').output, name='new_rob_part') \n 270 | inputs = [tf.keras.layers.Input(adp_tf_model_new.input.shape[1],name='adaptive_input'), \\\n 271 |           tf.keras.layers.Input(rob_tf_model_new.input.shape[1],name='robust_input')]\n 272 | outputs = [adp_tf_model_new(inputs[0]), rob_tf_model_new(inputs[1])]\n 273 | len(new_feature_list[2])\n 274 | adp_yg_input = tf.keras.layers.Input(len(new_feature_list[2]),name='adaptive_yg_input')\n 275 | ensemble_model = tf.keras.models.Model(inputs, outputs, name='adp_rob_ems')\n 276 | for layer in ensemble_model.layers:\n 277 |     layer.trainable = False\n 278 |     if (layer.name == 'new_adp_part' or layer.name == 'new_rob_part'):\n 279 |         for t in layer.layers:\n 280 |             if (t.name == 'MMoE_Layer'):\n 281 |                 t.trainable = False\n 282 | inputs = ensemble_model.input\n 283 | mid = ensemble_model(inputs)\n 284 | task_names = ['cls_tc', 'cls_yg']\n 285 | output_activate = ['sigmoid', 'sigmoid']\n 286 | def share_mlp_for3(shared_layers_length,activate_func,drop_out,kernel_initializer,regular=None,share_name='Shared_mlp'):\n 287 |     shared_mlp = tf.keras.models.Sequential(name=share_name)\n 288 |     for nb in shared_layers_length:\n 289 |         shared_mlp.add(Dense(nb,activation=activate_func, kernel_initializer=kernel_initializer, kernel_regularizer=regular))\n 290 |         shared_mlp.add(BatchNormalization())\n 291 |         shared_mlp.add(Dropout(rate = drop_out))\n 292 |     return shared_mlp\n 293 | def reduce_sum(input_tensor,\n 294 |                axis=None,\n 295 |                keep_dims=False,\n 296 |                name=None,\n 297 |                reduction_indices=None):\n 298 |     try:\n 299 |         return tf.reduce_sum(input_tensor,\n 300 |                              axis=axis,\n 301 |                              keep_dims=keep_dims,\n 302 |                              name=name,\n 303 |                              reduction_indices=reduction_indices)\n 304 |     except TypeError:\n 305 |         return tf.reduce_sum(input_tensor,\n 306 |                              axis=axis,\n 307 |                              keepdims=keep_dims,\n 308 |                              name=name)\n 309 | # FM part\n 310 | stack_tc = tf.stack(mid, axis=1)\n 311 | square_of_sum_tc = tf.square(reduce_sum(\n 312 |         stack_tc, axis=1, keep_dims=True))\n 313 | sum_of_square_tc = reduce_sum(\n 314 |         stack_tc * stack_tc, axis=1, keep_dims=True)\n 315 | FM_out_tc = square_of_sum_tc - sum_of_square_tc\n 316 | FM_out_tc = 0.5 * reduce_sum(FM_out_tc, axis=2, keep_dims=False)\n 317 | concat_tc = tf.keras.layers.Concatenate(axis=-1,name='ensemble_embedding')(mid)\n 318 | yg_mlp_emb = share_mlp_for3([512, 128], tf.nn.relu, drop_out=0.5, \n 319 |                          kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), regular=None, share_name='bottom_mlp_yg')(adp_yg_input)\n 320 | yg_mlp_emb\n 321 | mid_yg = [mid[0], mid[1], yg_mlp_emb]\n 322 | mid_yg\n 323 | concat_yg = tf.keras.layers.Concatenate(axis=-1,name='concat_yg')(mid_yg)\n 324 | concat_yg\n 325 | # Linear part\n 326 | linear_out_tc = tf.keras.layers.Dense(units=1,\n 327 |                                     name= 'linear_tc',\n 328 |                                     activation= None,\n 329 |                                     kernel_initializer=tf.keras.initializers.VarianceScaling())(concat_tc)\n 330 | # Linear part\n 331 | linear_out_yg = tf.keras.layers.Dense(units=1,\n 332 |                                     name= 'linear_yg',\n 333 |                                     activation= None,\n 334 |                                     kernel_initializer=tf.keras.initializers.VarianceScaling())(concat_yg)\n 335 | # FM part\n 336 | stack_yg = tf.stack(mid_yg, axis=1)\n 337 | square_of_sum_yg = tf.square(reduce_sum(\n 338 |         stack_yg, axis=1, keep_dims=True))\n 339 | sum_of_square_yg = reduce_sum(\n 340 |         stack_yg * stack_yg, axis=1, keep_dims=True)\n 341 | FM_out_yg = square_of_sum_yg - sum_of_square_yg\n 342 | FM_out_yg = 0.5 * reduce_sum(FM_out_yg, axis=2, keep_dims=False)\n 343 | final_outputs = []\n 344 | kernel_initializer = tf.keras.initializers.glorot_normal(seed=1024)\n 345 | mlp_out_tc = share_mlp_for3([64, 16], tf.nn.relu, drop_out=0.5, \n 346 |                          kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), regular=None, share_name='mlp_tc')(concat_tc)\n 347 | nn_out_tc = tf.keras.layers.Dense(units=1,\n 348 |                                     name= 'nn_pred_' + 'tc',\n 349 |                                     activation= None,\n 350 |                                     kernel_initializer=tf.keras.initializers.VarianceScaling())(mlp_out_tc)\n 351 | mlp_out_yg = share_mlp_for3([64, 16], tf.nn.relu, drop_out=0.5, \n 352 |                          kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), regular=None, share_name='mlp_yg')(concat_yg)\n 353 | nn_out_yg = tf.keras.layers.Dense(units=1,\n 354 |                                     name= 'nn_pred_' + 'yg',\n 355 |                                     activation= None,\n 356 |                                     kernel_initializer=tf.keras.initializers.VarianceScaling())(mlp_out_yg)\n 357 | task_logit_tc = Add(name='logit_' + 'tc')([nn_out_tc, linear_out_tc, FM_out_tc])\n 358 | task_logit_yg = Add(name='logit_' + 'yg')([nn_out_yg, linear_out_yg, FM_out_yg])\n 359 | final_outputs.append(tf.sigmoid(task_logit_tc))\n 360 | final_outputs.append(tf.sigmoid(task_logit_yg))\n 361 | [inputs[0], inputs[1], adp_yg_input]\n 362 | model = tf.keras.Model([inputs[0], inputs[1], adp_yg_input] , \n 363 |                        outputs = final_outputs,\n 364 |                        # outputs = {task_names[0]: final_outputs[0], task_names[1]: final_outputs[1]},\n 365 |                        name='CAM24_IL_DeepFM_YGFilter_3towers_debug')\n 366 | old_ems_tf_model = tf.keras.models.load_model(ems_tf_model_path)\n 367 | model.get_layer('adp_rob_ems').set_weights(old_ems_tf_model.get_layer('adp_rob_ems').get_weights())\n 368 | model.get_layer('adp_rob_ems').trainable = False \n 369 | model.get_layer('mlp_tc').set_weights(old_ems_tf_model.get_layer('mlp_for_task_cls').get_weights())\n 370 | model.get_layer('mlp_tc').trainable = False \n 371 | model.get_layer('nn_pred_tc').set_weights(old_ems_tf_model.get_layer('nn_pred_cls').get_weights())\n 372 | model.get_layer('nn_pred_tc').trainable = False \n 373 | model.get_layer('linear_tc').set_weights(old_ems_tf_model.get_layer('linear_part').get_weights())\n 374 | model.get_layer('linear_tc').trainable = False \n 375 | model.summary()\n 376 | model_config = {}\n 377 | model_config['lr'] = 0.001\n 378 | model_config['loss_functions'] = ['binary_crossentropy', 'binary_crossentropy']\n 379 | model_config['loss_weight'] = [0.0, 1.0]\n 380 | lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n 381 |                                         initial_learning_rate=model_config['lr'],\n 382 |                                         decay_steps=4000,\n 383 |                                         decay_rate=0.96,\n 384 |                                         staircase=True)\n 385 | model_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n 386 | model.compile(optimizer=model_optimizer, loss=model_config['loss_functions'], loss_weights=model_config['loss_weight'])\n 387 | model.save('/projects/junrzhang/IL/model/ems/CAM24_IL_DeepFM_YGFilter_3towers_debug')\n 388 | model.fit(new_data_set,\n 389 |                   epochs=16,\n 390 |                   verbose=2,\n 391 |                   shuffle=True,\n 392 |                   validation_data=new_data_set,\n 393 |                   steps_per_epoch=10,\n 394 |                   validation_steps=10)\n 395 | local_path = '/projects/junrzhang/IL/model/ems//CAM24_IL_DeepFM_YGFilter_3towers_debug'\n 396 | gcs_path = 'gs://pypl-bkt-rsh-row-std-gds-focus/user/junrzhang/IL/model/ems/CAM24_IL_DeepFM_YGFilter_3towers_debug_Rebuild'\n 397 | # !gsutil rm -r $gcs_path\n 398 | from model_automation.utils.rmr import load_parameters, get_parameter, OutputRegistration, run_cmd\n 399 | run_cmd('gsutil -m cp -r {} {}'.format(local_path, gcs_path))\n 400 | # !gsutil ls $gcs_path\n 401 | # ### TC/YG ems, MLP and new var ensemble\n 402 | prit('h')\n 403 | base_model_path = \"gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/IL_CAM24/tf_models/keep_path/20240912032747/best_model\"\n 404 | base_model = tf.keras.models.load_model(base_model_path)\n 405 | base_model.summary()\n 406 | base_model.input\n 407 | def build_model(\n 408 |                base_model_path,\n 409 |                ensemble_model_path=None,\n 410 |                freeze_yg=True):\n 411 |     adp_tf_model_path = '/projects/gds-focus/data/makang/CAM24/source_code/adaptive/model/large_bottom_super/final_model/TagFineTune_best'\n 412 |     rob_tf_model_path = '/projects/gds-focus/data/makang/CAM24/source_code/robust/model/large_bottom_super/final_model/robust_large_bottom_super_mmoe_id_0_transfer_learning_10expert_2task_best'\n 413 |     adp_tf_model = tf.keras.models.load_model(adp_tf_model_path)\n 414 |     rob_tf_model = tf.keras.models.load_model(rob_tf_model_path)\n 415 |     adp_input = adp_tf_model.input\n 416 |     rob_input = rob_tf_model.input\n 417 |     adp_tf_model_new = tf.keras.models.Model(adp_input, adp_tf_model.get_layer('dropout_14').output, name='new_adp_part') \n 418 |     rob_tf_model_new = tf.keras.models.Model(rob_input, rob_tf_model.get_layer('dropout_14').output, name='new_rob_part') \n 419 |     inputs = [tf.keras.layers.Input(adp_tf_model_new.input.shape[1],name='adaptive_input'), \\\n 420 |               tf.keras.layers.Input(rob_tf_model_new.input.shape[1],name='robust_input')]\n 421 |     outputs = [adp_tf_model_new(inputs[0]), rob_tf_model_new(inputs[1])]\n 422 |     ensemble_model = tf.keras.models.Model(inputs, outputs, name='adp_rob_ems')\n 423 |     for layer in ensemble_model.layers:\n 424 |         layer.trainable = False\n 425 |         if (layer.name == 'new_adp_part' or layer.name == 'new_rob_part'):\n 426 |             for t in layer.layers:\n 427 |                 if (t.name == 'MMoE_Layer'):\n 428 |                     t.trainable = False\n 429 |     mid = ensemble_model(inputs)\n 430 |     inputs = ensemble_model.input\n 431 |     task_names = ['cls_tc', 'cls_yg']\n 432 |     output_activate = ['sigmoid', 'sigmoid']\n 433 |     # FM part\n 434 |     stack_tc = tf.stack(mid, axis=1)\n 435 |     square_of_sum_tc = tf.square(reduce_sum(\n 436 |             stack_tc, axis=1, keep_dims=True))\n 437 |     sum_of_square_tc = reduce_sum(\n 438 |             stack_tc * stack_tc, axis=1, keep_dims=True)\n 439 |     FM_out_tc = square_of_sum_tc - sum_of_square_tc\n 440 |     FM_out_tc = 0.5 * reduce_sum(FM_out_tc, axis=2, keep_dims=False)\n 441 |     concat_tc = tf.keras.layers.Concatenate(axis=-1,name='ensemble_embedding')(mid)\n 442 |     adp_yg_input = tf.keras.layers.Input(2000,name='adaptive_yg_input')\n 443 |     yg_mlp_emb = share_mlp_for3([512, 128], tf.nn.relu, drop_out=0.5, \n 444 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), regular=None, share_name='bottom_mlp_yg')(adp_yg_input)\n 445 |     new_var_input = tf.keras.layers.Input(shape=(500, ), name='new_var')\n 446 |     new_var_mlp_emb = share_mlp_for3([256, 128], \n 447 |                                      tf.nn.relu, \n 448 |                                      drop_out=0.5, \n 449 |                                      kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), \n 450 |                                      regular=None, \n 451 |                                      share_name='bottom_mlp_new_var')(new_var_input)\n 452 |     mid_yg = [mid[0], mid[1], yg_mlp_emb, new_var_mlp_emb]\n 453 |     concat_yg = tf.keras.layers.Concatenate(axis=-1,name='concat_yg')(mid_yg)\n 454 |     # Linear part\n 455 |     linear_out_tc = tf.keras.layers.Dense(units=1,\n 456 |                                         name= 'linear_tc',\n 457 |                                         activation= None,\n 458 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(concat_tc)\n 459 |     # Linear part\n 460 |     linear_out_yg = tf.keras.layers.Dense(units=1,\n 461 |                                         name= 'linear_yg',\n 462 |                                         activation= None,\n 463 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(concat_yg)\n 464 |     # FM part\n 465 |     stack_yg = tf.stack(mid_yg, axis=1)\n 466 |     square_of_sum_yg = tf.square(reduce_sum(\n 467 |             stack_yg, axis=1, keep_dims=True))\n 468 |     sum_of_square_yg = reduce_sum(\n 469 |             stack_yg * stack_yg, axis=1, keep_dims=True)\n 470 |     FM_out_yg = square_of_sum_yg - sum_of_square_yg\n 471 |     FM_out_yg = 0.5 * reduce_sum(FM_out_yg, axis=2, keep_dims=False)\n 472 |     final_outputs = []\n 473 |     kernel_initializer = tf.keras.initializers.glorot_normal(seed=1024)\n 474 |     mlp_out_tc = share_mlp_for3([64, 16], tf.nn.relu, drop_out=0.5, \n 475 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), regular=None, share_name='mlp_tc')(concat_tc)\n 476 |     nn_out_tc = tf.keras.layers.Dense(units=1,\n 477 |                                         name= 'nn_pred_' + 'tc',\n 478 |                                         activation= None,\n 479 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(mlp_out_tc)\n 480 |     mlp_out_yg = share_mlp_for3([64, 16], tf.nn.relu, drop_out=0.5, \n 481 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), share_name='mlp_yg')(concat_yg)\n 482 |     nn_out_yg = tf.keras.layers.Dense(units=1,\n 483 |                                         name= 'nn_pred_' + 'yg',\n 484 |                                         activation= None,\n 485 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(mlp_out_yg)\n 486 |     task_logit_tc = Add(name='logit_' + 'tc')([nn_out_tc, linear_out_tc, FM_out_tc])\n 487 |     task_logit_yg = Add(name='logit_' + 'yg')([nn_out_yg, linear_out_yg, FM_out_yg])\n 488 |     final_outputs.append(tf.keras.layers.Activation('sigmoid',name='tc_cls')(task_logit_tc))\n 489 |     final_outputs.append(tf.keras.layers.Activation('sigmoid',name='yg_cls')(task_logit_yg))\n 490 |     model = tf.keras.Model([inputs[0], inputs[1], adp_yg_input, new_var_input] , \n 491 |                            outputs = final_outputs,\n 492 |                            name='CAM24_IL_DeepFM_YGFilter_4towers_new_var')\n 493 |     best_model = tf.keras.models.load_model(base_model_path)\n 494 |     model.get_layer('adp_rob_ems').set_weights(best_model.get_layer('adp_rob_ems').get_weights())\n 495 |     model.get_layer('adp_rob_ems').trainable = False \n 496 |     model.get_layer('bottom_mlp_yg').set_weights(best_model.get_layer('bottom_mlp_yg').get_weights())\n 497 |     model.get_layer('bottom_mlp_yg').trainable = False  ## trainable=True or false\n 498 |     # model.get_layer('mlp_yg').set_weights(best_model.get_layer('mlp_yg').get_weights())\n 499 |     # model.get_layer('mlp_yg').trainable = False \n 500 |     # model.get_layer('nn_pred_yg').set_weights(best_model.get_layer('nn_pred_yg').get_weights())\n 501 |     # model.get_layer('nn_pred_yg').trainable = False \n 502 |     # model.get_layer('linear_yg').set_weights(best_model.get_layer('linear_yg').get_weights())\n 503 |     # model.get_layer('linear_yg').trainable = False \n 504 |     model.get_layer('mlp_tc').set_weights(best_model.get_layer('mlp_tc').get_weights())\n 505 |     model.get_layer('mlp_tc').trainable = False \n 506 |     model.get_layer('nn_pred_tc').set_weights(best_model.get_layer('nn_pred_tc').get_weights())\n 507 |     model.get_layer('nn_pred_tc').trainable = False \n 508 |     model.get_layer('linear_tc').set_weights(best_model.get_layer('linear_tc').get_weights())\n 509 |     model.get_layer('linear_tc').trainable = False \n 510 |     model.get_layer('mlp_yg').trainable = True \n 511 |     model.get_layer('linear_yg').trainable = True \n 512 |     model.get_layer('nn_pred_yg').trainable = True\n 513 |     model.summary()\n 514 |     model.save(ensemble_model_path)\n 515 | build_model(\"gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/IL_CAM24/tf_models/keep_path/20240926005650/best_model\",\n 516 |            ensemble_model_path=\"/projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/cam_il_new_var_logit_v6_0926\"\n 517 |            )\n 518 | final_outputs = []\n 519 | new_var_input = tf.keras.layers.Input(shape=(500, ), name='new_var')\n 520 | tc_output = base_model.output[0]\n 521 | final_outputs.append(tc_output)\n 522 | new_var_mlp_emb = share_mlp_for3([128, 64], \n 523 |                                  tf.nn.relu, \n 524 |                                  drop_out=0.5, \n 525 |                                  kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), \n 526 |                                  regular=None, \n 527 |                                  share_name='bottom_mlp_new_var')(new_var_input)\n 528 | new_var_logit = tf.keras.layers.Dense(units=1,\n 529 |                                     name= 'new_var_' + 'yg',\n 530 |                                     activation= None,\n 531 |                                     kernel_initializer=tf.keras.initializers.VarianceScaling())(new_var_mlp_emb)\n 532 | old_logit_yg = base_model.get_layer('logit_yg').output\n 533 | logit_yg_final = Add(name='logit_' + 'yg_final')([old_logit_yg, new_var_logit])\n 534 | final_outputs.append(tf.sigmoid(logit_yg_final))\n 535 | inputs = [base_model.input, new_var_input]\n 536 | logit_v5_model = tf.keras.Model(inputs=inputs, \n 537 |                        outputs=final_outputs,\n 538 |                        name='CAM24_IL_New_Var_add_logit_V5')\n 539 | logit_v5_model.summary()\n 540 | for layer in logit_v5_model.layers:\n 541 |     if layer.name in {'bottom_mlp_new_var','new_var_yg'}:\n 542 |         layer.trainable = True\n 543 |     else:\n 544 |         layer.trainable = False\n 545 |     # if (layer.name == 'adp_rob_ems'):\n 546 |     #     for t in layer.layers:\n 547 |     #         # if (t.name == 'MMoE_Layer'):\n 548 |     #         #     t.trainable = False\n 549 |     #         print(t.name)\n 550 | logit_v5_model.save(\"/projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/cam_il_new_var_logit_v5\")\n 551 | logit_v5_model.inputs, logit_v5_model.outputs\n 552 | # ### Extract embedding\n 553 | tf_model_path = \"/projects/gds-focus/data/makang/CAM24/source_code/ems/model/DeepFM_transfer_new_1task_202305_202403/final_model/DeepFM_transfer_new_1task_202305_202403_best\"\n 554 | base_model = tf.keras.models.load_model(tf_model_path)\n 555 | base_model.summary()\n 556 | base_model.inputs\n 557 | inputs = base_model.inputs\n 558 | tf_model_new = tf.keras.models.Model(inputs, base_model.get_layer('ensemble_embedding').output, name='embed') \n 559 | tf_model_new.summary()\n 560 | tf_model_new.save(\"/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/base/cam24_rmr_esm_embed\")\n 561 | # ### TC/YG ems, MLP and new var attention ensemble\n 562 | base_model_path = \"gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/IL_CAM24/tf_models/keep_path/20240926005650/best_model\"\n 563 | base_model = tf.keras.models.load_model(base_model_path)\n 564 | base_model.input\n 565 | def share_mlp_for3(shared_layers_length,activate_func,drop_out,kernel_initializer,regular=None,share_name='Shared_mlp'):\n 566 |     shared_mlp = tf.keras.models.Sequential(name=share_name)\n 567 |     for nb in shared_layers_length:\n 568 |         shared_mlp.add(Dense(nb,activation=activate_func, kernel_initializer=kernel_initializer, kernel_regularizer=regular))\n 569 |         shared_mlp.add(BatchNormalization())\n 570 |         shared_mlp.add(Dropout(rate = drop_out))\n 571 |     return shared_mlp\n 572 | def reduce_sum(input_tensor,\n 573 |                axis=None,\n 574 |                keep_dims=False,\n 575 |                name=None,\n 576 |                reduction_indices=None):\n 577 |     try:\n 578 |         return tf.reduce_sum(input_tensor,\n 579 |                              axis=axis,\n 580 |                              keep_dims=keep_dims,\n 581 |                              name=name,\n 582 |                              reduction_indices=reduction_indices)\n 583 |     except TypeError:\n 584 |         return tf.reduce_sum(input_tensor,\n 585 |                              axis=axis,\n 586 |                              keepdims=keep_dims,\n 587 |                              name=name)\n 588 | class AttentionLayer(tf.keras.layers.Layer):  \n 589 |     def __init__(self, d_k):  \n 590 |         super(AttentionLayer, self).__init__()  \n 591 |         self.d_k = d_k  \n 592 |     def call(self, inputs):  \n 593 |         # inputs shape: (batch_size, n_experts, d)  \n 594 |         batch_size, n_experts, d = inputs.shape[0], inputs.shape[1], inputs.shape[2]  \n 595 |         Q = tf.keras.layers.Dense(self.d_k)(inputs)  # Query  \n 596 |         K = tf.keras.layers.Dense(self.d_k)(inputs)  # Key  \n 597 |         V = tf.keras.layers.Dense(d)(inputs)         # Value (\u53ef\u4ee5\u4fdd\u6301\u503c\u7684\u7ef4\u5ea6\u4e3a d)  \n 598 |         attention_scores = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(float(self.d_k))  # \u5f62\u72b6: (batch_size, n_experts, n_experts)  \n 599 |         attention_weights = tf.nn.softmax(attention_scores, axis=-1)  # \u5f62\u72b6: (batch_size, n_experts, n_experts)  \n 600 |         attention_output = tf.matmul(attention_weights, V)  # \u5f62\u72b6: (batch_size, n_experts, d)  \n 601 |         return attention_output  \n 602 | def build_model(\n 603 |                base_model_path,\n 604 |                ensemble_model_path=None,\n 605 |                freeze_yg=True):\n 606 |     adp_tf_model_path = '/projects/gds-focus/data/makang/CAM24/source_code/adaptive/model/large_bottom_super/final_model/TagFineTune_best'\n 607 |     rob_tf_model_path = '/projects/gds-focus/data/makang/CAM24/source_code/robust/model/large_bottom_super/final_model/robust_large_bottom_super_mmoe_id_0_transfer_learning_10expert_2task_best'\n 608 |     adp_tf_model = tf.keras.models.load_model(adp_tf_model_path)\n 609 |     rob_tf_model = tf.keras.models.load_model(rob_tf_model_path)\n 610 |     adp_input = adp_tf_model.input\n 611 |     rob_input = rob_tf_model.input\n 612 |     adp_tf_model_new = tf.keras.models.Model(adp_input, adp_tf_model.get_layer('dropout_14').output, name='new_adp_part') \n 613 |     rob_tf_model_new = tf.keras.models.Model(rob_input, rob_tf_model.get_layer('dropout_14').output, name='new_rob_part') \n 614 |     inputs = [tf.keras.layers.Input(adp_tf_model_new.input.shape[1],name='adaptive_input'), \\\n 615 |               tf.keras.layers.Input(rob_tf_model_new.input.shape[1],name='robust_input')]\n 616 |     outputs = [adp_tf_model_new(inputs[0]), rob_tf_model_new(inputs[1])]\n 617 |     ensemble_model = tf.keras.models.Model(inputs, outputs, name='adp_rob_ems')\n 618 |     for layer in ensemble_model.layers:\n 619 |         layer.trainable = False\n 620 |         if (layer.name == 'new_adp_part' or layer.name == 'new_rob_part'):\n 621 |             for t in layer.layers:\n 622 |                 if (t.name == 'MMoE_Layer'):\n 623 |                     t.trainable = False\n 624 |     mid = ensemble_model(inputs)\n 625 |     inputs = ensemble_model.input\n 626 |     task_names = ['cls_tc', 'cls_yg']\n 627 |     output_activate = ['sigmoid', 'sigmoid']\n 628 |     # FM part\n 629 |     stack_tc = tf.stack(mid, axis=1)\n 630 |     square_of_sum_tc = tf.square(reduce_sum(\n 631 |             stack_tc, axis=1, keep_dims=True))\n 632 |     sum_of_square_tc = reduce_sum(\n 633 |             stack_tc * stack_tc, axis=1, keep_dims=True)\n 634 |     FM_out_tc = square_of_sum_tc - sum_of_square_tc\n 635 |     FM_out_tc = 0.5 * reduce_sum(FM_out_tc, axis=2, keep_dims=False)\n 636 |     concat_tc = tf.keras.layers.Concatenate(axis=-1,name='ensemble_embedding')(mid)\n 637 |     adp_yg_input = tf.keras.layers.Input(2000,name='adaptive_yg_input')\n 638 |     yg_mlp_emb = share_mlp_for3([512, 128], tf.nn.relu, drop_out=0.5, \n 639 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), regular=None, share_name='bottom_mlp_yg')(adp_yg_input)\n 640 |     num_experts = 7\n 641 |     new_var_inputs = []\n 642 |     expert_outputs = []\n 643 |     for expert_idx in range(num_experts):\n 644 |         new_var_input = tf.keras.layers.Input(shape=(500, ), name=f'new_var_expert_{expert_idx}')\n 645 |         new_var_inputs.append(new_var_input)\n 646 |         exp_output = share_mlp_for3([128, 64], \n 647 |                              tf.nn.relu, \n 648 |                              drop_out=0.5, \n 649 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), \n 650 |                              regular=None, \n 651 |                              share_name=f'expert_{expert_idx}')(new_var_input)\n 652 |         expert_outputs.append(exp_output)\n 653 |     qk_dim = 64\n 654 |     expert_outputs = tf.stack(expert_outputs, axis=1)\n 655 |     # att = AttentionLayer(d_k=64)(expert_outputs)\n 656 |     Q = tf.keras.layers.Dense(qk_dim)(expert_outputs)  # Query  \n 657 |     K = tf.keras.layers.Dense(qk_dim)(expert_outputs)  # Key  \n 658 |     V = tf.keras.layers.Dense(expert_outputs.shape[-1])(expert_outputs)    \n 659 |     attention_scores = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(float(qk_dim))  \n 660 |     attention_weights = tf.nn.softmax(attention_scores, axis=-1)   \n 661 |     attention_output = tf.matmul(attention_weights, V)\n 662 |     weighted_expert_output = tf.keras.layers.Flatten()(attention_output)\n 663 |     att_output = tf.keras.layers.Dense(units=128)(weighted_expert_output)\n 664 |     mid_yg = [mid[0], mid[1], yg_mlp_emb, att_output]\n 665 |     concat_yg = tf.keras.layers.Concatenate(axis=-1,name='concat_yg')(mid_yg)\n 666 |     # Linear part\n 667 |     linear_out_tc = tf.keras.layers.Dense(units=1,\n 668 |                                         name= 'linear_tc',\n 669 |                                         activation= None,\n 670 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(concat_tc)\n 671 |     # Linear part\n 672 |     linear_out_yg = tf.keras.layers.Dense(units=1,\n 673 |                                         name= 'linear_yg',\n 674 |                                         activation= None,\n 675 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(concat_yg)\n 676 |     # FM part\n 677 |     stack_yg = tf.stack(mid_yg, axis=1)\n 678 |     square_of_sum_yg = tf.square(reduce_sum(\n 679 |             stack_yg, axis=1, keep_dims=True))\n 680 |     sum_of_square_yg = reduce_sum(\n 681 |             stack_yg * stack_yg, axis=1, keep_dims=True)\n 682 |     FM_out_yg = square_of_sum_yg - sum_of_square_yg\n 683 |     FM_out_yg = 0.5 * reduce_sum(FM_out_yg, axis=2, keep_dims=False)\n 684 |     final_outputs = []\n 685 |     kernel_initializer = tf.keras.initializers.glorot_normal(seed=1024)\n 686 |     mlp_out_tc = share_mlp_for3([64, 16], tf.nn.relu, drop_out=0.5, \n 687 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), regular=None, share_name='mlp_tc')(concat_tc)\n 688 |     nn_out_tc = tf.keras.layers.Dense(units=1,\n 689 |                                         name= 'nn_pred_' + 'tc',\n 690 |                                         activation= None,\n 691 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(mlp_out_tc)\n 692 |     mlp_out_yg = share_mlp_for3([64, 16], tf.nn.relu, drop_out=0.5, \n 693 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), share_name='mlp_yg')(concat_yg)\n 694 |     nn_out_yg = tf.keras.layers.Dense(units=1,\n 695 |                                         name= 'nn_pred_' + 'yg',\n 696 |                                         activation= None,\n 697 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(mlp_out_yg)\n 698 |     task_logit_tc = Add(name='logit_' + 'tc')([nn_out_tc, linear_out_tc, FM_out_tc])\n 699 |     task_logit_yg = Add(name='logit_' + 'yg')([nn_out_yg, linear_out_yg, FM_out_yg])\n 700 |     final_outputs.append(tf.keras.layers.Activation('sigmoid',name='tc_cls')(task_logit_tc))\n 701 |     final_outputs.append(tf.keras.layers.Activation('sigmoid',name='yg_cls')(task_logit_yg))\n 702 |     model = tf.keras.Model([inputs[0], inputs[1], adp_yg_input]+new_var_inputs , \n 703 |                            outputs = final_outputs,\n 704 |                            name='CAM24_IL_DeepFM_YGFilter_4towers_new_var_attention')\n 705 |     best_model = tf.keras.models.load_model(base_model_path)\n 706 |     model.get_layer('adp_rob_ems').set_weights(best_model.get_layer('adp_rob_ems').get_weights())\n 707 |     model.get_layer('adp_rob_ems').trainable = False \n 708 |     model.get_layer('bottom_mlp_yg').set_weights(best_model.get_layer('bottom_mlp_yg').get_weights())\n 709 |     model.get_layer('bottom_mlp_yg').trainable = False  ## trainable=True or false\n 710 |     # model.get_layer('mlp_yg').set_weights(best_model.get_layer('mlp_yg').get_weights())\n 711 |     # model.get_layer('mlp_yg').trainable = False \n 712 |     # model.get_layer('nn_pred_yg').set_weights(best_model.get_layer('nn_pred_yg').get_weights())\n 713 |     # model.get_layer('nn_pred_yg').trainable = False \n 714 |     # model.get_layer('linear_yg').set_weights(best_model.get_layer('linear_yg').get_weights())\n 715 |     # model.get_layer('linear_yg').trainable = False \n 716 |     model.get_layer('mlp_tc').set_weights(best_model.get_layer('mlp_tc').get_weights())\n 717 |     model.get_layer('mlp_tc').trainable = False \n 718 |     model.get_layer('nn_pred_tc').set_weights(best_model.get_layer('nn_pred_tc').get_weights())\n 719 |     model.get_layer('nn_pred_tc').trainable = False \n 720 |     model.get_layer('linear_tc').set_weights(best_model.get_layer('linear_tc').get_weights())\n 721 |     model.get_layer('linear_tc').trainable = False \n 722 |     model.get_layer('mlp_yg').trainable = True \n 723 |     model.get_layer('linear_yg').trainable = True \n 724 |     model.get_layer('nn_pred_yg').trainable = True\n 725 |     model.summary()\n 726 |     model.save(ensemble_model_path)\n 727 | build_model(\"gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/IL_CAM24/tf_models/keep_path/20240912032747/best_model\",\n 728 |            ensemble_model_path=\"/projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/CAM24_IL_New_Var_add_attention_v3\"\n 729 |            )\n 730 | mid_yg[3].shape\n 731 | tf.keras.layers.Concatenate(axis=-1,name='concat_yg')(mid_yg)\n 732 | # ### TC/YG ems, MLP and new var gate ensemble\n 733 | def build_model(\n 734 |                base_model_path,\n 735 |                ensemble_model_path=None,\n 736 |                freeze_yg=True):\n 737 |     adp_tf_model_path = '/projects/gds-focus/data/makang/CAM24/source_code/adaptive/model/large_bottom_super/final_model/TagFineTune_best'\n 738 |     rob_tf_model_path = '/projects/gds-focus/data/makang/CAM24/source_code/robust/model/large_bottom_super/final_model/robust_large_bottom_super_mmoe_id_0_transfer_learning_10expert_2task_best'\n 739 |     adp_tf_model = tf.keras.models.load_model(adp_tf_model_path)\n 740 |     rob_tf_model = tf.keras.models.load_model(rob_tf_model_path)\n 741 |     adp_input = adp_tf_model.input\n 742 |     rob_input = rob_tf_model.input\n 743 |     adp_tf_model_new = tf.keras.models.Model(adp_input, adp_tf_model.get_layer('dropout_14').output, name='new_adp_part') \n 744 |     rob_tf_model_new = tf.keras.models.Model(rob_input, rob_tf_model.get_layer('dropout_14').output, name='new_rob_part') \n 745 |     inputs = [tf.keras.layers.Input(adp_tf_model_new.input.shape[1],name='adaptive_input'), \\\n 746 |               tf.keras.layers.Input(rob_tf_model_new.input.shape[1],name='robust_input')]\n 747 |     outputs = [adp_tf_model_new(inputs[0]), rob_tf_model_new(inputs[1])]\n 748 |     ensemble_model = tf.keras.models.Model(inputs, outputs, name='adp_rob_ems')\n 749 |     for layer in ensemble_model.layers:\n 750 |         layer.trainable = False\n 751 |         if (layer.name == 'new_adp_part' or layer.name == 'new_rob_part'):\n 752 |             for t in layer.layers:\n 753 |                 if (t.name == 'MMoE_Layer'):\n 754 |                     t.trainable = False\n 755 |     mid = ensemble_model(inputs)\n 756 |     inputs = ensemble_model.input\n 757 |     task_names = ['cls_tc', 'cls_yg']\n 758 |     output_activate = ['sigmoid', 'sigmoid']\n 759 |     # FM part\n 760 |     stack_tc = tf.stack(mid, axis=1)\n 761 |     square_of_sum_tc = tf.square(reduce_sum(\n 762 |             stack_tc, axis=1, keep_dims=True))\n 763 |     sum_of_square_tc = reduce_sum(\n 764 |             stack_tc * stack_tc, axis=1, keep_dims=True)\n 765 |     FM_out_tc = square_of_sum_tc - sum_of_square_tc\n 766 |     FM_out_tc = 0.5 * reduce_sum(FM_out_tc, axis=2, keep_dims=False)\n 767 |     concat_tc = tf.keras.layers.Concatenate(axis=-1,name='ensemble_embedding')(mid)\n 768 |     adp_yg_input = tf.keras.layers.Input(2000,name='adaptive_yg_input')\n 769 |     yg_mlp_emb = share_mlp_for3([512, 128], tf.nn.relu, drop_out=0.5, \n 770 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), regular=None, share_name='bottom_mlp_yg')(adp_yg_input)\n 771 |     num_experts = 7\n 772 |     new_var_inputs = []\n 773 |     expert_outputs = []\n 774 |     for expert_idx in range(num_experts):\n 775 |         new_var_input = tf.keras.layers.Input(shape=(500, ), name=f'new_var_expert_{expert_idx}')\n 776 |         new_var_inputs.append(new_var_input)\n 777 |         exp_output = share_mlp_for3([256, 128], \n 778 |                              tf.nn.relu, \n 779 |                              drop_out=0.5, \n 780 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), \n 781 |                              regular=None, \n 782 |                              share_name=f'expert_{expert_idx}')(new_var_input)\n 783 |         expert_outputs.append(exp_output)\n 784 |     expert_outputs = tf.stack(expert_outputs, axis=1)\n 785 |     adp_embed = tf.stack([mid[0]], axis=1)\n 786 |     expert_scores = tf.matmul(adp_embed, expert_outputs, transpose_b=True)  \n 787 |     gate_scores = tf.squeeze(tf.nn.sigmoid(expert_scores), axis=1)\n 788 |     selected_expert_indices = tf.argmax(gate_scores, axis=-1)\n 789 |     selected_expert_embeddings = tf.gather(expert_outputs, selected_expert_indices, batch_dims=0, axis=1)\n 790 |     mid_yg = [mid[0], mid[1], yg_mlp_emb, selected_expert_embeddings]\n 791 |     concat_yg = tf.keras.layers.Concatenate(axis=-1,name='concat_yg')(mid_yg)\n 792 |     # Linear part\n 793 |     linear_out_tc = tf.keras.layers.Dense(units=1,\n 794 |                                         name= 'linear_tc',\n 795 |                                         activation= None,\n 796 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(concat_tc)\n 797 |     # Linear part\n 798 |     linear_out_yg = tf.keras.layers.Dense(units=1,\n 799 |                                         name= 'linear_yg',\n 800 |                                         activation= None,\n 801 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(concat_yg)\n 802 |     # FM part\n 803 |     stack_yg = tf.stack(mid_yg, axis=1)\n 804 |     square_of_sum_yg = tf.square(reduce_sum(\n 805 |             stack_yg, axis=1, keep_dims=True))\n 806 |     sum_of_square_yg = reduce_sum(\n 807 |             stack_yg * stack_yg, axis=1, keep_dims=True)\n 808 |     FM_out_yg = square_of_sum_yg - sum_of_square_yg\n 809 |     FM_out_yg = 0.5 * reduce_sum(FM_out_yg, axis=2, keep_dims=False)\n 810 |     final_outputs = []\n 811 |     kernel_initializer = tf.keras.initializers.glorot_normal(seed=1024)\n 812 |     mlp_out_tc = share_mlp_for3([64, 16], tf.nn.relu, drop_out=0.5, \n 813 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), regular=None, share_name='mlp_tc')(concat_tc)\n 814 |     nn_out_tc = tf.keras.layers.Dense(units=1,\n 815 |                                         name= 'nn_pred_' + 'tc',\n 816 |                                         activation= None,\n 817 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(mlp_out_tc)\n 818 |     mlp_out_yg = share_mlp_for3([64, 16], tf.nn.relu, drop_out=0.5, \n 819 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), share_name='mlp_yg')(concat_yg)\n 820 |     nn_out_yg = tf.keras.layers.Dense(units=1,\n 821 |                                         name= 'nn_pred_' + 'yg',\n 822 |                                         activation= None,\n 823 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(mlp_out_yg)\n 824 |     task_logit_tc = Add(name='logit_' + 'tc')([nn_out_tc, linear_out_tc, FM_out_tc])\n 825 |     task_logit_yg = Add(name='logit_' + 'yg')([nn_out_yg, linear_out_yg, FM_out_yg])\n 826 |     final_outputs.append(tf.keras.layers.Activation('sigmoid',name='tc_cls')(task_logit_tc))\n 827 |     final_outputs.append(tf.keras.layers.Activation('sigmoid',name='yg_cls')(task_logit_yg))\n 828 |     model = tf.keras.Model([inputs[0], inputs[1], adp_yg_input]+new_var_inputs , \n 829 |                            outputs = final_outputs,\n 830 |                            name='CAM24_IL_DeepFM_YGFilter_4towers_new_var_attention')\n 831 |     best_model = tf.keras.models.load_model(base_model_path)\n 832 |     model.get_layer('adp_rob_ems').set_weights(best_model.get_layer('adp_rob_ems').get_weights())\n 833 |     model.get_layer('adp_rob_ems').trainable = False \n 834 |     model.get_layer('bottom_mlp_yg').set_weights(best_model.get_layer('bottom_mlp_yg').get_weights())\n 835 |     model.get_layer('bottom_mlp_yg').trainable = False  ## trainable=True or false\n 836 |     # model.get_layer('mlp_yg').set_weights(best_model.get_layer('mlp_yg').get_weights())\n 837 |     # model.get_layer('mlp_yg').trainable = False \n 838 |     # model.get_layer('nn_pred_yg').set_weights(best_model.get_layer('nn_pred_yg').get_weights())\n 839 |     # model.get_layer('nn_pred_yg').trainable = False \n 840 |     # model.get_layer('linear_yg').set_weights(best_model.get_layer('linear_yg').get_weights())\n 841 |     # model.get_layer('linear_yg').trainable = False \n 842 |     model.get_layer('mlp_tc').set_weights(best_model.get_layer('mlp_tc').get_weights())\n 843 |     model.get_layer('mlp_tc').trainable = False \n 844 |     model.get_layer('nn_pred_tc').set_weights(best_model.get_layer('nn_pred_tc').get_weights())\n 845 |     model.get_layer('nn_pred_tc').trainable = False \n 846 |     model.get_layer('linear_tc').set_weights(best_model.get_layer('linear_tc').get_weights())\n 847 |     model.get_layer('linear_tc').trainable = False \n 848 |     model.get_layer('mlp_yg').trainable = True \n 849 |     model.get_layer('linear_yg').trainable = True \n 850 |     model.get_layer('nn_pred_yg').trainable = True\n 851 |     model.summary()\n 852 |     model.save(ensemble_model_path)\n 853 | build_model(\"gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/IL_CAM24/tf_models/keep_path/20240912032747/best_model\",\n 854 |            ensemble_model_path=\"/projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/CAM24_IL_New_Var_add_gate_v3\"\n 855 |            )\n 856 | # ### TC/YG ems, MLP and new var gate max pooling ensemble\n 857 | def build_model(\n 858 |                base_model_path,\n 859 |                ensemble_model_path=None,\n 860 |                freeze_yg=True):\n 861 |     adp_tf_model_path = '/projects/gds-focus/data/makang/CAM24/source_code/adaptive/model/large_bottom_super/final_model/TagFineTune_best'\n 862 |     rob_tf_model_path = '/projects/gds-focus/data/makang/CAM24/source_code/robust/model/large_bottom_super/final_model/robust_large_bottom_super_mmoe_id_0_transfer_learning_10expert_2task_best'\n 863 |     adp_tf_model = tf.keras.models.load_model(adp_tf_model_path)\n 864 |     rob_tf_model = tf.keras.models.load_model(rob_tf_model_path)\n 865 |     adp_input = adp_tf_model.input\n 866 |     rob_input = rob_tf_model.input\n 867 |     adp_tf_model_new = tf.keras.models.Model(adp_input, adp_tf_model.get_layer('dropout_14').output, name='new_adp_part') \n 868 |     rob_tf_model_new = tf.keras.models.Model(rob_input, rob_tf_model.get_layer('dropout_14').output, name='new_rob_part') \n 869 |     inputs = [tf.keras.layers.Input(adp_tf_model_new.input.shape[1],name='adaptive_input'), \\\n 870 |               tf.keras.layers.Input(rob_tf_model_new.input.shape[1],name='robust_input')]\n 871 |     outputs = [adp_tf_model_new(inputs[0]), rob_tf_model_new(inputs[1])]\n 872 |     ensemble_model = tf.keras.models.Model(inputs, outputs, name='adp_rob_ems')\n 873 |     for layer in ensemble_model.layers:\n 874 |         layer.trainable = False\n 875 |         if (layer.name == 'new_adp_part' or layer.name == 'new_rob_part'):\n 876 |             for t in layer.layers:\n 877 |                 if (t.name == 'MMoE_Layer'):\n 878 |                     t.trainable = False\n 879 |     mid = ensemble_model(inputs)\n 880 |     inputs = ensemble_model.input\n 881 |     task_names = ['cls_tc', 'cls_yg']\n 882 |     output_activate = ['sigmoid', 'sigmoid']\n 883 |     # FM part\n 884 |     stack_tc = tf.stack(mid, axis=1)\n 885 |     square_of_sum_tc = tf.square(reduce_sum(\n 886 |             stack_tc, axis=1, keep_dims=True))\n 887 |     sum_of_square_tc = reduce_sum(\n 888 |             stack_tc * stack_tc, axis=1, keep_dims=True)\n 889 |     FM_out_tc = square_of_sum_tc - sum_of_square_tc\n 890 |     FM_out_tc = 0.5 * reduce_sum(FM_out_tc, axis=2, keep_dims=False)\n 891 |     concat_tc = tf.keras.layers.Concatenate(axis=-1,name='ensemble_embedding')(mid)\n 892 |     adp_yg_input = tf.keras.layers.Input(2000,name='adaptive_yg_input')\n 893 |     yg_mlp_emb = share_mlp_for3([512, 128], tf.nn.relu, drop_out=0.5, \n 894 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), regular=None, share_name='bottom_mlp_yg')(adp_yg_input)\n 895 |     num_experts = 7\n 896 |     new_var_inputs = []\n 897 |     expert_outputs = []\n 898 |     for expert_idx in range(num_experts):\n 899 |         new_var_input = tf.keras.layers.Input(shape=(500, ), name=f'new_var_expert_{expert_idx}')\n 900 |         new_var_inputs.append(new_var_input)\n 901 |         exp_output = share_mlp_for3([256, 128], \n 902 |                              tf.nn.relu, \n 903 |                              drop_out=0.5, \n 904 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), \n 905 |                              regular=None, \n 906 |                              share_name=f'expert_{expert_idx}')(new_var_input)\n 907 |         expert_outputs.append(exp_output)\n 908 |     expert_outputs = tf.stack(expert_outputs, axis=1)\n 909 |     max_pooled_embedding = tf.reduce_max(expert_outputs, axis=1)\n 910 |     mid_yg = [mid[0], mid[1], yg_mlp_emb, max_pooled_embedding]\n 911 |     concat_yg = tf.keras.layers.Concatenate(axis=-1,name='concat_yg')(mid_yg)\n 912 |     # Linear part\n 913 |     linear_out_tc = tf.keras.layers.Dense(units=1,\n 914 |                                         name= 'linear_tc',\n 915 |                                         activation= None,\n 916 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(concat_tc)\n 917 |     # Linear part\n 918 |     linear_out_yg = tf.keras.layers.Dense(units=1,\n 919 |                                         name= 'linear_yg',\n 920 |                                         activation= None,\n 921 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(concat_yg)\n 922 |     # FM part\n 923 |     stack_yg = tf.stack(mid_yg, axis=1)\n 924 |     square_of_sum_yg = tf.square(reduce_sum(\n 925 |             stack_yg, axis=1, keep_dims=True))\n 926 |     sum_of_square_yg = reduce_sum(\n 927 |             stack_yg * stack_yg, axis=1, keep_dims=True)\n 928 |     FM_out_yg = square_of_sum_yg - sum_of_square_yg\n 929 |     FM_out_yg = 0.5 * reduce_sum(FM_out_yg, axis=2, keep_dims=False)\n 930 |     final_outputs = []\n 931 |     kernel_initializer = tf.keras.initializers.glorot_normal(seed=1024)\n 932 |     mlp_out_tc = share_mlp_for3([64, 16], tf.nn.relu, drop_out=0.5, \n 933 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), regular=None, share_name='mlp_tc')(concat_tc)\n 934 |     nn_out_tc = tf.keras.layers.Dense(units=1,\n 935 |                                         name= 'nn_pred_' + 'tc',\n 936 |                                         activation= None,\n 937 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(mlp_out_tc)\n 938 |     mlp_out_yg = share_mlp_for3([64, 16], tf.nn.relu, drop_out=0.5, \n 939 |                              kernel_initializer=tf.keras.initializers.glorot_normal(seed=1024), share_name='mlp_yg')(concat_yg)\n 940 |     nn_out_yg = tf.keras.layers.Dense(units=1,\n 941 |                                         name= 'nn_pred_' + 'yg',\n 942 |                                         activation= None,\n 943 |                                         kernel_initializer=tf.keras.initializers.VarianceScaling())(mlp_out_yg)\n 944 |     task_logit_tc = Add(name='logit_' + 'tc')([nn_out_tc, linear_out_tc, FM_out_tc])\n 945 |     task_logit_yg = Add(name='logit_' + 'yg')([nn_out_yg, linear_out_yg, FM_out_yg])\n 946 |     final_outputs.append(tf.keras.layers.Activation('sigmoid',name='tc_cls')(task_logit_tc))\n 947 |     final_outputs.append(tf.keras.layers.Activation('sigmoid',name='yg_cls')(task_logit_yg))\n 948 |     model = tf.keras.Model([inputs[0], inputs[1], adp_yg_input]+new_var_inputs , \n 949 |                            outputs = final_outputs,\n 950 |                            name='CAM24_IL_DeepFM_YGFilter_4towers_new_var_attention')\n 951 |     best_model = tf.keras.models.load_model(base_model_path)\n 952 |     model.get_layer('adp_rob_ems').set_weights(best_model.get_layer('adp_rob_ems').get_weights())\n 953 |     model.get_layer('adp_rob_ems').trainable = False \n 954 |     model.get_layer('bottom_mlp_yg').set_weights(best_model.get_layer('bottom_mlp_yg').get_weights())\n 955 |     model.get_layer('bottom_mlp_yg').trainable = False  ## trainable=True or false\n 956 |     # model.get_layer('mlp_yg').set_weights(best_model.get_layer('mlp_yg').get_weights())\n 957 |     # model.get_layer('mlp_yg').trainable = False \n 958 |     # model.get_layer('nn_pred_yg').set_weights(best_model.get_layer('nn_pred_yg').get_weights())\n 959 |     # model.get_layer('nn_pred_yg').trainable = False \n 960 |     # model.get_layer('linear_yg').set_weights(best_model.get_layer('linear_yg').get_weights())\n 961 |     # model.get_layer('linear_yg').trainable = False \n 962 |     model.get_layer('mlp_tc').set_weights(best_model.get_layer('mlp_tc').get_weights())\n 963 |     model.get_layer('mlp_tc').trainable = False \n 964 |     model.get_layer('nn_pred_tc').set_weights(best_model.get_layer('nn_pred_tc').get_weights())\n 965 |     model.get_layer('nn_pred_tc').trainable = False \n 966 |     model.get_layer('linear_tc').set_weights(best_model.get_layer('linear_tc').get_weights())\n 967 |     model.get_layer('linear_tc').trainable = False \n 968 |     model.get_layer('mlp_yg').trainable = True \n 969 |     model.get_layer('linear_yg').trainable = True \n 970 |     model.get_layer('nn_pred_yg').trainable = True\n 971 |     model.summary()\n 972 |     model.save(ensemble_model_path)\n 973 | build_model(\"gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/IL_CAM24/tf_models/keep_path/20240912032747/best_model\",\n 974 |            ensemble_model_path=\"/projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/CAM24_IL_New_Var_add_gate_v2\"\n 975 |            )",
    "rmr_agent/repos/CAM_variable_research/3_2_gcp_training.py": "   1 | # # import prepare\n   2 | # %ppauth\n   3 | import tensorflow as tf\n   4 | import sys,os\n   5 | import aml.cloud_v1 as cloud\n   6 | client = cloud.DataProcClient(gcp_project='ccg24-hrzana-gds-focus')\n   7 | from aml import cloud_v1 as cloud\n   8 | from sklearn.model_selection import train_test_split\n   9 | import pandas as pd\n  10 | import json\n  11 | import copy\n  12 | util_path = '/projects/cchen16/FlagshipModelResearch/01_phase2_research/il_utils'\n  13 | sys.path.append(util_path)\n  14 | sys.path.append('/projects/gds-focus/data/catch/IL/CAM_variable/assets')\n  15 | from helper import *\n  16 | tf.__version__\n  17 | # !nvidia-smi\n  18 | # # exp trainer define\n  19 | def exp_trainer_all(model_config):\n  20 |     import sys\n  21 |     import logging\n  22 |     import sys   \n  23 |     import tensorflow as tf\n  24 |     from tensorflow import keras\n  25 |     from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n  26 |     from tensorflow.keras import backend as K\n  27 |     # from junrui_utils import global_data_prepare_for_ensemble as data_prepare    \n  28 |     from sf_il_utils import global_data_prepare_for_ensemble as data_prepare\n  29 |     from sf_il_utils import custom_loss, binary_focal_loss\n  30 |     K.set_floatx('float32')\n  31 |     K.set_epsilon(1e-16)\n  32 |     logging.basicConfig(stream=sys.stdout, level='INFO')\n  33 |     logger = logging.getLogger('Train')\n  34 |     strategy = tf.distribute.MultiWorkerMirroredStrategy()\n  35 |     from tensorflow.keras import metrics\n  36 |     with strategy.scope():\n  37 |         model_seg =tf.keras.models.load_model(model_config['previous_model_path'])\n  38 |         model_seg.summary()\n  39 |         # increasing lr schedule\n  40 |         custom_objects = {\"custom_loss\": custom_loss,\n  41 |                           \"focal_loss\": binary_focal_loss(gamma=2., alpha=.25),\n  42 |                           \"binary_crossentropy\":\"binary_crossentropy\"}\n  43 |         with keras.utils.custom_object_scope(custom_objects):\n  44 |             model_seg.compile(tf.keras.optimizers.Adam(model_config['lr']),\n  45 |                           loss=[custom_objects[f] for f in model_config['loss_functions']],\n  46 |                           loss_weights=model_config['loss_list'],\n  47 |                           metrics=[\n  48 |                                    metrics.Recall(thresholds=0.1,name='recall'), \n  49 |                                    metrics.Precision(thresholds=0.1,name='precision')\n  50 |                                   ],\n  51 |                          )\n  52 |         prepare = data_prepare()\n  53 |         dev_fl = tf.io.gfile.glob(model_config['dev_files'])\n  54 |         val_fl = tf.io.gfile.glob(model_config['val_files'])\n  55 |         dev_dataset = prepare.make_dataset(filenames = dev_fl, \n  56 |                                             feature_list = model_config['candidate_vars'], \n  57 |                                             target_list = model_config['target_list'], \n  58 |                                             weight_list = model_config['weight_list'], \n  59 |                                             batch_size = model_config['global_batch_size'], \n  60 |                                             buffer_size = model_config['buffer_size'],\n  61 |                                             multi_task = model_config['multi_task'], \n  62 |                                             compress = 'GZIP',\n  63 |                                           )    \n  64 |         val_dataset = prepare.make_dataset(filenames = val_fl, \n  65 |                                             feature_list = model_config['candidate_vars'], \n  66 |                                             target_list = model_config['target_list'], \n  67 |                                             weight_list = model_config['weight_list'], \n  68 |                                             batch_size = model_config['global_batch_size'], \n  69 |                                             buffer_size = model_config['buffer_size'],\n  70 |                                             multi_task = model_config['multi_task'],\n  71 |                                             compress = 'GZIP',\n  72 |                                           )\n  73 |         callbacks_list = [\n  74 |         EarlyStopping(monitor='val_precision', patience=model_config['patience'], verbose=2, mode='max',restore_best_weights=True),\n  75 |         ModelCheckpoint(filepath=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'model.{epoch:02d}-{loss:.8f}', \\\n  76 |                         monitor='val_precision',mode='max',save_freq='epoch',save_best_only=False),\n  77 |         ModelCheckpoint(filepath=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'week-'+str(model_config['week_id'])+'-best_model', \\\n  78 |                         monitor='val_precision',mode='max',save_freq='epoch',save_best_only=True),\n  79 |         TensorBoard(log_dir=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'week-'+str(model_config['week_id'])+'-tensorboard', \\\n  80 |                            histogram_freq=1,batch_size=model_config['global_batch_size'],write_graph=False,\n  81 |                            write_grads=False,write_images=False,update_freq='batch')\n  82 |         ]\n  83 |         dev_dataset = strategy.experimental_distribute_dataset(dev_dataset)\n  84 |         val_dataset = strategy.experimental_distribute_dataset(val_dataset)\n  85 |     model_seg.fit(dev_dataset,\n  86 |                   epochs=model_config['train_epochs'],#model_config['train_epochs'],\n  87 |                   verbose=2,\n  88 |                   shuffle=True,\n  89 |                   validation_data=val_dataset,\n  90 |                   steps_per_epoch=model_config['train_epoch_steps'],  # 10000,  total dev records/num_works/batch size\n  91 |                   validation_steps=model_config['val_epoch_steps'],\n  92 |                   callbacks=callbacks_list)\n  93 |     return\n  94 | gfs_gcs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-gfs\"\n  95 | focus_gcs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-focus\" \n  96 | num_workers = 4\n  97 | batch_size = 512\n  98 | sample_size = 9000000\n  99 | target_list = ['driver_is_brm3_4_11_15_bad','driver_is_brm3_4_11_15_bad']\n 100 | weight_list = ['driver_train_wgt_3','driver_train_wgt_3']\n 101 | global_batch_size = batch_size*num_workers\n 102 | train_test_ratio = 0.8\n 103 | train_epochs = 30\n 104 | import pandas as pd\n 105 | candidate_vars = [list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 106 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/robust_tf_input.txt',sep='|').columns),\n 107 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 108 |                   # read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt\")\n 109 |                  ]\n 110 | candidate_vars[0] = [item+\"_adp\" for item in candidate_vars[0]]\n 111 | candidate_vars[1] = [item+\"_rob\" for item in candidate_vars[1]]\n 112 | candidate_vars[2] = [item+\"_adp\" for item in candidate_vars[2]]\n 113 | # candidate_vars[3] = [item for item in candidate_vars[3]]\n 114 | model_config = {\n 115 |     'candidate_vars': candidate_vars,\n 116 |     'target_list': target_list,\n 117 |     'weight_list': weight_list,\n 118 |     'loss_list':[0.0, 1.0],\n 119 |     'dev_files': None,   \n 120 |     'val_files': None,\n 121 |     'global_batch_size': global_batch_size,\n 122 |     'buffer_size': 1,\n 123 |     'train_epoch_steps': int(sample_size*train_test_ratio/(batch_size*num_workers)), ## \u9700\u8981\u8ba1\u7b97\u539f\u59cb\u6570\u636e\u6570\u91cf n\uff0ctrain\u2014\u2014epoch-steps=n/global_batch_size/num_workers  n \u6309\u71671600\u4e07\u6765\u8ba1\u7b97\n 124 |     'val_epoch_steps': int(sample_size*(1-train_test_ratio)/(batch_size*num_workers)), ##\u540c\u4e0a\n 125 |     # 'train_epoch_steps': 200,\n 126 |     # 'val_epoch_steps': 100,\n 127 |     'train_epochs': train_epochs,\n 128 |     'lr':1e-3,\n 129 |     'loss_functions': ['binary_crossentropy', 'binary_crossentropy'],\n 130 |     'week_id': 1,\n 131 |     'job_id': None,\n 132 |     'patience': 10,\n 133 |     'multi_task': len(target_list) > 1,\n 134 |     # 'nn_shape': [2000, [[512],[128],[64]]],\n 135 |     # 'keep_model_path': '/projects/cchen16/FlagshipModelResearch/01_phase2_research/tf_model/test',\n 136 |     # 'week_id':'all',\n 137 |     # 'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/data/TF/segment_feature_v1/',\n 138 | }\n 139 | working_path = '/projects/gds-focus/data/catch/IL/CAM_variable/training_configs'\n 140 | num_workers = 4\n 141 | batch_size = 256\n 142 | sample_size = 800000\n 143 | int(sample_size*train_test_ratio/(batch_size*num_workers)),int(sample_size*(1-train_test_ratio)/(batch_size*num_workers))\n 144 | global_batch_size \n 145 | # ## Specific config\n 146 | # !ls -tl /projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/\n 147 | seg_names = ['cam24_il_Tot',\n 148 |  'cam24_il_Young',\n 149 |  'cam24_il_Guest',\n 150 |  'cam24_il_Top',\n 151 |  'cam24_il_Casual',\n 152 |  'cam24_il_BrandedXO',\n 153 |  'cam24_il_BrandedRecurring',\n 154 |  'cam24_il_P2PFF',\n 155 |  'cam24_il_P2PGS',\n 156 |  'cam24_il_EbayMor',\n 157 |  'cam24_il_Other',\n 158 |  'cam24_il_MidCAM22Score']\n 159 | for i, name in enumerate(seg_names):\n 160 | def get_candidate_vars(var_path, suffix):\n 161 |     candidate_ls = read_line_file(var_path)\n 162 |     return [c+suffix for c in candidate_ls]\n 163 | se_files = {\n 164 |     'all_seg':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt',\"\"),\n 165 |     'recur':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedRecurring/columns/candidates_top200.txt',\"_seg6\"),\n 166 |     'xo':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedXO/columns/candidates_top200.txt',\"_seg5\"),\n 167 |     'other':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Other/columns/candidates_top200.txt',\"_seg10\"),\n 168 |     'p2pff':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PFF/columns/candidates_top200.txt',\"_seg7\"),\n 169 |     'p2pgs':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PGS/columns/candidates_top200.txt',\"_seg8\"),\n 170 |     'ebaymor':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_EbayMor/columns/candidates_top200.txt',\"_seg9\"),\n 171 | }\n 172 | 1800/4,450/4\n 173 | gfs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-gfs\"\n 174 | focus_bucket = \"gs://pypl-bkt-rsh-row-std-gds-focus\"\n 175 | specific_config = {\n 176 |     \"exp1_1\": {\n 177 |         'week_id':'all',\n 178 |         'num_workers':12,\n 179 |         'train_epoch_steps': 880,\n 180 |         'val_epoch_steps': 220,\n 181 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v1\",\n 182 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data/',\n 183 |     },\n 184 |     \"exp1_2\": {\n 185 |         'week_id':'all',\n 186 |         'num_workers': 4,\n 187 |         'train_epoch_steps': 1250,\n 188 |         'val_epoch_steps': 350,\n 189 |         'candidate_vars': candidate_vars + [se_files['all_seg']],\n 190 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v2\",\n 191 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_all_seg_tfrecords/',\n 192 |     },\n 193 |     \"exp1_3\": {\n 194 |         'week_id':'all',\n 195 |         'num_workers': 4,\n 196 |         'train_epoch_steps': 450,\n 197 |         'val_epoch_steps': 100,\n 198 |         'candidate_vars': candidate_vars + [se_files['recur']],\n 199 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v3\",\n 200 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_recur_tfrecords/',\n 201 |     },\n 202 |      \"exp1_4\": {\n 203 |         'week_id':'all',\n 204 |         'num_workers': 4,\n 205 |         'train_epoch_steps': 450,\n 206 |         'val_epoch_steps': 100,\n 207 |          'candidate_vars': candidate_vars + [se_files['xo']],\n 208 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v3\",\n 209 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_xo_tfrecords/',\n 210 |     },\n 211 |      \"exp1_5\": {\n 212 |         'week_id':'all',\n 213 |         'num_workers': 4,\n 214 |         'train_epoch_steps': 600,\n 215 |         'val_epoch_steps': 150,\n 216 |          'candidate_vars': candidate_vars + [se_files['p2pff']],\n 217 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v3\",\n 218 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_p2pff_tfrecords/',\n 219 |     },\n 220 |     \"exp1_6\": {\n 221 |         'week_id':'all',\n 222 |         'num_workers': 4,\n 223 |         'global_'\n 224 |         'train_epoch_steps': 70,\n 225 |         'val_epoch_steps': 25,\n 226 |         'candidate_vars': candidate_vars + [se_files['p2pgs']],\n 227 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v3\",\n 228 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_p2pgs_tfrecords/',\n 229 |     },\n 230 |     \"exp1_7\": {\n 231 |         'week_id':'all',\n 232 |         'num_workers': 4,\n 233 |         'train_epoch_steps': 70,\n 234 |         'val_epoch_steps': 25,\n 235 |         'candidate_vars': candidate_vars + [se_files['ebaymor']],\n 236 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v3\",\n 237 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_ebaymor_tfrecords/',\n 238 |     },\n 239 |     \"exp1_8\": {\n 240 |         'week_id':'all',\n 241 |         'num_workers': 4,\n 242 |         'train_epoch_steps': 600,\n 243 |         'val_epoch_steps': 150,\n 244 |         'candidate_vars': candidate_vars + [se_files['other']],\n 245 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v3\",\n 246 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_other_tfrecords/',\n 247 |     },\n 248 |      \"exp2_1\": {\n 249 |         'week_id':'all',\n 250 |         'num_workers':4,\n 251 |          'num_workers': 4,\n 252 |         'global_batch_size': 2048,\n 253 |         'train_epoch_steps': 1250,\n 254 |         'val_epoch_steps': 375,\n 255 |          'candidate_vars': candidate_vars + [se_files['all_seg']],\n 256 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v1\",\n 257 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data/',\n 258 |     },\n 259 |     \"exp2_2\": {\n 260 |         'week_id':'all',\n 261 |         'num_workers': 4,\n 262 |         'global_batch_size': 2048,\n 263 |         'train_epoch_steps': 2500,\n 264 |         'val_epoch_steps': 800,\n 265 |         'candidate_vars': candidate_vars + [se_files['all_seg']],\n 266 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v2\",\n 267 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_all_seg_tfrecords/',\n 268 |     },\n 269 |     \"exp2_3\": {\n 270 |         'week_id':'all',\n 271 |         'num_workers': 4,\n 272 |         'global_batch_size': 2048,\n 273 |         'train_epoch_steps': 1800,\n 274 |         'val_epoch_steps': 450,\n 275 |         'candidate_vars': candidate_vars + [se_files['recur']],\n 276 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v3\",\n 277 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_recur_tfrecords/',\n 278 |     },\n 279 |      \"exp2_4\": {\n 280 |         'week_id':'all',\n 281 |         'num_workers': 4,\n 282 |         'global_batch_size': 2048,\n 283 |         'train_epoch_steps': 900,\n 284 |         'val_epoch_steps': 225,\n 285 |          'candidate_vars': candidate_vars + [se_files['xo']],\n 286 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v3\",\n 287 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_xo_tfrecords/',\n 288 |     },\n 289 |      \"exp2_5\": {\n 290 |         'week_id':'all',\n 291 |         'num_workers': 4,\n 292 |          'global_batch_size': 2048,\n 293 |         'train_epoch_steps': 300,\n 294 |         'val_epoch_steps': 75,\n 295 |          'candidate_vars': candidate_vars + [se_files['p2pff']],\n 296 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v3\",\n 297 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_p2pff_tfrecords/',\n 298 |     },\n 299 |     \"exp2_6\": {\n 300 |         'week_id':'all',\n 301 |         'num_workers': 4,\n 302 |         'global_batch_size': 2048,\n 303 |         'train_epoch_steps': 150,\n 304 |         'val_epoch_steps': 50,\n 305 |         'candidate_vars': candidate_vars + [se_files['p2pgs']],\n 306 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v3\",\n 307 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_p2pgs_tfrecords/',\n 308 |     },\n 309 |     \"exp2_7\": {\n 310 |         'week_id':'all',\n 311 |         'num_workers': 4,\n 312 |         'global_batch_size': 2048,\n 313 |         'train_epoch_steps': 100,\n 314 |         'val_epoch_steps': 50,\n 315 |         'candidate_vars': candidate_vars + [se_files['ebaymor']],\n 316 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v3\",\n 317 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_ebaymor_tfrecords/',\n 318 |     },\n 319 |     \"exp2_8\": {\n 320 |         'week_id':'all',\n 321 |         'num_workers': 4,\n 322 |         'global_batch_size': 2048,\n 323 |         'train_epoch_steps': 300,\n 324 |         'val_epoch_steps': 75,\n 325 |         'candidate_vars': candidate_vars + [se_files['other']],\n 326 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v3\",\n 327 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_other_tfrecords/',\n 328 |     },\n 329 | }\n 330 | # ### Local debug\n 331 | import logging\n 332 | logging.basicConfig(stream=sys.stdout, level='INFO')\n 333 | logger = logging.getLogger('Train')\n 334 | strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n 335 | exp_trainer_all(tmp_config)\n 336 | working_path\n 337 | md = \"1120\"\n 338 | exp_idx = 'exp1'\n 339 | t =os.path.join(working_path,f'{exp_idx}_job_ids_{md}.json')\n 340 | if os.path.exists(t):\n 341 |     with open(t, 'r') as f:\n 342 |         job_ids = json.load(f)\n 343 | else:\n 344 |     job_ids = {}\n 345 | run_ls = ['exp2_1','exp2_2','exp2_3','exp2_4','exp2_5','exp2_6','exp2_7','exp2_8',]\n 346 | for exp_id in run_ls:\n 347 |     week_id = specific_config[exp_id][\"week_id\"]\n 348 |     tmp_config = copy.deepcopy(model_config)  \n 349 |     tmp_config['week_id'] = str(week_id)\n 350 |     for k, v in specific_config[exp_id].items():\n 351 |         if k not in {\"week_id\", \"data_path\"}:\n 352 |             tmp_config[k] = v\n 353 |     data = specific_config[exp_id][\"data_path\"]\n 354 |     tmp_config['previous_model_path'] = specific_config[exp_id][\"previous_model_path\"]\n 355 |     all_files = tf.io.gfile.glob(data+'*.gz')\n 356 |     dev_files, val_files = train_test_split(all_files, test_size=0.2, shuffle=True)\n 357 |     tmp_config['dev_files']=dev_files\n 358 |     tmp_config['val_files']=val_files\n 359 |     tmp_config['keep_model_path'] = f'{focus_bucket}/user/cchen16/IL_variable/{md}_{exp_idx}/{exp_id}_gcp_loss/week_'\n 360 |     # exp_trainer_all(tmp_config)\n 361 |     job_id = client.create_job(\n 362 |                 func = exp_trainer_all,\n 363 |                 pyfiles_to_import=[\n 364 |                                     '/projects/gds-focus/data/catch/IL/CAM_variable/assets/sf_il_utils.py',\n 365 |                                     '/projects/cchen16/FlagshipModelResearch/01_phase2_research/il_utils/cc_functions.py',\n 366 |                                   ], \n 367 |                 num_workers = tmp_config['num_workers'],\n 368 |                 machine_type_per_worker = \"n1-highmem-16\",\n 369 |                 # machine_type_per_worker = \"n2d-highmem-8\",\n 370 |                 model_config = tmp_config\n 371 |                 )\n 372 |     model_name = f'{exp_id}_week{week_id}'\n 373 |     job_ids[model_name] = job_id\n 374 |     tmp_config['job_id'] = job_id\n 375 |     with open(os.path.join(working_path,f'{model_name}_{md}.json'),'w') as f:\n 376 |         json.dump(tmp_config,f)\n 377 | with open(t,'w') as f:\n 378 |     json.dump(job_ids, f)\n 379 | # client.cancel_job('maglev/cloud/dataproc/job/gds-focus-3d570d/2aa7536c-25bb-35f9-9590-bb70afaa76fd')\n 380 | # ## Tracking\n 381 | def pretty_model_config(configs):\n 382 |     for k, v in configs.items():\n 383 |         if k not in {\"candidate_vars\", \"dev_files\", \"val_files\"}:\n 384 |         if k in {\"dev_files\"}:\n 385 |         if k in {\"candidate_vars\"}:\n 386 |             candidate_len = []\n 387 |             for ls in v:\n 388 |                 candidate_len.append(len(ls))\n 389 |     return\n 390 | def check_job_state(job_ids):\n 391 |     for k,v in job_ids.items():\n 392 | def cancel_job(job_ids):\n 393 |     for k,v in job_ids.items():\n 394 | week_id = 'all'\n 395 | model_name = f'{exp_id}_week{week_id}'\n 396 | config_path = os.path.join(working_path,f'{model_name}_{md}.json')\n 397 | with open(os.path.join(working_path,f'{model_name}_{md}.json'),'r') as f:\n 398 |     config = json.load(f)\n 399 | pretty_model_config(config)\n 400 | config_path\n 401 | check_job_state(job_ids)\n 402 | client.wait_job_for_completion(job_ids['exp2_3_weekall'],\n 403 |                                time_to_sleep=300)\n 404 | client.get_job_driver_logs(job_ids['exp1_7_weekall'])\n 405 | # client.get_job_application_logs(job_ids['exp1_2_weekall'])\n 406 | # !gsutil -m cp -r gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/1120_exp1/exp1_1_gcp_loss /projects/cchen16/FlagshipModelResearch/01_phase2_research/tensorboard/\n 407 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/0930_exp1/*/week_all\n 408 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/0924_exp1/exp1_2_gcp_loss/week_all/\n 409 | # ## Check config and log\n 410 | exp_id = 'exp1_1'\n 411 | week_id = 'all'\n 412 | model_name = f'{exp_id}_week{week_id}'\n 413 | config_path = os.path.join(working_path,f'{model_name}_{md}.json')\n 414 | with open(os.path.join(working_path,f'{model_name}_{md}.json'),'r') as f:\n 415 |     config = json.load(f)\n 416 | pretty_model_config(config)\n 417 | set(se_files['all_seg'])-set(config['candidate_vars'][3])\n 418 | client.get_job_application_logs(config['job_id'])\n 419 | 1024*1800",
    "rmr_agent/repos/CAM_variable_research/3_3_gcp_training.py": "   1 | # # import prepare\n   2 | # %ppauth\n   3 | import tensorflow as tf\n   4 | import sys,os\n   5 | import aml.cloud_v1 as cloud\n   6 | import tensorboard\n   7 | client = cloud.DataProcClient(gcp_project='ccg24-hrzana-gds-focus')\n   8 | from model_automation.gcp import dataproc_config\n   9 | from aml import cloud_v1 as cloud\n  10 | from tqdm import tqdm\n  11 | from sklearn.model_selection import train_test_split\n  12 | import pandas as pd\n  13 | import json\n  14 | import copy\n  15 | from model_automation.utils.rmr.helper import run_cmd\n  16 | util_path = '/projects/cchen16/FlagshipModelResearch/01_phase2_research/il_utils'\n  17 | sys.path.append(util_path)\n  18 | sys.path.append('/projects/gds-focus/data/catch/IL/CAM_variable/assets')\n  19 | from helper import *\n  20 | from sf_il_utils import custom_loss, binary_focal_loss\n  21 | import importlib\n  22 | from sf_il_utils import global_data_prepare_for_ensemble as data_prepare\n  23 | tf.__version__\n  24 | # # exp trainer define\n  25 | def exp_trainer_all(model_config):\n  26 |     import sys\n  27 |     import logging\n  28 |     import random    \n  29 |     import os\n  30 |     import sys   \n  31 |     import numpy as np\n  32 |     import tensorflow as tf\n  33 |     from tensorflow import keras\n  34 |     from tensorflow.keras.models import Sequential\n  35 |     from tensorflow.keras.layers import Dense, Dropout\n  36 |     from tensorflow.keras import optimizers\n  37 |     from tensorflow.python.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, TensorBoard\n  38 |     from tensorflow.keras import backend as K, Sequential, Model, layers\n  39 |     from tensorflow.keras.utils import get_custom_objects\n  40 |     # from junrui_utils import global_data_prepare_for_ensemble as data_prepare    \n  41 |     from sf_il_utils import global_data_prepare_for_ensemble as data_prepare\n  42 |     from sf_il_utils import custom_loss, binary_focal_loss\n  43 |     from cc_functions import create_single_pepnet, create_mlp_model, create_cascade_model, create_cascade_model_v2\n  44 |     K.set_floatx('float32')\n  45 |     K.set_epsilon(1e-16)\n  46 |     logging.basicConfig(stream=sys.stdout, level='INFO')\n  47 |     logger = logging.getLogger('Train')\n  48 |     strategy = tf.distribute.MultiWorkerMirroredStrategy()\n  49 |     from tensorflow.keras import metrics\n  50 |     from sklearn.model_selection import train_test_split\n  51 |     with strategy.scope():\n  52 |         model_seg =tf.keras.models.load_model(model_config['previous_model_path'])\n  53 |         model_seg.summary()\n  54 |         # increasing lr schedule\n  55 |         custom_objects = {\"custom_loss\": custom_loss,\n  56 |                           \"focal_loss\": binary_focal_loss(gamma=2., alpha=.25),\n  57 |                           \"binary_crossentropy\":\"binary_crossentropy\"}\n  58 |         with keras.utils.custom_object_scope(custom_objects):\n  59 |             model_seg.compile(tf.keras.optimizers.Adam(model_config['lr']),\n  60 |                           loss=[custom_objects[f] for f in model_config['loss_functions']],\n  61 |                           loss_weights=model_config['loss_list'],\n  62 |                           metrics=[\n  63 |                                    metrics.Recall(thresholds=0.1,name='recall'), \n  64 |                                    metrics.Precision(thresholds=0.1,name='precision')\n  65 |                                   ],\n  66 |                          )\n  67 |         prepare = data_prepare()\n  68 |         dev_fl = tf.io.gfile.glob(model_config['dev_files'])\n  69 |         val_fl = tf.io.gfile.glob(model_config['val_files'])\n  70 |         dev_dataset = prepare.make_dataset(filenames = dev_fl, \n  71 |                                             feature_list = model_config['candidate_vars'], \n  72 |                                             target_list = model_config['target_list'], \n  73 |                                             weight_list = model_config['weight_list'], \n  74 |                                             batch_size = model_config['global_batch_size'], \n  75 |                                             buffer_size = model_config['buffer_size'],\n  76 |                                             multi_task = model_config['multi_task'], \n  77 |                                             compress = 'GZIP',\n  78 |                                           )    \n  79 |         val_dataset = prepare.make_dataset(filenames = val_fl, \n  80 |                                             feature_list = model_config['candidate_vars'], \n  81 |                                             target_list = model_config['target_list'], \n  82 |                                             weight_list = model_config['weight_list'], \n  83 |                                             batch_size = model_config['global_batch_size'], \n  84 |                                             buffer_size = model_config['buffer_size'],\n  85 |                                             multi_task = model_config['multi_task'],\n  86 |                                             compress = 'GZIP',\n  87 |                                           )\n  88 |         callbacks_list = [\n  89 |         EarlyStopping(monitor='val_loss', patience=model_config['patience'], verbose=2, mode='min',restore_best_weights=True),\n  90 |         ModelCheckpoint(filepath=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'model.{epoch:02d}-{loss:.8f}', \\\n  91 |                         monitor='val_loss',mode='min',save_freq='epoch',save_best_only=False),\n  92 |         ModelCheckpoint(filepath=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'week-'+str(model_config['week_id'])+'-best_model', \\\n  93 |                         monitor='val_loss',mode='min',save_freq='epoch',save_best_only=True),\n  94 |         TensorBoard(log_dir=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'week-'+str(model_config['week_id'])+'-tensorboard', \\\n  95 |                            histogram_freq=1,batch_size=model_config['global_batch_size'],write_graph=False,\n  96 |                            write_grads=False,write_images=False,update_freq='batch')\n  97 |         ]\n  98 |         dev_dataset = strategy.experimental_distribute_dataset(dev_dataset)\n  99 |         val_dataset = strategy.experimental_distribute_dataset(val_dataset)\n 100 |     model_seg.fit(dev_dataset,\n 101 |                   epochs=model_config['train_epochs'], # model_config['train_epochs'],\n 102 |                   verbose=2,\n 103 |                   shuffle=True,\n 104 |                   validation_data=val_dataset,\n 105 |                   steps_per_epoch=model_config['train_epoch_steps'],  # 10000,  total dev records/num_works/batch size\n 106 |                   validation_steps=model_config['val_epoch_steps'],\n 107 |                   callbacks=callbacks_list)\n 108 |     return\n 109 | gfs_gcs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-gfs\"\n 110 | focus_gcs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-focus\" \n 111 | num_workers =16\n 112 | batch_size = 512\n 113 | sample_size = 7000000\n 114 | target_list = ['driver_is_brm3_4_11_15_bad','driver_is_brm3_4_11_15_bad']\n 115 | weight_list = ['driver_train_wgt_3','driver_train_wgt_3']\n 116 | global_batch_size = batch_size*num_workers\n 117 | train_test_ratio = 0.8\n 118 | train_epochs = 30\n 119 | import pandas as pd\n 120 | candidate_vars = [list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 121 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/robust_tf_input.txt',sep='|').columns),\n 122 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 123 |                   # read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt\")\n 124 |                  ]\n 125 | candidate_vars[0] = [item+\"_adp\" for item in candidate_vars[0]]\n 126 | candidate_vars[1] = [item+\"_rob\" for item in candidate_vars[1]]\n 127 | candidate_vars[2] = [item+\"_adp\" for item in candidate_vars[2]]\n 128 | # candidate_vars[3] = [item for item in candidate_vars[3]]\n 129 | model_config = {\n 130 |     'candidate_vars': candidate_vars,\n 131 |     'target_list': target_list,\n 132 |     'weight_list': weight_list,\n 133 |     'loss_list':[0.0, 1.0],\n 134 |     'dev_files': None,   \n 135 |     'val_files': None,\n 136 |     'global_batch_size': global_batch_size,\n 137 |     'buffer_size': 1,\n 138 |     'train_epoch_steps': int(sample_size*train_test_ratio/(batch_size*num_workers)), ## \u9700\u8981\u8ba1\u7b97\u539f\u59cb\u6570\u636e\u6570\u91cf n\uff0ctrain\u2014\u2014epoch-steps=n/global_batch_size/num_workers  n \u6309\u71671600\u4e07\u6765\u8ba1\u7b97\n 139 |     'val_epoch_steps': int(sample_size*(1-train_test_ratio)/(batch_size*num_workers)), ##\u540c\u4e0a\n 140 |     # 'train_epoch_steps': 200,\n 141 |     # 'val_epoch_steps': 100,\n 142 |     'train_epochs': train_epochs,\n 143 |     'lr':1e-3,\n 144 |     'loss_functions': ['binary_crossentropy', 'binary_crossentropy'],\n 145 |     'week_id': 1,\n 146 |     'job_id': None,\n 147 |     'patience': 10,\n 148 |     'multi_task': len(target_list) > 1,\n 149 |     # 'nn_shape': [2000, [[512],[128],[64]]],\n 150 |     # 'keep_model_path': '/projects/cchen16/FlagshipModelResearch/01_phase2_research/tf_model/test',\n 151 |     # 'week_id':'all',\n 152 |     # 'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/data/TF/segment_feature_v1/',\n 153 | }\n 154 | working_path = '/projects/gds-focus/data/catch/IL/CAM_variable/training_configs'\n 155 | num_workers = 4\n 156 | batch_size = 256\n 157 | sample_size = 800000\n 158 | int(sample_size*train_test_ratio/(batch_size*num_workers)),int(sample_size*(1-train_test_ratio)/(batch_size*num_workers))\n 159 | global_batch_size \n 160 | # ## Specific config\n 161 | seg_names = ['cam24_il_Tot',\n 162 |  'cam24_il_Young',\n 163 |  'cam24_il_Guest',\n 164 |  'cam24_il_Top',\n 165 |  'cam24_il_Casual',\n 166 |  'cam24_il_BrandedXO',\n 167 |  'cam24_il_BrandedRecurring',\n 168 |  'cam24_il_P2PFF',\n 169 |  'cam24_il_P2PGS',\n 170 |  'cam24_il_EbayMor',\n 171 |  'cam24_il_Other',\n 172 |  'cam24_il_MidCAM22Score']\n 173 | for i, name in enumerate(seg_names):\n 174 | def get_candidate_vars(var_path, suffix):\n 175 |     candidate_ls = read_line_file(var_path)\n 176 |     return [c+suffix for c in candidate_ls]\n 177 | se_files = {\n 178 |     # 'all_seg':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt',\"\"),\n 179 |     'Tot':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/candidates_top500.txt',\"\"),\n 180 |     'recur':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedRecurring/columns/candidates_top500.txt',\"_seg6\"),\n 181 |     'xo':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedXO/columns/candidates_top500.txt',\"_seg5\"),\n 182 |     'other':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Other/columns/candidates_top500.txt',\"_seg10\"),\n 183 |     'p2pff':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PFF/columns/candidates_top500.txt',\"_seg7\"),\n 184 |     'p2pgs':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PGS/columns/candidates_top500.txt',\"_seg8\"),\n 185 |     'ebaymor':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_EbayMor/columns/candidates_top500.txt',\"_seg9\"),\n 186 | }\n 187 | gfs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-gfs\"\n 188 | focus_bucket = \"gs://pypl-bkt-rsh-row-std-gds-focus\"\n 189 | specific_config = {\n 190 |     \"exp4_1\": {\n 191 |         'week_id':'all',\n 192 |         'candidate_vars': candidate_vars + [se_files['recur']],\n 193 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v5\",\n 194 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_tot_recur_top500_tf/',\n 195 |     },\n 196 |     \"exp4_2\": {\n 197 |         'week_id':'all',\n 198 |         'candidate_vars': candidate_vars + [se_files['xo']],\n 199 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v5\",\n 200 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_tot_xo_top500_tf/',\n 201 |     },\n 202 |     \"exp4_3\": {\n 203 |         'week_id':'all',\n 204 |         'candidate_vars': candidate_vars + [se_files['other']],\n 205 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v5\",\n 206 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_tot_other_top500_tf/',\n 207 |     },\n 208 |     \"exp4_4\": {\n 209 |         'week_id':'all',\n 210 |         'candidate_vars': candidate_vars + [se_files['p2pff']],\n 211 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v5\",\n 212 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_tot_p2pff_top500_tf/',\n 213 |     },\n 214 |     \"exp4_5\": {\n 215 |         'week_id':'all',\n 216 |         'candidate_vars': candidate_vars + [se_files['p2pgs']],\n 217 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v5\",\n 218 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_tot_p2pgs_top500_tf/',\n 219 |     },\n 220 |     \"exp4_6\": {\n 221 |         'week_id':'all',\n 222 |         'candidate_vars': candidate_vars + [se_files['ebaymor']],\n 223 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v5\",\n 224 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_tot_ebaymor_top500_tf/',\n 225 |     },\n 226 | }\n 227 | # !gsutil -m cp -r /projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/cam_il_new_var_logit_v5 gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/\n 228 | # ### Local debug\n 229 | # import logging\n 230 | # logging.basicConfig(stream=sys.stdout, level='INFO')\n 231 | # logger = logging.getLogger('Train')\n 232 | # strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n 233 | tf.strings.to_hash_bucket_fast([\"Hello\", \"TensorFlow\", \"2.x\"], 3).numpy()\n 234 | exp_trainer_all(tmp_config)\n 235 | # !ls -tl $working_path\n 236 | working_path\n 237 | md = \"0106\"\n 238 | exp_idx = 'exp4'\n 239 | t =os.path.join(working_path,f'{exp_idx}_job_ids_{md}.json')\n 240 | if os.path.exists(t):\n 241 |     with open(t, 'r') as f:\n 242 |         job_ids = json.load(f)\n 243 | with open(f\"{working_path}/exp3_job_ids_1120.json\", 'r') as f:\n 244 |     exp3_job_ids = json.load(f)\n 245 | exp3_job_ids\n 246 | specific_config.keys()\n 247 | job_ids = {}\n 248 | run_ls = ['exp4_1', 'exp4_2', 'exp4_3', 'exp4_4', 'exp4_5', 'exp4_6']\n 249 | for exp_id in run_ls:\n 250 |     week_id = specific_config[exp_id][\"week_id\"]\n 251 |     tmp_config = copy.deepcopy(model_config)  \n 252 |     tmp_config['week_id'] = str(week_id)\n 253 |     for k, v in specific_config[exp_id].items():\n 254 |         if k not in {\"week_id\", \"data_path\"}:\n 255 |             tmp_config[k] = v\n 256 |     data = specific_config[exp_id][\"data_path\"]\n 257 |     tmp_config['previous_model_path'] = specific_config[exp_id][\"previous_model_path\"]\n 258 |     all_files = tf.io.gfile.glob(data+'*.gz')\n 259 |     dev_files, val_files = train_test_split(all_files, test_size=0.2, shuffle=True)\n 260 |     tmp_config['dev_files']=dev_files\n 261 |     tmp_config['val_files']=val_files\n 262 |     tmp_config['keep_model_path'] = f'{focus_bucket}/user/cchen16/IL_variable/{md}_{exp_idx}/{exp_id}_gcp_loss/week_'\n 263 |     # exp_trainer_all(tmp_config)\n 264 |     job_id = client.create_job(\n 265 |                 func = exp_trainer_all,\n 266 |                 pyfiles_to_import=[\n 267 |                                     '/projects/gds-focus/data/catch/IL/CAM_variable/assets/sf_il_utils.py',\n 268 |                                     '/projects/cchen16/FlagshipModelResearch/01_phase2_research/il_utils/cc_functions.py',\n 269 |                                   ], \n 270 |                 num_workers = num_workers,\n 271 |                 machine_type_per_worker = \"n1-highmem-16\",\n 272 |                 # machine_type_per_worker = \"n2d-highmem-8\",\n 273 |                 model_config = tmp_config\n 274 |                 )\n 275 |     model_name = f'{exp_id}_week{week_id}'\n 276 |     job_ids[model_name] = job_id\n 277 |     tmp_config['job_id'] = job_id\n 278 |     with open(os.path.join(working_path,f'{model_name}_{md}.json'),'w') as f:\n 279 |         json.dump(tmp_config,f)\n 280 | with open(t,'w') as f:\n 281 |     json.dump(job_ids, f)\n 282 | # client.cancel_job('maglev/cloud/dataproc/job/gds-focus-3d570d/2aa7536c-25bb-35f9-9590-bb70afaa76fd')\n 283 | # ## Tracking\n 284 | def pretty_model_config(configs):\n 285 |     for k, v in configs.items():\n 286 |         if k not in {\"candidate_vars\", \"dev_files\", \"val_files\"}:\n 287 |         if k in {\"dev_files\"}:\n 288 |         if k in {\"candidate_vars\"}:\n 289 |             candidate_len = []\n 290 |             for ls in v:\n 291 |                 candidate_len.append(len(ls))\n 292 |     return\n 293 | def check_job_state(job_ids):\n 294 |     for k,v in job_ids.items():\n 295 | def cancel_job(job_ids):\n 296 |     for k,v in job_ids.items():\n 297 | week_id = 'all'\n 298 | model_name = f'{exp_id}_week{week_id}'\n 299 | config_path = os.path.join(working_path,f'{model_name}_{md}.json')\n 300 | with open(os.path.join(working_path,f'{model_name}_{md}.json'),'r') as f:\n 301 |     config = json.load(f)\n 302 | pretty_model_config(config)\n 303 | job_ids\n 304 | check_job_state(job_ids)\n 305 | client.wait_job_for_completion(job_ids['exp4_1_weekall'],\n 306 |                                time_to_sleep=300)\n 307 | # # %%capture job_log  \n 308 | client.get_job_driver_logs(job_ids['exp4_2_weekall'])\n 309 | # # %%capture job_log  \n 310 | client.get_job_driver_logs(\"cosmos/cloud/dataproc/job/gds-focus-2822ac/3914f280-cdfd-39e5-b912-41cfa8cb3c80\")\n 311 | import re\n 312 | log = \"\"\"\n 313 | tf.math.sigmoid_loss: 0.6841 - tf.math.sigmoid_5_loss: 0.9473 - tf.math.sigmoid_recall: 0.3386 - tf.math.sigmoid_precision: 0.6501 - tf.math.sigmoid_5_recall: 0.5958 - tf.math.sigmoid_5_precision: 0.5299 - val_loss: 0.6088 - val_tf.math.sigmoid_loss: 0.6763 - val_tf.math.sigmoid_5_loss: 0.6088 - val_tf.math.sigmoid_recall: 0.3230 - val_tf.math.sigmoid_precision: 0.6918 - val_tf.math.sigmoid_5_recall: 0.5891 - val_tf.math.sigmoid_5_precision: 0.6756\n 314 | \"\"\"\n 315 | pattern = r'(\\w+[\\.\\w]*)\\:\\s*([\\d\\.\\-]+)'  \n 316 | metrics = re.findall(pattern, log)  \n 317 | metrics_dict = dict(metrics)  \n 318 | for key, value in metrics_dict.items(): \n 319 | # !gsutil -m cp -r gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/1120_exp1/exp1_1_gcp_loss /projects/cchen16/FlagshipModelResearch/01_phase2_research/tensorboard/\n 320 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/0930_exp1/*/week_all\n 321 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/0924_exp1/exp1_2_gcp_loss/week_all/\n 322 | # ## Check config and log\n 323 | exp_id = 'exp3_7'\n 324 | week_id = 'all'\n 325 | model_name = f'{exp_id}_week{week_id}'\n 326 | config_path = os.path.join(working_path,f'{model_name}_{md}.json')\n 327 | with open(os.path.join(working_path,f'{model_name}_{md}.json'),'r') as f:\n 328 |     config = json.load(f)\n 329 | pretty_model_config(config)\n 330 | with open()\n 331 | client.get_job_driver_logs(job_id)\n 332 | log_file = f\"/projects/gds-focus/data/catch/IL/CAM_variable/training_logs/{model_name}.log\"\n 333 | with open(log_file, \"a\") as f:\n 334 |     f.write(job_ids[model_name])\n 335 |     f.write(\"**\"*50)\n 336 |     client.get_job_driver_logs(gcp_job_id, file=f)\n 337 | import re\n 338 | def extract_metric(log_path):\n 339 |     with open(log_path, 'r') as f:\n 340 |         log = f.read()\n 341 |     pattern = r'(\\w+[\\.\\w]*)\\:\\s*([\\d\\.\\-]+)'  \n 342 |     metrics = re.findall(pattern, log)  \n 343 |     metrics_dict = dict(metrics)  \n 344 |     for key, value in metrics_dict.items():  \n 345 |     return\n 346 | extract_metric(log_file)\n 347 | set(se_files['all_seg'])-set(config['candidate_vars'][3])\n 348 | client.get_job_application_logs(config['job_id'])\n 349 | 1024*1800\n 350 | client.get_job_driver_logs('maglev/cloud/dataproc/job/gds-focus-5aa772/927ee8e9-2e74-3539-ad7b-463e1126e952')",
    "rmr_agent/repos/CAM_variable_research/3_4_gcp_training.py": "   1 | # # import prepare\n   2 | # %ppauth\n   3 | import tensorflow as tf\n   4 | import sys,os\n   5 | import aml.cloud_v1 as cloud\n   6 | import tensorboard\n   7 | client = cloud.DataProcClient(gcp_project='ccg24-hrzana-gds-focus')\n   8 | from model_automation.gcp import dataproc_config\n   9 | from aml import cloud_v1 as cloud\n  10 | from tqdm import tqdm\n  11 | from sklearn.model_selection import train_test_split\n  12 | import pandas as pd\n  13 | import json\n  14 | import copy\n  15 | from model_automation.utils.rmr.helper import run_cmd\n  16 | util_path = '/projects/cchen16/FlagshipModelResearch/01_phase2_research/il_utils'\n  17 | sys.path.append(util_path)\n  18 | sys.path.append('/projects/gds-focus/data/catch/IL/CAM_variable/assets')\n  19 | from helper import *\n  20 | from sf_il_utils import custom_loss, binary_focal_loss\n  21 | import importlib\n  22 | from sf_il_utils import global_data_prepare_for_ensemble as data_prepare\n  23 | tf.__version__\n  24 | # # exp trainer define\n  25 | def exp_trainer_all(model_config):\n  26 |     import sys\n  27 |     import logging\n  28 |     import random    \n  29 |     import os\n  30 |     import sys   \n  31 |     import numpy as np\n  32 |     import tensorflow as tf\n  33 |     from tensorflow import keras\n  34 |     from tensorflow.keras.models import Sequential\n  35 |     from tensorflow.keras.layers import Dense, Dropout\n  36 |     from tensorflow.keras import optimizers\n  37 |     from tensorflow.python.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, TensorBoard\n  38 |     from tensorflow.keras import backend as K, Sequential, Model, layers\n  39 |     from tensorflow.keras.utils import get_custom_objects\n  40 |     # from junrui_utils import global_data_prepare_for_ensemble as data_prepare    \n  41 |     from sf_il_utils import global_data_prepare_for_ensemble as data_prepare\n  42 |     from sf_il_utils import custom_loss, binary_focal_loss\n  43 |     from cc_functions import create_single_pepnet, create_mlp_model, create_cascade_model, create_cascade_model_v2\n  44 |     K.set_floatx('float32')\n  45 |     K.set_epsilon(1e-16)\n  46 |     logging.basicConfig(stream=sys.stdout, level='INFO')\n  47 |     logger = logging.getLogger('Train')\n  48 |     strategy = tf.distribute.MultiWorkerMirroredStrategy()\n  49 |     from tensorflow.keras import metrics\n  50 |     from sklearn.model_selection import train_test_split\n  51 |     with strategy.scope():\n  52 |         model_seg =tf.keras.models.load_model(model_config['previous_model_path'])\n  53 |         model_seg.summary()\n  54 |         # increasing lr schedule\n  55 |         custom_objects = {\"custom_loss\": custom_loss,\n  56 |                           \"focal_loss\": binary_focal_loss(gamma=2., alpha=.25),\n  57 |                           \"binary_crossentropy\":\"binary_crossentropy\"}\n  58 |         with keras.utils.custom_object_scope(custom_objects):\n  59 |             model_seg.compile(tf.keras.optimizers.Adam(model_config['lr']),\n  60 |                           loss=[custom_objects[f] for f in model_config['loss_functions']],\n  61 |                           loss_weights=model_config['loss_list'],\n  62 |                           metrics=[\n  63 |                                    metrics.Recall(thresholds=0.1,name='recall'), \n  64 |                                    metrics.Precision(thresholds=0.1,name='precision')\n  65 |                                   ],\n  66 |                          )\n  67 |         prepare = data_prepare()\n  68 |         dev_fl = tf.io.gfile.glob(model_config['dev_files'])\n  69 |         val_fl = tf.io.gfile.glob(model_config['val_files'])\n  70 |         dev_dataset = prepare.make_dataset(filenames = dev_fl, \n  71 |                                             feature_list = model_config['candidate_vars'], \n  72 |                                             target_list = model_config['target_list'], \n  73 |                                             weight_list = model_config['weight_list'], \n  74 |                                             batch_size = model_config['global_batch_size'], \n  75 |                                             buffer_size = model_config['buffer_size'],\n  76 |                                             multi_task = model_config['multi_task'], \n  77 |                                             compress = 'GZIP',\n  78 |                                           )    \n  79 |         val_dataset = prepare.make_dataset(filenames = val_fl, \n  80 |                                             feature_list = model_config['candidate_vars'], \n  81 |                                             target_list = model_config['target_list'], \n  82 |                                             weight_list = model_config['weight_list'], \n  83 |                                             batch_size = model_config['global_batch_size'], \n  84 |                                             buffer_size = model_config['buffer_size'],\n  85 |                                             multi_task = model_config['multi_task'],\n  86 |                                             compress = 'GZIP',\n  87 |                                           )\n  88 |         callbacks_list = [\n  89 |         EarlyStopping(monitor='val_loss', patience=model_config['patience'], verbose=2, mode='min',restore_best_weights=True),\n  90 |         ModelCheckpoint(filepath=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'model.{epoch:02d}-{loss:.8f}', \\\n  91 |                         monitor='val_loss',mode='min',save_freq='epoch',save_best_only=False),\n  92 |         ModelCheckpoint(filepath=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'week-'+str(model_config['week_id'])+'-best_model', \\\n  93 |                         monitor='val_loss',mode='min',save_freq='epoch',save_best_only=True),\n  94 |         TensorBoard(log_dir=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'week-'+str(model_config['week_id'])+'-tensorboard', \\\n  95 |                            histogram_freq=1,batch_size=model_config['global_batch_size'],write_graph=False,\n  96 |                            write_grads=False,write_images=False,update_freq='batch')\n  97 |         ]\n  98 |         dev_dataset = strategy.experimental_distribute_dataset(dev_dataset)\n  99 |         val_dataset = strategy.experimental_distribute_dataset(val_dataset)\n 100 |     model_seg.fit(dev_dataset,\n 101 |                   epochs=model_config['train_epochs'], # model_config['train_epochs'],\n 102 |                   verbose=2,\n 103 |                   shuffle=True,\n 104 |                   validation_data=val_dataset,\n 105 |                   steps_per_epoch=model_config['train_epoch_steps'],  # 10000,  total dev records/num_works/batch size\n 106 |                   validation_steps=model_config['val_epoch_steps'],\n 107 |                   callbacks=callbacks_list)\n 108 |     return\n 109 | gfs_gcs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-gfs\"\n 110 | focus_gcs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-focus\" \n 111 | num_workers =16\n 112 | batch_size = 512\n 113 | sample_size = 4500000\n 114 | target_list = ['driver_is_brm3_4_11_15_bad','driver_is_brm3_4_11_15_bad']\n 115 | weight_list = ['driver_train_wgt_3','driver_train_wgt_3']\n 116 | global_batch_size = batch_size*num_workers\n 117 | train_test_ratio = 0.8\n 118 | train_epochs = 30\n 119 | import pandas as pd\n 120 | candidate_vars = [list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 121 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/robust_tf_input.txt',sep='|').columns),\n 122 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 123 |                   # read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt\")\n 124 |                  ]\n 125 | candidate_vars[0] = [item+\"_adp\" for item in candidate_vars[0]]\n 126 | candidate_vars[1] = [item+\"_rob\" for item in candidate_vars[1]]\n 127 | candidate_vars[2] = [item+\"_adp\" for item in candidate_vars[2]]\n 128 | # candidate_vars[3] = [item for item in candidate_vars[3]]\n 129 | model_config = {\n 130 |     'candidate_vars': candidate_vars,\n 131 |     'target_list': target_list,\n 132 |     'weight_list': weight_list,\n 133 |     'loss_list':[0.0, 1.0],\n 134 |     'dev_files': None,   \n 135 |     'val_files': None,\n 136 |     'global_batch_size': global_batch_size,\n 137 |     'buffer_size': 1,\n 138 |     'train_epoch_steps': int(sample_size*train_test_ratio/(batch_size*num_workers)), ## \u9700\u8981\u8ba1\u7b97\u539f\u59cb\u6570\u636e\u6570\u91cf n\uff0ctrain\u2014\u2014epoch-steps=n/global_batch_size/num_workers  n \u6309\u71671600\u4e07\u6765\u8ba1\u7b97\n 139 |     'val_epoch_steps': int(sample_size*(1-train_test_ratio)/(batch_size*num_workers)), ##\u540c\u4e0a\n 140 |     # 'train_epoch_steps': 200,\n 141 |     # 'val_epoch_steps': 100,\n 142 |     'train_epochs': train_epochs,\n 143 |     'lr':1e-3,\n 144 |     'loss_functions': ['binary_crossentropy', 'binary_crossentropy'],\n 145 |     'week_id': 1,\n 146 |     'job_id': None,\n 147 |     'patience': 10,\n 148 |     'multi_task': len(target_list) > 1,\n 149 |     # 'nn_shape': [2000, [[512],[128],[64]]],\n 150 |     # 'keep_model_path': '/projects/cchen16/FlagshipModelResearch/01_phase2_research/tf_model/test',\n 151 |     # 'week_id':'all',\n 152 |     # 'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/data/TF/segment_feature_v1/',\n 153 | }\n 154 | working_path = '/projects/gds-focus/data/catch/IL/CAM_variable/training_configs'\n 155 | num_workers = 4\n 156 | batch_size = 256\n 157 | sample_size = 800000\n 158 | int(sample_size*train_test_ratio/(batch_size*num_workers)),int(sample_size*(1-train_test_ratio)/(batch_size*num_workers))\n 159 | global_batch_size \n 160 | # ## Specific config\n 161 | seg_names = ['cam24_il_Tot',\n 162 |  'cam24_il_Young',\n 163 |  'cam24_il_Guest',\n 164 |  'cam24_il_Top',\n 165 |  'cam24_il_Casual',\n 166 |  'cam24_il_BrandedXO',\n 167 |  'cam24_il_BrandedRecurring',\n 168 |  'cam24_il_P2PFF',\n 169 |  'cam24_il_P2PGS',\n 170 |  'cam24_il_EbayMor',\n 171 |  'cam24_il_Other',\n 172 |  'cam24_il_MidCAM22Score']\n 173 | for i, name in enumerate(seg_names):\n 174 | def get_candidate_vars(var_path, suffix):\n 175 |     candidate_ls = read_line_file(var_path)\n 176 |     return [c+suffix for c in candidate_ls]\n 177 | se_files = {\n 178 |     # 'all_seg':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt',\"\"),\n 179 |     'Tot':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/candidates_top500.txt',\"\"),\n 180 |     'recur':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedRecurring/columns/candidates_top500.txt',\"_seg6\"),\n 181 |     'xo':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedXO/columns/candidates_top500.txt',\"_seg5\"),\n 182 |     'other':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Other/columns/candidates_top500.txt',\"_seg10\"),\n 183 |     'p2pff':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PFF/columns/candidates_top500.txt',\"_seg7\"),\n 184 |     'p2pgs':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PGS/columns/candidates_top500.txt',\"_seg8\"),\n 185 |     'ebaymor':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_EbayMor/columns/candidates_top500.txt',\"_seg9\"),\n 186 | }\n 187 | gfs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-gfs\"\n 188 | focus_bucket = \"gs://pypl-bkt-rsh-row-std-gds-focus\"\n 189 | specific_config = {\n 190 |     \"exp5_1\": {\n 191 |         'week_id':'all',\n 192 |         'candidate_vars': candidate_vars + [se_files['recur']],\n 193 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v5\",\n 194 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_yg_tot_recur_top500_tf/',\n 195 |     },\n 196 |     \"exp5_2\": {\n 197 |         'week_id':'all',\n 198 |         'candidate_vars': candidate_vars + [se_files['xo']],\n 199 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v5\",\n 200 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_yg_tot_xo_top500_tf/',\n 201 |     },\n 202 |     \"exp5_3\": {\n 203 |         'week_id':'all',\n 204 |         'candidate_vars': candidate_vars + [se_files['other']],\n 205 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v5\",\n 206 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_yg_tot_other_top500_tf/',\n 207 |     },\n 208 |     \"exp5_4\": {\n 209 |         'week_id':'all',\n 210 |         'candidate_vars': candidate_vars + [se_files['p2pff']],\n 211 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v5\",\n 212 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_yg_tot_p2pff_top500_tf/',\n 213 |     },\n 214 |     \"exp5_5\": {\n 215 |         'week_id':'all',\n 216 |         'candidate_vars': candidate_vars + [se_files['p2pgs']],\n 217 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v5\",\n 218 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_yg_tot_p2pgs_top500_tf/',\n 219 |     },\n 220 |     \"exp5_6\": {\n 221 |         'week_id':'all',\n 222 |         'candidate_vars': candidate_vars + [se_files['ebaymor']],\n 223 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v5\",\n 224 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_yg_tot_ebaymor_top500_tf/',\n 225 |     },\n 226 | }\n 227 | # !gsutil -m cp -r /projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/cam_il_new_var_logit_v5 gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/\n 228 | # ### Local debug\n 229 | import logging\n 230 | logging.basicConfig(stream=sys.stdout, level='INFO')\n 231 | logger = logging.getLogger('Train')\n 232 | strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n 233 | exp_trainer_all(tmp_config)\n 234 | # !ls -tl $working_path\n 235 | working_path\n 236 | md = \"0109\"\n 237 | exp_idx = 'exp5'\n 238 | t =os.path.join(working_path,f'{exp_idx}_job_ids_{md}.json')\n 239 | if os.path.exists(t):\n 240 |     with open(t, 'r') as f:\n 241 |         job_ids = json.load(f)\n 242 | specific_config.keys()\n 243 | job_ids = {}\n 244 | run_ls = ['exp5_1', 'exp5_2', 'exp5_3', 'exp5_4', 'exp5_5', 'exp5_6']\n 245 | for exp_id in run_ls:\n 246 |     week_id = specific_config[exp_id][\"week_id\"]\n 247 |     tmp_config = copy.deepcopy(model_config)  \n 248 |     tmp_config['week_id'] = str(week_id)\n 249 |     for k, v in specific_config[exp_id].items():\n 250 |         if k not in {\"week_id\", \"data_path\"}:\n 251 |             tmp_config[k] = v\n 252 |     data = specific_config[exp_id][\"data_path\"]\n 253 |     tmp_config['previous_model_path'] = specific_config[exp_id][\"previous_model_path\"]\n 254 |     all_files = tf.io.gfile.glob(data+'*.gz')\n 255 |     dev_files, val_files = train_test_split(all_files, test_size=0.2, shuffle=True)\n 256 |     tmp_config['dev_files']=dev_files\n 257 |     tmp_config['val_files']=val_files\n 258 |     tmp_config['keep_model_path'] = f'{focus_bucket}/user/cchen16/IL_variable/{md}_{exp_idx}/{exp_id}_gcp_loss/week_'\n 259 |     # exp_trainer_all(tmp_config)\n 260 |     job_id = client.create_job(\n 261 |                 func = exp_trainer_all,\n 262 |                 pyfiles_to_import=[\n 263 |                                     '/projects/gds-focus/data/catch/IL/CAM_variable/assets/sf_il_utils.py',\n 264 |                                     '/projects/cchen16/FlagshipModelResearch/01_phase2_research/il_utils/cc_functions.py',\n 265 |                                   ], \n 266 |                 num_workers = num_workers,\n 267 |                 machine_type_per_worker = \"n1-highmem-16\",\n 268 |                 # machine_type_per_worker = \"n2d-highmem-8\",\n 269 |                 model_config = tmp_config\n 270 |                 )\n 271 |     model_name = f'{exp_id}_week{week_id}'\n 272 |     job_ids[model_name] = job_id\n 273 |     tmp_config['job_id'] = job_id\n 274 |     with open(os.path.join(working_path,f'{model_name}_{md}.json'),'w') as f:\n 275 |         json.dump(tmp_config,f)\n 276 | with open(t,'w') as f:\n 277 |     json.dump(job_ids, f)\n 278 | # client.cancel_job('maglev/cloud/dataproc/job/gds-focus-3d570d/2aa7536c-25bb-35f9-9590-bb70afaa76fd')\n 279 | # ## Tracking\n 280 | def pretty_model_config(configs):\n 281 |     for k, v in configs.items():\n 282 |         if k not in {\"candidate_vars\", \"dev_files\", \"val_files\"}:\n 283 |         if k in {\"dev_files\"}:\n 284 |         if k in {\"candidate_vars\"}:\n 285 |             candidate_len = []\n 286 |             for ls in v:\n 287 |                 candidate_len.append(len(ls))\n 288 |     return\n 289 | def check_job_state(job_ids):\n 290 |     for k,v in job_ids.items():\n 291 | def cancel_job(job_ids):\n 292 |     for k,v in job_ids.items():\n 293 | week_id = 'all'\n 294 | model_name = f'{exp_id}_week{week_id}'\n 295 | config_path = os.path.join(working_path,f'{model_name}_{md}.json')\n 296 | with open(os.path.join(working_path,f'{model_name}_{md}.json'),'r') as f:\n 297 |     config = json.load(f)\n 298 | pretty_model_config(config)\n 299 | job_ids\n 300 | check_job_state(job_ids)\n 301 | client.wait_job_for_completion(job_ids['exp5_2_weekall'],\n 302 |                                time_to_sleep=300)\n 303 | # # %%capture job_log  \n 304 | client.get_job_driver_logs(job_ids['exp5_1_weekall'])\n 305 | # # %%capture job_log  \n 306 | client.get_job_driver_logs(\"cosmos/cloud/dataproc/job/gds-focus-2822ac/3914f280-cdfd-39e5-b912-41cfa8cb3c80\")\n 307 | import re\n 308 | log = \"\"\"\n 309 | tf.math.sigmoid_loss: 0.6841 - tf.math.sigmoid_5_loss: 0.9473 - tf.math.sigmoid_recall: 0.3386 - tf.math.sigmoid_precision: 0.6501 - tf.math.sigmoid_5_recall: 0.5958 - tf.math.sigmoid_5_precision: 0.5299 - val_loss: 0.6088 - val_tf.math.sigmoid_loss: 0.6763 - val_tf.math.sigmoid_5_loss: 0.6088 - val_tf.math.sigmoid_recall: 0.3230 - val_tf.math.sigmoid_precision: 0.6918 - val_tf.math.sigmoid_5_recall: 0.5891 - val_tf.math.sigmoid_5_precision: 0.6756\n 310 | \"\"\"\n 311 | pattern = r'(\\w+[\\.\\w]*)\\:\\s*([\\d\\.\\-]+)'  \n 312 | metrics = re.findall(pattern, log)  \n 313 | metrics_dict = dict(metrics)  \n 314 | for key, value in metrics_dict.items(): \n 315 | # !gsutil -m cp -r gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/1120_exp1/exp1_1_gcp_loss /projects/cchen16/FlagshipModelResearch/01_phase2_research/tensorboard/\n 316 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/0930_exp1/*/week_all\n 317 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/0924_exp1/exp1_2_gcp_loss/week_all/\n 318 | # ## Check config and log\n 319 | exp_id = 'exp3_7'\n 320 | week_id = 'all'\n 321 | model_name = f'{exp_id}_week{week_id}'\n 322 | config_path = os.path.join(working_path,f'{model_name}_{md}.json')\n 323 | with open(os.path.join(working_path,f'{model_name}_{md}.json'),'r') as f:\n 324 |     config = json.load(f)\n 325 | pretty_model_config(config)\n 326 | with open()\n 327 | client.get_job_driver_logs(job_id)\n 328 | log_file = f\"/projects/gds-focus/data/catch/IL/CAM_variable/training_logs/{model_name}.log\"\n 329 | with open(log_file, \"a\") as f:\n 330 |     f.write(job_ids[model_name])\n 331 |     f.write(\"**\"*50)\n 332 |     client.get_job_driver_logs(gcp_job_id, file=f)\n 333 | import re\n 334 | def extract_metric(log_path):\n 335 |     with open(log_path, 'r') as f:\n 336 |         log = f.read()\n 337 |     pattern = r'(\\w+[\\.\\w]*)\\:\\s*([\\d\\.\\-]+)'  \n 338 |     metrics = re.findall(pattern, log)  \n 339 |     metrics_dict = dict(metrics)  \n 340 |     for key, value in metrics_dict.items():  \n 341 |     return\n 342 | extract_metric(log_file)\n 343 | set(se_files['all_seg'])-set(config['candidate_vars'][3])\n 344 | client.get_job_application_logs(config['job_id'])\n 345 | 1024*1800\n 346 | client.get_job_driver_logs('maglev/cloud/dataproc/job/gds-focus-5aa772/927ee8e9-2e74-3539-ad7b-463e1126e952')",
    "rmr_agent/repos/CAM_variable_research/3_5_gcp_training.py": "   1 | # # import prepare\n   2 | # %ppauth\n   3 | import tensorflow as tf\n   4 | import sys,os\n   5 | import aml.cloud_v1 as cloud\n   6 | import tensorboard\n   7 | client = cloud.DataProcClient(gcp_project='ccg24-hrzana-gds-focus')\n   8 | from model_automation.gcp import dataproc_config\n   9 | from aml import cloud_v1 as cloud\n  10 | from tqdm import tqdm\n  11 | from sklearn.model_selection import train_test_split\n  12 | import pandas as pd\n  13 | import json\n  14 | import copy\n  15 | from model_automation.utils.rmr.helper import run_cmd\n  16 | util_path = '/projects/cchen16/FlagshipModelResearch/01_phase2_research/il_utils'\n  17 | sys.path.append(util_path)\n  18 | sys.path.append('/projects/gds-focus/data/catch/IL/CAM_variable/assets')\n  19 | from helper import *\n  20 | from sf_il_utils import custom_loss, binary_focal_loss\n  21 | import importlib\n  22 | from sf_il_utils import global_data_prepare_for_ensemble as data_prepare\n  23 | tf.__version__\n  24 | # !nvidia-smi\n  25 | # # exp trainer define\n  26 | def exp_trainer_all(model_config):\n  27 |     import sys\n  28 |     import logging\n  29 |     import random    \n  30 |     import os\n  31 |     import sys   \n  32 |     import numpy as np\n  33 |     import tensorflow as tf\n  34 |     from tensorflow import keras\n  35 |     from tensorflow.keras.models import Sequential\n  36 |     from tensorflow.keras.layers import Dense, Dropout\n  37 |     from tensorflow.keras import optimizers\n  38 |     from tensorflow.python.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, TensorBoard\n  39 |     from tensorflow.keras import backend as K, Sequential, Model, layers\n  40 |     from tensorflow.keras.utils import get_custom_objects\n  41 |     # from junrui_utils import global_data_prepare_for_ensemble as data_prepare    \n  42 |     from sf_il_utils import global_data_prepare_for_ensemble as data_prepare\n  43 |     from sf_il_utils import custom_loss, binary_focal_loss\n  44 |     from cc_functions import create_single_pepnet, create_mlp_model, create_cascade_model, create_cascade_model_v2\n  45 |     K.set_floatx('float32')\n  46 |     K.set_epsilon(1e-16)\n  47 |     logging.basicConfig(stream=sys.stdout, level='INFO')\n  48 |     logger = logging.getLogger('Train')\n  49 |     strategy = tf.distribute.MultiWorkerMirroredStrategy()\n  50 |     from tensorflow.keras import metrics\n  51 |     from sklearn.model_selection import train_test_split\n  52 |     with strategy.scope():\n  53 |         model_seg =tf.keras.models.load_model(model_config['previous_model_path'])\n  54 |         model_seg.summary()\n  55 |         # increasing lr schedule\n  56 |         custom_objects = {\"custom_loss\": custom_loss,\n  57 |                           \"focal_loss\": binary_focal_loss(gamma=2., alpha=.25),\n  58 |                           \"binary_crossentropy\":\"binary_crossentropy\"}\n  59 |         with keras.utils.custom_object_scope(custom_objects):\n  60 |             model_seg.compile(tf.keras.optimizers.Adam(model_config['lr']),\n  61 |                           loss=[custom_objects[f] for f in model_config['loss_functions']],\n  62 |                           loss_weights=model_config['loss_list'],\n  63 |                           metrics=[metrics.AUC(name='auc'),\n  64 |                                    metrics.Recall(thresholds=0.1,name='recall'), \n  65 |                                    metrics.Precision(thresholds=0.1,name='precision')\n  66 |                                   ],\n  67 |                          )\n  68 |         prepare = data_prepare()\n  69 |         dev_fl = tf.io.gfile.glob(model_config['dev_files'])\n  70 |         val_fl = tf.io.gfile.glob(model_config['val_files'])\n  71 |         dev_dataset = prepare.make_dataset(filenames = dev_fl, \n  72 |                                             feature_list = model_config['candidate_vars'], \n  73 |                                             target_list = model_config['target_list'], \n  74 |                                             weight_list = model_config['weight_list'], \n  75 |                                             batch_size = model_config['global_batch_size'], \n  76 |                                             buffer_size = model_config['buffer_size'],\n  77 |                                             multi_task = model_config['multi_task'], \n  78 |                                             compress = 'GZIP',\n  79 |                                           )    \n  80 |         val_dataset = prepare.make_dataset(filenames = val_fl, \n  81 |                                             feature_list = model_config['candidate_vars'], \n  82 |                                             target_list = model_config['target_list'], \n  83 |                                             weight_list = model_config['weight_list'], \n  84 |                                             batch_size = model_config['global_batch_size'], \n  85 |                                             buffer_size = model_config['buffer_size'],\n  86 |                                             multi_task = model_config['multi_task'],\n  87 |                                             compress = 'GZIP',\n  88 |                                           )\n  89 |         callbacks_list = [\n  90 |         EarlyStopping(monitor='val_loss', patience=model_config['patience'], verbose=2, mode='min',restore_best_weights=True),\n  91 |         ModelCheckpoint(filepath=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'model.{epoch:02d}-{loss:.8f}', \\\n  92 |                         monitor='val_loss',mode='min',save_freq='epoch',save_best_only=False),\n  93 |         ModelCheckpoint(filepath=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'week-'+str(model_config['week_id'])+'-best_model', \\\n  94 |                         monitor='val_loss',mode='min',save_freq='epoch',save_best_only=True),\n  95 |         TensorBoard(log_dir=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'week-'+str(model_config['week_id'])+'-tensorboard', \\\n  96 |                            histogram_freq=1,batch_size=model_config['global_batch_size'],write_graph=False,\n  97 |                            write_grads=False,write_images=False,update_freq='batch')\n  98 |         ]\n  99 |         dev_dataset = strategy.experimental_distribute_dataset(dev_dataset)\n 100 |         val_dataset = strategy.experimental_distribute_dataset(val_dataset)\n 101 |     model_seg.fit(dev_dataset,\n 102 |                   epochs=model_config['train_epochs'], # model_config['train_epochs'],\n 103 |                   verbose=2,\n 104 |                   shuffle=True,\n 105 |                   validation_data=val_dataset,\n 106 |                   steps_per_epoch=model_config['train_epoch_steps'],  # 10000,  total dev records/num_works/batch size\n 107 |                   validation_steps=model_config['val_epoch_steps'],\n 108 |                   callbacks=callbacks_list)\n 109 |     return\n 110 | gfs_gcs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-gfs\"\n 111 | focus_gcs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-focus\" \n 112 | num_workers = 16\n 113 | batch_size = 512\n 114 | sample_size = 6000000\n 115 | target_list = ['driver_is_brm3_4_11_15_bad','driver_is_brm3_4_11_15_bad']\n 116 | weight_list = ['driver_train_wgt_3','driver_train_wgt_3']\n 117 | global_batch_size = batch_size*num_workers\n 118 | train_test_ratio = 0.8\n 119 | train_epochs = 10\n 120 | import pandas as pd\n 121 | candidate_vars = [list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 122 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/robust_tf_input.txt',sep='|').columns),\n 123 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 124 |                   # read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt\")\n 125 |                  ]\n 126 | candidate_vars[0] = [item+\"_adp\" for item in candidate_vars[0]]\n 127 | candidate_vars[1] = [item+\"_rob\" for item in candidate_vars[1]]\n 128 | candidate_vars[2] = [item+\"_adp\" for item in candidate_vars[2]]\n 129 | # candidate_vars[3] = [item for item in candidate_vars[3]]\n 130 | model_config = {\n 131 |     'candidate_vars': candidate_vars,\n 132 |     'target_list': target_list,\n 133 |     'weight_list': weight_list,\n 134 |     'loss_list':[0.0, 1.0],\n 135 |     'dev_files': None,   \n 136 |     'val_files': None,\n 137 |     'global_batch_size': global_batch_size,\n 138 |     'buffer_size': 1,\n 139 |     'train_epoch_steps': int(sample_size*train_test_ratio/(batch_size*num_workers)), ## \u9700\u8981\u8ba1\u7b97\u539f\u59cb\u6570\u636e\u6570\u91cf n\uff0ctrain\u2014\u2014epoch-steps=n/global_batch_size/num_workers  n \u6309\u71671600\u4e07\u6765\u8ba1\u7b97\n 140 |     'val_epoch_steps': int(sample_size*(1-train_test_ratio)/(batch_size*num_workers)), ##\u540c\u4e0a\n 141 |     # 'train_epoch_steps': 200,\n 142 |     # 'val_epoch_steps': 100,\n 143 |     'train_epochs': train_epochs,\n 144 |     'lr':1e-3,\n 145 |     'loss_functions': ['binary_crossentropy', 'binary_crossentropy'],\n 146 |     'week_id': 1,\n 147 |     'job_id': None,\n 148 |     'patience': 10,\n 149 |     'multi_task': len(target_list) > 1,\n 150 |     # 'nn_shape': [2000, [[512],[128],[64]]],\n 151 |     # 'keep_model_path': '/projects/cchen16/FlagshipModelResearch/01_phase2_research/tf_model/test',\n 152 |     # 'week_id':'all',\n 153 |     # 'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/data/TF/segment_feature_v1/',\n 154 | }\n 155 | working_path = '/projects/gds-focus/data/catch/IL/CAM_variable/training_configs'\n 156 | num_workers = 16\n 157 | batch_size = 512\n 158 | sample_size = 6000000\n 159 | int(sample_size*train_test_ratio/(batch_size*num_workers)),int(sample_size*(1-train_test_ratio)/(batch_size*num_workers))\n 160 | num_workers = 16\n 161 | batch_size = 512\n 162 | sample_size = 4000000\n 163 | int(sample_size*train_test_ratio/(batch_size*num_workers)),int(sample_size*(1-train_test_ratio)/(batch_size*num_workers))\n 164 | global_batch_size \n 165 | # ## Specific config\n 166 | seg_names = ['cam24_il_Tot',\n 167 |  'cam24_il_Young',\n 168 |  'cam24_il_Guest',\n 169 |  'cam24_il_Top',\n 170 |  'cam24_il_Casual',\n 171 |  'cam24_il_BrandedXO',\n 172 |  'cam24_il_BrandedRecurring',\n 173 |  'cam24_il_P2PFF',\n 174 |  'cam24_il_P2PGS',\n 175 |  'cam24_il_EbayMor',\n 176 |  'cam24_il_Other',\n 177 |  'cam24_il_MidCAM22Score']\n 178 | for i, name in enumerate(seg_names):\n 179 | def get_candidate_vars(var_path, suffix):\n 180 |     candidate_ls = read_line_file(var_path)\n 181 |     # return [c+suffix for c in candidate_ls]\n 182 |     return [c+suffix+\"_new_var\" for c in candidate_ls]\n 183 | se_files = {\n 184 |     # 'all_seg':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt',\"\"),\n 185 |     'Tot':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/candidates_top500.txt',\"\"),\n 186 |     'recur':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedRecurring/columns/candidates_top500.txt',\"_seg6\"),\n 187 |     'xo':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedXO/columns/candidates_top500.txt',\"_seg5\"),\n 188 |     'other':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Other/columns/candidates_top500.txt',\"_seg10\"),\n 189 |     'p2pff':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PFF/columns/candidates_top500.txt',\"_seg7\"),\n 190 |     'p2pgs':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PGS/columns/candidates_top500.txt',\"_seg8\"),\n 191 |     'ebaymor':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_EbayMor/columns/candidates_top500.txt',\"_seg9\"),\n 192 | }\n 193 | se_files['Tot'][:3]\n 194 | gfs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-gfs\"\n 195 | focus_bucket = \"gs://pypl-bkt-rsh-row-std-gds-focus\"\n 196 | specific_config = {\n 197 |     \"exp8_1\": {\n 198 |         'week_id':'all',\n 199 |         'train_epoch_steps': 585,\n 200 |         'val_epoch_steps': 146,\n 201 |         'train_epochs': 40,\n 202 |         'candidate_vars': candidate_vars + [se_files['Tot']],\n 203 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v6_0926\",\n 204 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240926_recent_data_overall_7217_tf_bad4_25/',\n 205 |     },\n 206 |     \"exp8_3\": {\n 207 |         'week_id':'all',\n 208 |         'train_epoch_steps': 585,\n 209 |         'val_epoch_steps': 146,\n 210 |         'train_epochs': 40,\n 211 |         'candidate_vars': candidate_vars + [se_files['Tot']],\n 212 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v6_0926\",\n 213 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240926_recent_data_overall_7217_tf_bad4_50/',\n 214 |     },\n 215 |     \"exp8_5\": {\n 216 |         'week_id':'all',\n 217 |         'train_epoch_steps': 585,\n 218 |         'val_epoch_steps': 146,\n 219 |         'train_epochs': 40,\n 220 |         'candidate_vars': candidate_vars + [se_files['Tot']],\n 221 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v6_0926\",\n 222 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240926_recent_data_overall_7217_tf_bad4_75/',\n 223 |     },\n 224 | }\n 225 | # !gsutil -m cp -r /projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/cam_il_new_var_logit_v6_0926 gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/\n 226 | # ### Local debug\n 227 | import logging\n 228 | logging.basicConfig(stream=sys.stdout, level='INFO')\n 229 | logger = logging.getLogger('Train')\n 230 | strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n 231 | exp_trainer_all(tmp_config)\n 232 | # !ls -tl $working_path\n 233 | working_path\n 234 | md = \"0116\"\n 235 | exp_idx = 'exp8'\n 236 | t =os.path.join(working_path,f'{exp_idx}_job_ids_{md}.json')\n 237 | if os.path.exists(t):\n 238 |     with open(t, 'r') as f:\n 239 |         job_ids = json.load(f)\n 240 | specific_config.keys()\n 241 | job_ids = {}\n 242 | run_ls = ['exp8_1', 'exp8_3', 'exp8_5']\n 243 | for exp_id in run_ls:\n 244 |     week_id = specific_config[exp_id][\"week_id\"]\n 245 |     tmp_config = copy.deepcopy(model_config)  \n 246 |     tmp_config['week_id'] = str(week_id)\n 247 |     for k, v in specific_config[exp_id].items():\n 248 |         if k not in {\"week_id\", \"data_path\"}:\n 249 |             tmp_config[k] = v\n 250 |     data = specific_config[exp_id][\"data_path\"]\n 251 |     tmp_config['previous_model_path'] = specific_config[exp_id][\"previous_model_path\"]\n 252 |     all_files = tf.io.gfile.glob(data+'*.gz')\n 253 |     dev_files, val_files = train_test_split(all_files, test_size=0.2, shuffle=True)\n 254 |     tmp_config['dev_files']=dev_files\n 255 |     tmp_config['val_files']=val_files\n 256 |     tmp_config['keep_model_path'] = f'{focus_bucket}/user/cchen16/IL_variable/{md}_{exp_idx}/{exp_id}_gcp_loss/week_'\n 257 |     # exp_trainer_all(tmp_config)\n 258 |     job_id = client.create_job(\n 259 |                 func = exp_trainer_all,\n 260 |                 pyfiles_to_import=[\n 261 |                                     '/projects/gds-focus/data/catch/IL/CAM_variable/assets/sf_il_utils.py',\n 262 |                                     '/projects/cchen16/FlagshipModelResearch/01_phase2_research/il_utils/cc_functions.py',\n 263 |                                   ], \n 264 |                 num_workers = num_workers,\n 265 |                 machine_type_per_worker = \"n1-highmem-16\",\n 266 |                 # machine_type_per_worker = \"n2d-highmem-8\",\n 267 |                 model_config = tmp_config\n 268 |                 )\n 269 |     model_name = f'{exp_id}_week{week_id}'\n 270 |     job_ids[model_name] = job_id\n 271 |     tmp_config['job_id'] = job_id\n 272 |     with open(os.path.join(working_path,f'{model_name}_{md}.json'),'w') as f:\n 273 |         json.dump(tmp_config,f)\n 274 | with open(t,'w') as f:\n 275 |     json.dump(job_ids, f)\n 276 | # client.cancel_job('maglev/cloud/dataproc/job/gds-focus-3d570d/2aa7536c-25bb-35f9-9590-bb70afaa76fd')\n 277 | # ## Tracking\n 278 | def pretty_model_config(configs):\n 279 |     for k, v in configs.items():\n 280 |         if k not in {\"candidate_vars\", \"dev_files\", \"val_files\"}:\n 281 |         if k in {\"dev_files\"}:\n 282 |         if k in {\"candidate_vars\"}:\n 283 |             candidate_len = []\n 284 |             for ls in v:\n 285 |                 candidate_len.append(len(ls))\n 286 |     return\n 287 | def check_job_state(job_ids):\n 288 |     for k,v in job_ids.items():\n 289 | def cancel_job(job_ids):\n 290 |     for k,v in job_ids.items():\n 291 | # cancel_job(job_ids)\n 292 | week_id = 'all'\n 293 | model_name = f'{exp_id}_week{week_id}'\n 294 | config_path = os.path.join(working_path,f'{model_name}_{md}.json')\n 295 | with open(os.path.join(working_path,f'{model_name}_{md}.json'),'r') as f:\n 296 |     config = json.load(f)\n 297 | pretty_model_config(config)\n 298 | job_ids\n 299 | check_job_state(job_ids)\n 300 | client.wait_job_for_completion(job_ids['exp8_1_weekall'],\n 301 |                                time_to_sleep=300)\n 302 | # # %%capture job_log  \n 303 | client.get_job_driver_logs(job_ids['exp8_1_weekall'])\n 304 | import re\n 305 | log = \"\"\"\n 306 | tf.math.sigmoid_loss: 0.6841 - tf.math.sigmoid_5_loss: 0.9473 - tf.math.sigmoid_recall: 0.3386 - tf.math.sigmoid_precision: 0.6501 - tf.math.sigmoid_5_recall: 0.5958 - tf.math.sigmoid_5_precision: 0.5299 - val_loss: 0.6088 - val_tf.math.sigmoid_loss: 0.6763 - val_tf.math.sigmoid_5_loss: 0.6088 - val_tf.math.sigmoid_recall: 0.3230 - val_tf.math.sigmoid_precision: 0.6918 - val_tf.math.sigmoid_5_recall: 0.5891 - val_tf.math.sigmoid_5_precision: 0.6756\n 307 | \"\"\"\n 308 | pattern = r'(\\w+[\\.\\w]*)\\:\\s*([\\d\\.\\-]+)'  \n 309 | metrics = re.findall(pattern, log)  \n 310 | metrics_dict = dict(metrics)  \n 311 | for key, value in metrics_dict.items(): \n 312 | # !gsutil -m cp -r gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/1120_exp1/exp1_1_gcp_loss /projects/cchen16/FlagshipModelResearch/01_phase2_research/tensorboard/\n 313 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/0930_exp1/*/week_all\n 314 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/0924_exp1/exp1_2_gcp_loss/week_all/\n 315 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0116_exp8/exp8_*_gcp_loss/week_all/ | grep 'model.15'\n 316 | # ## Check config and log\n 317 | exp_id = 'exp3_7'\n 318 | week_id = 'all'\n 319 | model_name = f'{exp_id}_week{week_id}'\n 320 | config_path = os.path.join(working_path,f'{model_name}_{md}.json')\n 321 | with open(os.path.join(working_path,f'{model_name}_{md}.json'),'r') as f:\n 322 |     config = json.load(f)\n 323 | pretty_model_config(config)\n 324 | with open()\n 325 | client.get_job_driver_logs(job_id)\n 326 | log_file = f\"/projects/gds-focus/data/catch/IL/CAM_variable/training_logs/{model_name}.log\"\n 327 | with open(log_file, \"a\") as f:\n 328 |     f.write(job_ids[model_name])\n 329 |     f.write(\"**\"*50)\n 330 |     client.get_job_driver_logs(gcp_job_id, file=f)\n 331 | import re\n 332 | def extract_metric(log_path):\n 333 |     with open(log_path, 'r') as f:\n 334 |         log = f.read()\n 335 |     pattern = r'(\\w+[\\.\\w]*)\\:\\s*([\\d\\.\\-]+)'  \n 336 |     metrics = re.findall(pattern, log)  \n 337 |     metrics_dict = dict(metrics)  \n 338 |     for key, value in metrics_dict.items():  \n 339 |     return\n 340 | extract_metric(log_file)\n 341 | set(se_files['all_seg'])-set(config['candidate_vars'][3])\n 342 | client.get_job_application_logs(config['job_id'])\n 343 | 1024*1800\n 344 | client.get_job_driver_logs('maglev/cloud/dataproc/job/gds-focus-5aa772/927ee8e9-2e74-3539-ad7b-463e1126e952')\n 345 | # ### check tensorboard\n 346 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0110_exp6/exp6_7_gcp_loss/week_all/\n 347 | # !gsutil cp -r gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0110_exp6/exp6_7_gcp_loss/week_all/week-all-tensorboard ./",
    "rmr_agent/repos/CAM_variable_research/3_6_gcp_training.py": "   1 | # # import prepare\n   2 | # %ppauth\n   3 | import tensorflow as tf\n   4 | import sys,os\n   5 | import aml.cloud_v1 as cloud\n   6 | import tensorboard\n   7 | client = cloud.DataProcClient(gcp_project='ccg24-hrzana-gds-focus')\n   8 | from model_automation.gcp import dataproc_config\n   9 | from aml import cloud_v1 as cloud\n  10 | from tqdm import tqdm\n  11 | from sklearn.model_selection import train_test_split\n  12 | import pandas as pd\n  13 | import json\n  14 | import copy\n  15 | from model_automation.utils.rmr.helper import run_cmd\n  16 | util_path = '/projects/cchen16/FlagshipModelResearch/01_phase2_research/il_utils'\n  17 | sys.path.append(util_path)\n  18 | sys.path.append('/projects/gds-focus/data/catch/IL/CAM_variable/assets')\n  19 | from helper import *\n  20 | from sf_il_utils import custom_loss, binary_focal_loss\n  21 | import importlib\n  22 | from sf_il_utils import global_data_prepare_for_ensemble as data_prepare\n  23 | tf.__version__\n  24 | # !nvidia-smi\n  25 | # # exp trainer define\n  26 | def exp_trainer_all(model_config):\n  27 |     import sys\n  28 |     import logging\n  29 |     import random    \n  30 |     import os\n  31 |     import sys   \n  32 |     import numpy as np\n  33 |     import tensorflow as tf\n  34 |     from tensorflow import keras\n  35 |     from tensorflow.keras.models import Sequential\n  36 |     from tensorflow.keras.layers import Dense, Dropout\n  37 |     from tensorflow.keras import optimizers\n  38 |     from tensorflow.python.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, TensorBoard\n  39 |     from tensorflow.keras import backend as K, Sequential, Model, layers\n  40 |     from tensorflow.keras.utils import get_custom_objects\n  41 |     # from junrui_utils import global_data_prepare_for_ensemble as data_prepare    \n  42 |     from sf_il_utils import global_data_prepare_for_ensemble as data_prepare\n  43 |     from sf_il_utils import custom_loss, binary_focal_loss\n  44 |     from cc_functions import create_single_pepnet, create_mlp_model, create_cascade_model, create_cascade_model_v2\n  45 |     K.set_floatx('float32')\n  46 |     K.set_epsilon(1e-16)\n  47 |     logging.basicConfig(stream=sys.stdout, level='INFO')\n  48 |     logger = logging.getLogger('Train')\n  49 |     strategy = tf.distribute.MultiWorkerMirroredStrategy()\n  50 |     from tensorflow.keras import metrics\n  51 |     from sklearn.model_selection import train_test_split\n  52 |     with strategy.scope():\n  53 |         model_seg =tf.keras.models.load_model(model_config['previous_model_path'])\n  54 |         model_seg.summary()\n  55 |         # increasing lr schedule\n  56 |         custom_objects = {\"custom_loss\": custom_loss,\n  57 |                           \"focal_loss\": binary_focal_loss(gamma=2., alpha=.25),\n  58 |                           \"binary_crossentropy\":\"binary_crossentropy\"}\n  59 |         with keras.utils.custom_object_scope(custom_objects):\n  60 |             model_seg.compile(tf.keras.optimizers.Adam(model_config['lr']),\n  61 |                           loss=[custom_objects[f] for f in model_config['loss_functions']],\n  62 |                           loss_weights=model_config['loss_list'],\n  63 |                           metrics=[\n  64 |                                    metrics.Recall(thresholds=0.1,name='recall'), \n  65 |                                    metrics.Precision(thresholds=0.1,name='precision')\n  66 |                                   ],\n  67 |                          )\n  68 |         prepare = data_prepare()\n  69 |         dev_fl = tf.io.gfile.glob(model_config['dev_files'])\n  70 |         val_fl = tf.io.gfile.glob(model_config['val_files'])\n  71 |         dev_dataset = prepare.make_dataset(filenames = dev_fl, \n  72 |                                             feature_list = model_config['candidate_vars'], \n  73 |                                             target_list = model_config['target_list'], \n  74 |                                             weight_list = model_config['weight_list'], \n  75 |                                             batch_size = model_config['global_batch_size'], \n  76 |                                             buffer_size = model_config['buffer_size'],\n  77 |                                             multi_task = model_config['multi_task'], \n  78 |                                             compress = 'GZIP',\n  79 |                                           )    \n  80 |         val_dataset = prepare.make_dataset(filenames = val_fl, \n  81 |                                             feature_list = model_config['candidate_vars'], \n  82 |                                             target_list = model_config['target_list'], \n  83 |                                             weight_list = model_config['weight_list'], \n  84 |                                             batch_size = model_config['global_batch_size'], \n  85 |                                             buffer_size = model_config['buffer_size'],\n  86 |                                             multi_task = model_config['multi_task'],\n  87 |                                             compress = 'GZIP',\n  88 |                                           )\n  89 |         callbacks_list = [\n  90 |         EarlyStopping(monitor='val_loss', patience=model_config['patience'], verbose=2, mode='min',restore_best_weights=True),\n  91 |         ModelCheckpoint(filepath=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'model.{epoch:02d}-{loss:.8f}', \\\n  92 |                         monitor='val_loss',mode='min',save_freq='epoch',save_best_only=False),\n  93 |         ModelCheckpoint(filepath=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'week-'+str(model_config['week_id'])+'-best_model', \\\n  94 |                         monitor='val_loss',mode='min',save_freq='epoch',save_best_only=True),\n  95 |         TensorBoard(log_dir=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'week-'+str(model_config['week_id'])+'-tensorboard', \\\n  96 |                            histogram_freq=1,batch_size=model_config['global_batch_size'],write_graph=False,\n  97 |                            write_grads=False,write_images=False,update_freq='batch')\n  98 |         ]\n  99 |         dev_dataset = strategy.experimental_distribute_dataset(dev_dataset)\n 100 |         val_dataset = strategy.experimental_distribute_dataset(val_dataset)\n 101 |     model_seg.fit(dev_dataset,\n 102 |                   epochs=model_config['train_epochs'], # model_config['train_epochs'],\n 103 |                   verbose=2,\n 104 |                   shuffle=True,\n 105 |                   validation_data=val_dataset,\n 106 |                   steps_per_epoch=model_config['train_epoch_steps'],  # 10000,  total dev records/num_works/batch size\n 107 |                   validation_steps=model_config['val_epoch_steps'],\n 108 |                   callbacks=callbacks_list)\n 109 |     return\n 110 | gfs_gcs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-gfs\"\n 111 | focus_gcs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-focus\" \n 112 | num_workers = 16\n 113 | batch_size = 512\n 114 | sample_size = 8000000\n 115 | target_list = ['driver_is_brm3_4_11_15_bad','driver_is_brm3_4_11_15_bad']\n 116 | weight_list = ['driver_train_wgt_3','driver_train_wgt_3']\n 117 | global_batch_size = batch_size*num_workers\n 118 | train_test_ratio = 0.8\n 119 | train_epochs = 30\n 120 | import pandas as pd\n 121 | candidate_vars = [list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 122 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/robust_tf_input.txt',sep='|').columns),\n 123 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 124 |                   # read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt\")\n 125 |                  ]\n 126 | candidate_vars[0] = [item+\"_adp\" for item in candidate_vars[0]]\n 127 | candidate_vars[1] = [item+\"_rob\" for item in candidate_vars[1]]\n 128 | candidate_vars[2] = [item+\"_adp\" for item in candidate_vars[2]]\n 129 | # candidate_vars[3] = [item for item in candidate_vars[3]]\n 130 | model_config = {\n 131 |     'candidate_vars': candidate_vars,\n 132 |     'target_list': target_list,\n 133 |     'weight_list': weight_list,\n 134 |     'loss_list':[0.0, 1.0],\n 135 |     'dev_files': None,   \n 136 |     'val_files': None,\n 137 |     'global_batch_size': global_batch_size,\n 138 |     'buffer_size': 1,\n 139 |     'train_epoch_steps': int(sample_size*train_test_ratio/(batch_size*num_workers)), ## \u9700\u8981\u8ba1\u7b97\u539f\u59cb\u6570\u636e\u6570\u91cf n\uff0ctrain\u2014\u2014epoch-steps=n/global_batch_size/num_workers  n \u6309\u71671600\u4e07\u6765\u8ba1\u7b97\n 140 |     'val_epoch_steps': int(sample_size*(1-train_test_ratio)/(batch_size*num_workers)), ##\u540c\u4e0a\n 141 |     # 'train_epoch_steps': 200,\n 142 |     # 'val_epoch_steps': 100,\n 143 |     'train_epochs': train_epochs,\n 144 |     'lr':1e-3,\n 145 |     'loss_functions': ['binary_crossentropy', 'binary_crossentropy'],\n 146 |     'week_id': 1,\n 147 |     'job_id': None,\n 148 |     'patience': 10,\n 149 |     'multi_task': len(target_list) > 1,\n 150 |     # 'nn_shape': [2000, [[512],[128],[64]]],\n 151 |     # 'keep_model_path': '/projects/cchen16/FlagshipModelResearch/01_phase2_research/tf_model/test',\n 152 |     # 'week_id':'all',\n 153 |     # 'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/data/TF/segment_feature_v1/',\n 154 | }\n 155 | working_path = '/projects/gds-focus/data/catch/IL/CAM_variable/training_configs'\n 156 | num_workers = 4\n 157 | batch_size = 256\n 158 | sample_size = 800000\n 159 | int(sample_size*train_test_ratio/(batch_size*num_workers)),int(sample_size*(1-train_test_ratio)/(batch_size*num_workers))\n 160 | global_batch_size \n 161 | # ## Specific config\n 162 | seg_names = ['cam24_il_Tot',\n 163 |  'cam24_il_Young',\n 164 |  'cam24_il_Guest',\n 165 |  'cam24_il_Top',\n 166 |  'cam24_il_Casual',\n 167 |  'cam24_il_BrandedXO',\n 168 |  'cam24_il_BrandedRecurring',\n 169 |  'cam24_il_P2PFF',\n 170 |  'cam24_il_P2PGS',\n 171 |  'cam24_il_EbayMor',\n 172 |  'cam24_il_Other',\n 173 |  'cam24_il_MidCAM22Score']\n 174 | for i, name in enumerate(seg_names):\n 175 | def get_candidate_vars(var_path, suffix):\n 176 |     candidate_ls = read_line_file(var_path)\n 177 |     return [c+suffix for c in candidate_ls]\n 178 | se_files = {\n 179 |     # 'all_seg':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt',\"\"),\n 180 |     'Tot':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/candidates_top500.txt',\"\"),\n 181 |     'recur':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedRecurring/columns/candidates_top500.txt',\"_seg6\"),\n 182 |     'xo':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedXO/columns/candidates_top500.txt',\"_seg5\"),\n 183 |     'other':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Other/columns/candidates_top500.txt',\"_seg10\"),\n 184 |     'p2pff':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PFF/columns/candidates_top500.txt',\"_seg7\"),\n 185 |     'p2pgs':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PGS/columns/candidates_top500.txt',\"_seg8\"),\n 186 |     'ebaymor':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_EbayMor/columns/candidates_top500.txt',\"_seg9\"),\n 187 | }\n 188 | se_files.keys()\n 189 | se_files.values\n 190 | len(sum((candidate_vars + list(se_files.values())),[])), len(sum(list(se_files.values()), []))\n 191 | 4000+1717+500*7\n 192 | gfs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-gfs\"\n 193 | focus_bucket = \"gs://pypl-bkt-rsh-row-std-gds-focus\"\n 194 | specific_config = {\n 195 |     \"exp7_1\": {\n 196 |         'week_id':'all',\n 197 |         'candidate_vars': candidate_vars + list(se_files.values()),\n 198 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/CAM24_IL_New_Var_add_attention_v3\",\n 199 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_overall_7217_tf/',\n 200 |     },\n 201 |      \"exp7_2\": {\n 202 |         'week_id':'all',\n 203 |         'candidate_vars': candidate_vars + list(se_files.values()),\n 204 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/CAM24_IL_New_Var_add_gate_v1\",\n 205 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_overall_7217_tf/',\n 206 |     },\n 207 |     \"exp7_3\": {\n 208 |         'week_id':'all',\n 209 |         'candidate_vars': candidate_vars + list(se_files.values()),\n 210 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/CAM24_IL_New_Var_add_gate_v2\",\n 211 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_overall_7217_tf/',\n 212 |     },\n 213 | }\n 214 | # !gsutil -m cp -r /projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/CAM24_IL_New_Var_add_gate_v2 gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/\n 215 | # ### Local debug\n 216 | import logging\n 217 | logging.basicConfig(stream=sys.stdout, level='INFO')\n 218 | logger = logging.getLogger('Train')\n 219 | strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n 220 | 4096/128,512/16\n 221 | exp_trainer_all(tmp_config)\n 222 | # !ls -tl $working_path\n 223 | working_path\n 224 | md = \"0110\"\n 225 | exp_idx = 'exp7'\n 226 | t =os.path.join(working_path,f'{exp_idx}_job_ids_{md}.json')\n 227 | if os.path.exists(t):\n 228 |     with open(t, 'r') as f:\n 229 |         job_ids = json.load(f)\n 230 | job_ids\n 231 | run_ls = ['exp7_2']\n 232 | for exp_id in run_ls:\n 233 |     week_id = specific_config[exp_id][\"week_id\"]\n 234 |     tmp_config = copy.deepcopy(model_config)  \n 235 |     tmp_config['week_id'] = str(week_id)\n 236 |     for k, v in specific_config[exp_id].items():\n 237 |         if k not in {\"week_id\", \"data_path\"}:\n 238 |             tmp_config[k] = v\n 239 |     data = specific_config[exp_id][\"data_path\"]\n 240 |     tmp_config['previous_model_path'] = specific_config[exp_id][\"previous_model_path\"]\n 241 |     all_files = tf.io.gfile.glob(data+'*.gz')\n 242 |     dev_files, val_files = train_test_split(all_files, test_size=0.2, shuffle=True)\n 243 |     tmp_config['dev_files']=dev_files\n 244 |     tmp_config['val_files']=val_files\n 245 |     tmp_config['keep_model_path'] = f'{focus_bucket}/user/cchen16/IL_variable/{md}_{exp_idx}/{exp_id}_gcp_loss/week_'\n 246 | #     job_id = client.create_job(\n 247 | #                 func = exp_trainer_all,\n 248 | #                 pyfiles_to_import=[\n 249 | #                                     '/projects/gds-focus/data/catch/IL/CAM_variable/assets/sf_il_utils.py',\n 250 | #                                     '/projects/cchen16/FlagshipModelResearch/01_phase2_research/il_utils/cc_functions.py',\n 251 | #                                   ], \n 252 | #                 num_workers = num_workers,\n 253 | #                 machine_type_per_worker = \"n1-highmem-16\",\n 254 | #                 # machine_type_per_worker = \"n2d-highmem-8\",\n 255 | #                 model_config = tmp_config\n 256 | #                 )\n 257 | #     model_name = f'{exp_id}_week{week_id}'\n 258 | #     job_ids[model_name] = job_id\n 259 | #     print(f'{model_name} DataProc Job Submitted')\n 260 | #     print('Job ID \\n', job_id)\n 261 | #     tmp_config['job_id'] = job_id\n 262 | #     with open(os.path.join(working_path,f'{model_name}_{md}.json'),'w') as f:\n 263 | #         json.dump(tmp_config,f)\n 264 | # with open(t,'w') as f:\n 265 | #     json.dump(job_ids, f)\n 266 | # client.cancel_job('maglev/cloud/dataproc/job/gds-focus-3d570d/2aa7536c-25bb-35f9-9590-bb70afaa76fd')\n 267 | # ## Tracking\n 268 | def pretty_model_config(configs):\n 269 |     for k, v in configs.items():\n 270 |         if k not in {\"candidate_vars\", \"dev_files\", \"val_files\"}:\n 271 |         if k in {\"dev_files\"}:\n 272 |         if k in {\"candidate_vars\"}:\n 273 |             candidate_len = []\n 274 |             for ls in v:\n 275 |                 candidate_len.append(len(ls))\n 276 |     return\n 277 | def check_job_state(job_ids):\n 278 |     for k,v in job_ids.items():\n 279 | def cancel_job(job_ids):\n 280 |     for k,v in job_ids.items():\n 281 | week_id = 'all'\n 282 | model_name = f'{exp_id}_week{week_id}'\n 283 | config_path = os.path.join(working_path,f'{model_name}_{md}.json')\n 284 | with open(os.path.join(working_path,f'{model_name}_{md}.json'),'r') as f:\n 285 |     config = json.load(f)\n 286 | pretty_model_config(config)\n 287 | job_ids\n 288 | check_job_state(job_ids)\n 289 | client.wait_job_for_completion(job_ids['exp7_2_weekall'],\n 290 |                                time_to_sleep=300)\n 291 | # # %%capture job_log  \n 292 | client.get_job_driver_logs(job_ids['exp7_1_weekall'])\n 293 | import re\n 294 | log = \"\"\"\n 295 | tf.math.sigmoid_loss: 0.6841 - tf.math.sigmoid_5_loss: 0.9473 - tf.math.sigmoid_recall: 0.3386 - tf.math.sigmoid_precision: 0.6501 - tf.math.sigmoid_5_recall: 0.5958 - tf.math.sigmoid_5_precision: 0.5299 - val_loss: 0.6088 - val_tf.math.sigmoid_loss: 0.6763 - val_tf.math.sigmoid_5_loss: 0.6088 - val_tf.math.sigmoid_recall: 0.3230 - val_tf.math.sigmoid_precision: 0.6918 - val_tf.math.sigmoid_5_recall: 0.5891 - val_tf.math.sigmoid_5_precision: 0.6756\n 296 | \"\"\"\n 297 | pattern = r'(\\w+[\\.\\w]*)\\:\\s*([\\d\\.\\-]+)'  \n 298 | metrics = re.findall(pattern, log)  \n 299 | metrics_dict = dict(metrics)  \n 300 | for key, value in metrics_dict.items(): \n 301 | # !gsutil -m cp -r gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/1120_exp1/exp1_1_gcp_loss /projects/cchen16/FlagshipModelResearch/01_phase2_research/tensorboard/\n 302 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/0930_exp1/*/week_all\n 303 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/0924_exp1/exp1_2_gcp_loss/week_all/\n 304 | # ## Check config and log\n 305 | exp_id = 'exp3_7'\n 306 | week_id = 'all'\n 307 | model_name = f'{exp_id}_week{week_id}'\n 308 | config_path = os.path.join(working_path,f'{model_name}_{md}.json')\n 309 | with open(os.path.join(working_path,f'{model_name}_{md}.json'),'r') as f:\n 310 |     config = json.load(f)\n 311 | pretty_model_config(config)\n 312 | with open()\n 313 | client.get_job_driver_logs(job_id)\n 314 | log_file = f\"/projects/gds-focus/data/catch/IL/CAM_variable/training_logs/{model_name}.log\"\n 315 | with open(log_file, \"a\") as f:\n 316 |     f.write(job_ids[model_name])\n 317 |     f.write(\"**\"*50)\n 318 |     client.get_job_driver_logs(gcp_job_id, file=f)\n 319 | import re\n 320 | def extract_metric(log_path):\n 321 |     with open(log_path, 'r') as f:\n 322 |         log = f.read()\n 323 |     pattern = r'(\\w+[\\.\\w]*)\\:\\s*([\\d\\.\\-]+)'  \n 324 |     metrics = re.findall(pattern, log)  \n 325 |     metrics_dict = dict(metrics)  \n 326 |     for key, value in metrics_dict.items():  \n 327 |     return\n 328 | extract_metric(log_file)\n 329 | set(se_files['all_seg'])-set(config['candidate_vars'][3])\n 330 | client.get_job_application_logs(config['job_id'])\n 331 | 1024*1800\n 332 | client.get_job_driver_logs('maglev/cloud/dataproc/job/gds-focus-5aa772/927ee8e9-2e74-3539-ad7b-463e1126e952')\n 333 | # ## Check tensorboard\n 334 | # !rm ./week-all-tensorboard/train/.nfs00da000000371eb9000004d4\n 335 | # !rm -r ./week-all-tensorboard\n 336 | # !gsutil -m cp -r gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0110_exp7/exp7_1_gcp_loss/week_all/week-all-tensorboard ./",
    "rmr_agent/repos/CAM_variable_research/3_7_gcp_training.py": "   1 | # # import prepare\n   2 | # %ppauth\n   3 | import tensorflow as tf\n   4 | import sys,os\n   5 | import aml.cloud_v1 as cloud\n   6 | import tensorboard\n   7 | client = cloud.DataProcClient(gcp_project='ccg24-hrzana-gds-focus')\n   8 | from model_automation.gcp import dataproc_config\n   9 | from aml import cloud_v1 as cloud\n  10 | from tqdm import tqdm\n  11 | from sklearn.model_selection import train_test_split\n  12 | import pandas as pd\n  13 | import json\n  14 | import copy\n  15 | from model_automation.utils.rmr.helper import run_cmd\n  16 | util_path = '/projects/cchen16/FlagshipModelResearch/01_phase2_research/il_utils'\n  17 | sys.path.append(util_path)\n  18 | sys.path.append('/projects/gds-focus/data/catch/IL/CAM_variable/assets')\n  19 | from helper import *\n  20 | from sf_il_utils import custom_loss, binary_focal_loss\n  21 | import importlib\n  22 | from sf_il_utils import global_data_prepare_for_ensemble as data_prepare\n  23 | tf.__version__\n  24 | # !nvidia-smi\n  25 | # # exp trainer define\n  26 | def exp_trainer_all(model_config):\n  27 |     import sys\n  28 |     import logging\n  29 |     import random    \n  30 |     import os\n  31 |     import sys   \n  32 |     import numpy as np\n  33 |     import tensorflow as tf\n  34 |     from tensorflow import keras\n  35 |     from tensorflow.keras.models import Sequential\n  36 |     from tensorflow.keras.layers import Dense, Dropout\n  37 |     from tensorflow.keras import optimizers\n  38 |     from tensorflow.python.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, TensorBoard\n  39 |     from tensorflow.keras import backend as K, Sequential, Model, layers\n  40 |     from tensorflow.keras.utils import get_custom_objects\n  41 |     # from junrui_utils import global_data_prepare_for_ensemble as data_prepare    \n  42 |     from sf_il_utils import global_data_prepare_for_ensemble as data_prepare\n  43 |     from sf_il_utils import custom_loss, binary_focal_loss, custom_penalty_loss\n  44 |     from cc_functions import create_single_pepnet, create_mlp_model, create_cascade_model, create_cascade_model_v2\n  45 |     K.set_floatx('float32')\n  46 |     K.set_epsilon(1e-16)\n  47 |     logging.basicConfig(stream=sys.stdout, level='INFO')\n  48 |     logger = logging.getLogger('Train')\n  49 |     strategy = tf.distribute.MultiWorkerMirroredStrategy()\n  50 |     from tensorflow.keras import metrics\n  51 |     from sklearn.model_selection import train_test_split\n  52 |     with strategy.scope():\n  53 |         model_seg =tf.keras.models.load_model(model_config['previous_model_path'])\n  54 |         model_seg.summary()\n  55 |         # increasing lr schedule\n  56 |         custom_objects = {\"penalty_loss\": custom_penalty_loss(pos_scale=model_config['pos_scale'], neg_scale=model_config['neg_scale']),\n  57 |                           \"focal_loss\": binary_focal_loss(gamma=2., alpha=.25),\n  58 |                           \"binary_crossentropy\":\"binary_crossentropy\"}\n  59 |         with keras.utils.custom_object_scope(custom_objects):\n  60 |             model_seg.compile(tf.keras.optimizers.Adam(model_config['lr']),\n  61 |                           loss=[custom_objects[f] for f in model_config['loss_functions']],\n  62 |                           loss_weights=model_config['loss_list'],\n  63 |                           metrics=[metrics.AUC(name='auc'),\n  64 |                                    metrics.Recall(thresholds=0.1,name='recall'), \n  65 |                                    metrics.Precision(thresholds=0.1,name='precision')\n  66 |                                   ],\n  67 |                          )\n  68 |         prepare = data_prepare()\n  69 |         dev_fl = tf.io.gfile.glob(model_config['dev_files'])\n  70 |         val_fl = tf.io.gfile.glob(model_config['val_files'])\n  71 |         dev_dataset = prepare.make_dataset(filenames = dev_fl, \n  72 |                                             feature_list = model_config['candidate_vars'], \n  73 |                                             target_list = model_config['target_list'], \n  74 |                                             weight_list = model_config['weight_list'], \n  75 |                                             batch_size = model_config['global_batch_size'], \n  76 |                                             buffer_size = model_config['buffer_size'],\n  77 |                                             multi_task = model_config['multi_task'], \n  78 |                                             compress = 'GZIP',\n  79 |                                           )    \n  80 |         val_dataset = prepare.make_dataset(filenames = val_fl, \n  81 |                                             feature_list = model_config['candidate_vars'], \n  82 |                                             target_list = model_config['target_list'], \n  83 |                                             weight_list = model_config['weight_list'], \n  84 |                                             batch_size = model_config['global_batch_size'], \n  85 |                                             buffer_size = model_config['buffer_size'],\n  86 |                                             multi_task = model_config['multi_task'],\n  87 |                                             compress = 'GZIP',\n  88 |                                           )\n  89 |         callbacks_list = [\n  90 |         EarlyStopping(monitor='val_loss', patience=model_config['patience'], verbose=2, mode='min',restore_best_weights=True),\n  91 |         ModelCheckpoint(filepath=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'model.{epoch:02d}-{loss:.8f}', \\\n  92 |                         monitor='val_loss',mode='min',save_freq='epoch',save_best_only=False),\n  93 |         ModelCheckpoint(filepath=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'week-'+str(model_config['week_id'])+'-best_model', \\\n  94 |                         monitor='val_loss',mode='min',save_freq='epoch',save_best_only=True),\n  95 |         TensorBoard(log_dir=model_config['keep_model_path']+str(model_config['week_id'])+'/'+'week-'+str(model_config['week_id'])+'-tensorboard', \\\n  96 |                            histogram_freq=1,batch_size=model_config['global_batch_size'],write_graph=False,\n  97 |                            write_grads=False,write_images=False,update_freq='batch')\n  98 |         ]\n  99 |         dev_dataset = strategy.experimental_distribute_dataset(dev_dataset)\n 100 |         val_dataset = strategy.experimental_distribute_dataset(val_dataset)\n 101 |     model_seg.fit(dev_dataset,\n 102 |                   epochs=model_config['train_epochs'], # model_config['train_epochs'],\n 103 |                   verbose=2,\n 104 |                   shuffle=True,\n 105 |                   validation_data=val_dataset,\n 106 |                   steps_per_epoch=model_config['train_epoch_steps'],  # 10000,  total dev records/num_works/batch size\n 107 |                   validation_steps=model_config['val_epoch_steps'],\n 108 |                   callbacks=callbacks_list)\n 109 |     return\n 110 | gfs_gcs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-gfs\"\n 111 | focus_gcs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-focus\" \n 112 | num_workers = 16\n 113 | batch_size = 512\n 114 | sample_size = 6000000\n 115 | target_list = ['driver_is_brm3_4_11_15_bad','driver_is_brm3_4_11_15_bad']\n 116 | weight_list = ['driver_train_wgt_3','driver_train_wgt_3']\n 117 | global_batch_size = batch_size*num_workers\n 118 | train_test_ratio = 0.8\n 119 | train_epochs = 10\n 120 | import pandas as pd\n 121 | candidate_vars = [list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 122 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/robust_tf_input.txt',sep='|').columns),\n 123 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 124 |                   # read_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt\")\n 125 |                  ]\n 126 | candidate_vars[0] = [item+\"_adp\" for item in candidate_vars[0]]\n 127 | candidate_vars[1] = [item+\"_rob\" for item in candidate_vars[1]]\n 128 | candidate_vars[2] = [item+\"_adp\" for item in candidate_vars[2]]\n 129 | # candidate_vars[3] = [item for item in candidate_vars[3]]\n 130 | model_config = {\n 131 |     'candidate_vars': candidate_vars,\n 132 |     'target_list': target_list,\n 133 |     'weight_list': weight_list,\n 134 |     'loss_list':[0.0, 1.0],\n 135 |     'dev_files': None,   \n 136 |     'val_files': None,\n 137 |     'global_batch_size': global_batch_size,\n 138 |     'buffer_size': 1,\n 139 |     'train_epoch_steps': int(sample_size*train_test_ratio/(batch_size*num_workers)), ## \u9700\u8981\u8ba1\u7b97\u539f\u59cb\u6570\u636e\u6570\u91cf n\uff0ctrain\u2014\u2014epoch-steps=n/global_batch_size/num_workers  n \u6309\u71671600\u4e07\u6765\u8ba1\u7b97\n 140 |     'val_epoch_steps': int(sample_size*(1-train_test_ratio)/(batch_size*num_workers)), ##\u540c\u4e0a\n 141 |     # 'train_epoch_steps': 200,\n 142 |     # 'val_epoch_steps': 100,\n 143 |     'train_epochs': train_epochs,\n 144 |     'lr':1e-3,\n 145 |     'loss_functions': ['binary_crossentropy', 'binary_crossentropy'],\n 146 |     'week_id': 1,\n 147 |     'job_id': None,\n 148 |     'patience': 10,\n 149 |     'multi_task': len(target_list) > 1,\n 150 |     # 'nn_shape': [2000, [[512],[128],[64]]],\n 151 |     # 'keep_model_path': '/projects/cchen16/FlagshipModelResearch/01_phase2_research/tf_model/test',\n 152 |     # 'week_id':'all',\n 153 |     # 'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/data/TF/segment_feature_v1/',\n 154 | }\n 155 | working_path = '/projects/gds-focus/data/catch/IL/CAM_variable/training_configs'\n 156 | num_workers = 16\n 157 | batch_size = 512\n 158 | sample_size = 6000000\n 159 | int(sample_size*train_test_ratio/(batch_size*num_workers)),int(sample_size*(1-train_test_ratio)/(batch_size*num_workers))\n 160 | num_workers = 16\n 161 | batch_size = 512\n 162 | sample_size = 4000000\n 163 | int(sample_size*train_test_ratio/(batch_size*num_workers)),int(sample_size*(1-train_test_ratio)/(batch_size*num_workers))\n 164 | global_batch_size \n 165 | # ## Specific config\n 166 | seg_names = ['cam24_il_Tot',\n 167 |  'cam24_il_Young',\n 168 |  'cam24_il_Guest',\n 169 |  'cam24_il_Top',\n 170 |  'cam24_il_Casual',\n 171 |  'cam24_il_BrandedXO',\n 172 |  'cam24_il_BrandedRecurring',\n 173 |  'cam24_il_P2PFF',\n 174 |  'cam24_il_P2PGS',\n 175 |  'cam24_il_EbayMor',\n 176 |  'cam24_il_Other',\n 177 |  'cam24_il_MidCAM22Score']\n 178 | for i, name in enumerate(seg_names):\n 179 | def get_candidate_vars(var_path, suffix):\n 180 |     candidate_ls = read_line_file(var_path)\n 181 |     return [c+suffix for c in candidate_ls]\n 182 | se_files = {\n 183 |     # 'all_seg':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt',\"\"),\n 184 |     'Tot':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/candidates_top500.txt',\"\"),\n 185 |     'recur':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedRecurring/columns/candidates_top500.txt',\"_seg6\"),\n 186 |     'xo':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedXO/columns/candidates_top500.txt',\"_seg5\"),\n 187 |     'other':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Other/columns/candidates_top500.txt',\"_seg10\"),\n 188 |     'p2pff':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PFF/columns/candidates_top500.txt',\"_seg7\"),\n 189 |     'p2pgs':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PGS/columns/candidates_top500.txt',\"_seg8\"),\n 190 |     'ebaymor':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_EbayMor/columns/candidates_top500.txt',\"_seg9\"),\n 191 | }\n 192 | se_files.keys()\n 193 | gfs_bucket = \"gs://pypl-bkt-rsh-row-std-gds-gfs\"\n 194 | focus_bucket = \"gs://pypl-bkt-rsh-row-std-gds-focus\"\n 195 | specific_config = {\n 196 |     \"exp9_1\": {\n 197 |         'week_id':'all',\n 198 |         'train_epoch_steps': 585,\n 199 |         'val_epoch_steps': 146,\n 200 |         'train_epochs': 15,\n 201 |         'candidate_vars': candidate_vars + [se_files['Tot']],\n 202 |         'loss_functions': ['binary_crossentropy', 'penalty_loss'],\n 203 |         'pos_scale': 5.0,\n 204 |         'neg_scale': 5.0,\n 205 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v6\",\n 206 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_overall_7217_tf_bad4_50/',\n 207 |     },\n 208 |     \"exp9_2\": {\n 209 |         'week_id':'all',\n 210 |         'train_epoch_steps': 585,\n 211 |         'val_epoch_steps': 146,\n 212 |         'train_epochs': 15,\n 213 |         'candidate_vars': candidate_vars + [se_files['Tot']],\n 214 |         'loss_functions': ['binary_crossentropy', 'penalty_loss'],\n 215 |         'pos_scale': 2.0,\n 216 |         'neg_scale': 5.0,\n 217 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v6\",\n 218 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_overall_7217_tf_bad4_50/',\n 219 |     },\n 220 |     \"exp9_3\": {\n 221 |         'week_id':'all',\n 222 |         'train_epoch_steps': 585,\n 223 |         'val_epoch_steps': 146,\n 224 |         'train_epochs': 15,\n 225 |         'candidate_vars': candidate_vars + [se_files['Tot']],\n 226 |         'loss_functions': ['binary_crossentropy', 'penalty_loss'],\n 227 |         'pos_scale': 2.0,\n 228 |         'neg_scale': 2.0,\n 229 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v6\",\n 230 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_overall_7217_tf_bad4_50/',\n 231 |     },\n 232 |     \"exp9_4\": {\n 233 |         'week_id':'all',\n 234 |         'train_epoch_steps': 585,\n 235 |         'val_epoch_steps': 146,\n 236 |         'train_epochs': 15,\n 237 |         'candidate_vars': candidate_vars + [se_files['Tot']],\n 238 |         'loss_functions': ['binary_crossentropy', 'penalty_loss'],\n 239 |         'pos_scale': 5.0,\n 240 |         'neg_scale': 2.0,\n 241 |         'previous_model_path': \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/cam_il_new_var_logit_v6\",\n 242 |         'data_path': f'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/cam_20240912_recent_data_overall_7217_tf_bad4_50/',\n 243 |     },\n 244 | }\n 245 | # !gsutil -m cp -r /projects/gds-focus/data/catch/IL/CAM_variable/models/tf_models/cam_il_new_var_logit_v6 gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/base_models/\n 246 | # ### Local debug\n 247 | import logging\n 248 | logging.basicConfig(stream=sys.stdout, level='INFO')\n 249 | logger = logging.getLogger('Train')\n 250 | strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n 251 | exp_trainer_all(tmp_config)\n 252 | # !ls -tl $working_path\n 253 | working_path\n 254 | md = \"0113\"\n 255 | exp_idx = 'exp9'\n 256 | t =os.path.join(working_path,f'{exp_idx}_job_ids_{md}.json')\n 257 | if os.path.exists(t):\n 258 |     with open(t, 'r') as f:\n 259 |         job_ids = json.load(f)\n 260 | specific_config.keys()\n 261 | job_ids = {}\n 262 | run_ls = ['exp9_1', 'exp9_2', 'exp9_3', 'exp9_4']\n 263 | for exp_id in run_ls:\n 264 |     week_id = specific_config[exp_id][\"week_id\"]\n 265 |     tmp_config = copy.deepcopy(model_config)  \n 266 |     tmp_config['week_id'] = str(week_id)\n 267 |     for k, v in specific_config[exp_id].items():\n 268 |         if k not in {\"week_id\", \"data_path\"}:\n 269 |             tmp_config[k] = v\n 270 |     data = specific_config[exp_id][\"data_path\"]\n 271 |     tmp_config['previous_model_path'] = specific_config[exp_id][\"previous_model_path\"]\n 272 |     all_files = tf.io.gfile.glob(data+'*.gz')\n 273 |     dev_files, val_files = train_test_split(all_files, test_size=0.2, shuffle=True)\n 274 |     tmp_config['dev_files']=dev_files\n 275 |     tmp_config['val_files']=val_files\n 276 |     tmp_config['keep_model_path'] = f'{focus_bucket}/user/cchen16/IL_variable/{md}_{exp_idx}/{exp_id}_gcp_loss/week_'\n 277 |     # exp_trainer_all(tmp_config)\n 278 |     job_id = client.create_job(\n 279 |                 func = exp_trainer_all,\n 280 |                 pyfiles_to_import=[\n 281 |                                     '/projects/gds-focus/data/catch/IL/CAM_variable/assets/sf_il_utils.py',\n 282 |                                     '/projects/cchen16/FlagshipModelResearch/01_phase2_research/il_utils/cc_functions.py',\n 283 |                                   ], \n 284 |                 num_workers = num_workers,\n 285 |                 machine_type_per_worker = \"n1-highmem-16\",\n 286 |                 # machine_type_per_worker = \"n2d-highmem-8\",\n 287 |                 model_config = tmp_config\n 288 |                 )\n 289 |     model_name = f'{exp_id}_week{week_id}'\n 290 |     job_ids[model_name] = job_id\n 291 |     tmp_config['job_id'] = job_id\n 292 |     with open(os.path.join(working_path,f'{model_name}_{md}.json'),'w') as f:\n 293 |         json.dump(tmp_config,f)\n 294 | with open(t,'w') as f:\n 295 |     json.dump(job_ids, f)\n 296 | # client.cancel_job('maglev/cloud/dataproc/job/gds-focus-3d570d/2aa7536c-25bb-35f9-9590-bb70afaa76fd')\n 297 | # ## Tracking\n 298 | def pretty_model_config(configs):\n 299 |     for k, v in configs.items():\n 300 |         if k not in {\"candidate_vars\", \"dev_files\", \"val_files\"}:\n 301 |         if k in {\"dev_files\"}:\n 302 |         if k in {\"candidate_vars\"}:\n 303 |             candidate_len = []\n 304 |             for ls in v:\n 305 |                 candidate_len.append(len(ls))\n 306 |     return\n 307 | def check_job_state(job_ids):\n 308 |     for k,v in job_ids.items():\n 309 | def cancel_job(job_ids):\n 310 |     for k,v in job_ids.items():\n 311 | week_id = 'all'\n 312 | model_name = f'{exp_id}_week{week_id}'\n 313 | config_path = os.path.join(working_path,f'{model_name}_{md}.json')\n 314 | with open(os.path.join(working_path,f'{model_name}_{md}.json'),'r') as f:\n 315 |     config = json.load(f)\n 316 | pretty_model_config(config)\n 317 | job_ids\n 318 | check_job_state(job_ids)\n 319 | client.wait_job_for_completion(job_ids['exp9_1_weekall'],\n 320 |                                time_to_sleep=300)\n 321 | # # %%capture job_log  \n 322 | client.get_job_driver_logs(job_ids['exp9_4_weekall'])\n 323 | import re\n 324 | log = \"\"\"\n 325 | tf.math.sigmoid_loss: 0.6841 - tf.math.sigmoid_5_loss: 0.9473 - tf.math.sigmoid_recall: 0.3386 - tf.math.sigmoid_precision: 0.6501 - tf.math.sigmoid_5_recall: 0.5958 - tf.math.sigmoid_5_precision: 0.5299 - val_loss: 0.6088 - val_tf.math.sigmoid_loss: 0.6763 - val_tf.math.sigmoid_5_loss: 0.6088 - val_tf.math.sigmoid_recall: 0.3230 - val_tf.math.sigmoid_precision: 0.6918 - val_tf.math.sigmoid_5_recall: 0.5891 - val_tf.math.sigmoid_5_precision: 0.6756\n 326 | \"\"\"\n 327 | pattern = r'(\\w+[\\.\\w]*)\\:\\s*([\\d\\.\\-]+)'  \n 328 | metrics = re.findall(pattern, log)  \n 329 | metrics_dict = dict(metrics)  \n 330 | for key, value in metrics_dict.items(): \n 331 | # !gsutil -m cp -r gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/1120_exp1/exp1_1_gcp_loss /projects/cchen16/FlagshipModelResearch/01_phase2_research/tensorboard/\n 332 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0110_exp6/exp6_7_gcp_loss/week_all/\n 333 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0113_exp8/exp8_1_gcp_loss/week_all/\n 334 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0113_exp9/exp9_3_gcp_loss/week_all/\n 335 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/0924_exp1/exp1_2_gcp_loss/week_all/\n 336 | # ## Check config and log\n 337 | exp_id = 'exp3_7'\n 338 | week_id = 'all'\n 339 | model_name = f'{exp_id}_week{week_id}'\n 340 | config_path = os.path.join(working_path,f'{model_name}_{md}.json')\n 341 | with open(os.path.join(working_path,f'{model_name}_{md}.json'),'r') as f:\n 342 |     config = json.load(f)\n 343 | pretty_model_config(config)\n 344 | with open()\n 345 | client.get_job_driver_logs(job_id)\n 346 | log_file = f\"/projects/gds-focus/data/catch/IL/CAM_variable/training_logs/{model_name}.log\"\n 347 | with open(log_file, \"a\") as f:\n 348 |     f.write(job_ids[model_name])\n 349 |     f.write(\"**\"*50)\n 350 |     client.get_job_driver_logs(gcp_job_id, file=f)\n 351 | import re\n 352 | def extract_metric(log_path):\n 353 |     with open(log_path, 'r') as f:\n 354 |         log = f.read()\n 355 |     pattern = r'(\\w+[\\.\\w]*)\\:\\s*([\\d\\.\\-]+)'  \n 356 |     metrics = re.findall(pattern, log)  \n 357 |     metrics_dict = dict(metrics)  \n 358 |     for key, value in metrics_dict.items():  \n 359 |     return\n 360 | extract_metric(log_file)\n 361 | set(se_files['all_seg'])-set(config['candidate_vars'][3])\n 362 | client.get_job_application_logs(config['job_id'])\n 363 | 1024*1800\n 364 | client.get_job_driver_logs('maglev/cloud/dataproc/job/gds-focus-5aa772/927ee8e9-2e74-3539-ad7b-463e1126e952')\n 365 | # ### check tensorboard\n 366 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0110_exp6/exp6_7_gcp_loss/week_all/\n 367 | # !gsutil cp -r gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0110_exp6/exp6_7_gcp_loss/week_all/week-all-tensorboard ./",
    "rmr_agent/repos/CAM_variable_research/4_pack_scoring.py": "   1 | # ## Auth & packages\n   2 | # %url -c horton\n   3 | # %ppauth\n   4 | import tensorflow as tf\n   5 | from tensorflow import keras\n   6 | from tensorflow.keras.models import Sequential\n   7 | from tensorflow.keras.layers import Dense, Dropout\n   8 | from tensorflow.keras import optimizers\n   9 | from tensorflow.python.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, TensorBoard\n  10 | from tensorflow.keras import backend as K, Sequential, Model, layers\n  11 | from tensorflow.keras.utils import get_custom_objects\n  12 | import json\n  13 | import warnings\n  14 | import os, sys\n  15 | # sys.path.append('/projects/cchen16/FlagshipModelResearch/01_phase2_research/il_utils')\n  16 | sys.path.append('/projects/gds-focus/data/catch/IL/CAM_variable/assets')\n  17 | from sf_il_utils import *\n  18 | from model_anatomy import *\n  19 | # from cc_functions import *\n  20 | from helper import *\n  21 | from model_automation.utils.rmr.helper import run_cmd\n  22 | from py_dpu.utils import  save_pig,load_pig, rm_df_alias\n  23 | from pyspark.sql import functions as F\n  24 | from pyScoring.model import ModelScorer\n  25 | from pyScoring import UMEModel\n  26 | from py_dpu import load_parquet\n  27 | from automation_utils.gcp.GSUtil import GSUtilHelper\n  28 | from pyspark.sql.functions import array, col\n  29 | from pyspark.sql import functions as F\n  30 | from pyspark.sql.functions import col, max, min, when, lit\n  31 | import time\n  32 | from model_automation.gcp import dataproc_config\n  33 | import pandas as pd\n  34 | import matplotlib.pyplot as plt\n  35 | import seaborn as sns\n  36 | pd.set_option('display.max_rows', 500)\n  37 | pd.set_option('display.max_columns', 500)\n  38 | pd.set_option('display.width', 1000)\n  39 | from pyScoring.node import ComputeNode, ReNameBuilder, SplitBuilder, InputBuilder\n  40 | from pyScoring.model.model import merge_model_specs\n  41 | from pyScoring.onnx.support.tf2.tf2_to_onnx import tf_model_to_onnx_as_spec\n  42 | from pyScoring import ModelScorer\n  43 | # ## Parameters\n  44 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0116_exp8/\n  45 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0116_exp8/exp8_1_gcp_loss/week_all/ \n  46 | # basic index\n  47 | local_root_dir = \"/projects/gds-focus/data/catch/IL/CAM_variable\"\n  48 | gcs_root_dir = \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable\"\n  49 | exp_id = \"exp8\"\n  50 | tf_model_dir_ls = [\n  51 | 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0116_exp8/exp8_1_gcp_loss/',\n  52 | 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0116_exp8/exp8_3_gcp_loss/',\n  53 | 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0116_exp8/exp8_5_gcp_loss/',\n  54 | 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0116_exp8/exp8_1_gcp_loss/week_all/model.15-0.49151716/',\n  55 | 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0116_exp8/exp8_3_gcp_loss/week_all/model.15-0.51099724/',\n  56 | 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0116_exp8/exp8_5_gcp_loss/week_all/model.15-0.52378106/',\n  57 | ]\n  58 | # !mkdir /projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_5_gcp_loss\n  59 | # !gsutil -m cp -r gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0116_exp8/exp8_5_gcp_loss/week_all/model.15-0.52378106 /projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_5_gcp_loss/\n  60 | # # model path\n  61 | tf_model_local_path_list = []\n  62 | for tf_model_dir in tf_model_dir_ls:\n  63 |     exp_name = tf_model_dir.split('/')[-2]\n  64 |     tf_model_local_dir = f\"{local_root_dir}/tf_models/{exp_name}\"\n  65 |     os.makedirs(tf_model_local_dir, exist_ok=True)\n  66 |     if tf_model_dir.startswith(\"gs\"):\n  67 |         tf_model_gcp_path_list = [i[:-1] for i in  GSUtilHelper.ls(f'{tf_model_dir}*/*best_model').split() if i.endswith('best_model/')]\n  68 |         # move tf model from gcs to local\n  69 |         for tf_model_gcp_path in tf_model_gcp_path_list:\n  70 |             run_cmd(f\"gsutil -m cp -r {tf_model_gcp_path} {tf_model_local_dir}/\")\n  71 |             tf_model_local_path_list.append(f\"{tf_model_local_dir}/{tf_model_gcp_path.split('/')[-1]}\")\n  72 |     else:\n  73 |         tf_model_gcp_path_list.extend([os.path.join(tf_model_dir,week_tree,ckpt) for week_tree in os.listdir(tf_model_dir) \\\n  74 |                                                for ckpt in os.listdir(os.path.join(tf_model_dir,week_tree)) \\\n  75 |                                                if ckpt.endswith(\"best_model\")]\n  76 |                                      )\n  77 |         # move tf model from gcs to local\n  78 |         for tf_model_gcp_path in tf_model_gcp_path_list:    \n  79 |             run_cmd(f\"cp -r {tf_model_gcp_path} {tf_model_local_dir}/\")\n  80 |             tf_model_local_path_list.append(f\"{tf_model_local_dir}/{tf_model_gcp_path.split('/')[-1]}\")\n  81 | tf_model_local_path_list\n  82 | other_gcs_model_list = [\n  83 | 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0116_exp8/exp8_1_gcp_loss/week_all/model.15-0.49151716/',\n  84 | 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0116_exp8/exp8_3_gcp_loss/week_all/model.15-0.51099724/',\n  85 | 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/0116_exp8/exp8_5_gcp_loss/week_all/model.15-0.52378106/',\n  86 | ]\n  87 | tf_model_local_dir\n  88 | for tf_model_gcp_path in other_gcs_model_list:\n  89 |     tf_model_local_dir = \"/projects/gds-focus/data/catch/IL/CAM_variable/tf_models\"\n  90 |     run_cmd(f\"gsutil -m cp -r {tf_model_gcp_path} {tf_model_local_dir}/\")\n  91 |     tf_model_local_path_list.append(f\"{tf_model_local_dir}/{tf_model_gcp_path.split('/')[-2]}\")\n  92 | tf_model_local_path_list\n  93 | # !mv /projects/gds-focus/data/catch/IL/CAM_variable/tf_models/model.15-0.51099724 /projects/gds-focus/data/catch/IL/CAM_variable/tf_models/exp8_3_gcp_loss/\n  94 | # !ls /projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_1_gcp_loss/model.15-0.49151716/\n  95 | # !find /projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_5_gcp_loss/ -type d -print\n  96 | tf_model_local_path_list = [\n  97 | '/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_1_gcp_loss/week-all-best_model',\n  98 | '/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_1_gcp_loss/model.15-0.49151716',\n  99 | '/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_3_gcp_loss/week-all-best_model',\n 100 | '/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_3_gcp_loss/model.15-0.51099724',\n 101 | '/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_5_gcp_loss/week-all-best_model',\n 102 | '/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_5_gcp_loss/model.15-0.52378106',\n 103 | ]\n 104 | # # 1. Package submodel\n 105 | import importlib\n 106 | import model_anatomy\n 107 | importlib.reload(model_anatomy)\n 108 | from model_anatomy import *\n 109 | import re\n 110 | from pyScoring import ConditionBuilder, RegressionBuilder, UMEModel,ReNameBuilder,ModeltoNodeBuilder\n 111 | from pyScoring.backend import DataType\n 112 | from pyScoring.graph import Graph\n 113 | # norm shifu\n 114 | from model_automation.utils.rmr import set_shifu_col\n 115 | from pyScoring.shifu import ShifuTransformer\n 116 | rbt_woe_path = \"/projects/gds-focus/data/catch/IL/CAM24/umes/base/robust_woe.m\"\n 117 | adp_woe_path = \"/projects/gds-focus/data/catch/IL/CAM24/umes/base/adaptive_woe.m\"\n 118 | # ## exp9_1-exp9-4\n 119 | def get_candidate_vars(var_path, suffix):\n 120 |     candidate_ls = read_line_file(var_path)\n 121 |     return [c+suffix for c in candidate_ls]\n 122 | from pyScoring import ConditionBuilder, RegressionBuilder, UMEModel,ReNameBuilder,ModeltoNodeBuilder\n 123 | from pyScoring.backend import DataType\n 124 | from pyScoring.graph import Graph\n 125 | def pack_flow_ume(exp_idx, seg_name, new_vars, tf_model_output, model_path):\n 126 |     # new_var_shifu_folder = \"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam_20240912_all_in_one\"\n 127 |     # set_shifu_col(new_var_shifu_folder, new_vars)\n 128 |     # new_var_shifu_norm = ShifuTransformer(new_var_shifu_folder)\n 129 |     # new_var_woe = new_var_shifu_norm.create_shifu_transformation_nodes(postfix=\"_new_var\")\n 130 |     # g = Graph()\n 131 |     # g.add_nodes(new_var_woe)\n 132 |     # new_var_norm_ume = g.generate_model_by_graph(model_name=f\"new_var_{seg_name}_{len(new_vars)}_woe\", optimization=True)\n 133 |     # new_var_norm_ume.save(path=\"/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/\")\n 134 |     candidate_vars = [list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 135 |                       list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/robust_tf_input.txt',sep='|').columns),\n 136 |                       list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 137 |                       new_vars\n 138 |                      ]\n 139 |     candidate_vars[0] = [item+\"_adp\" for item in candidate_vars[0]]\n 140 |     candidate_vars[1] = [item+\"_rob\" for item in candidate_vars[1]]\n 141 |     candidate_vars[2] = [item+\"_adp\" for item in candidate_vars[2]]\n 142 |     candidate_vars[3] = [item+\"_new_var\" for item in candidate_vars[3]]\n 143 |     new_var_woe_path = f\"/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/new_var_{seg_name}_{len(new_vars)}_woe.m\"\n 144 |     woe_model_path_ls = [rbt_woe_path, adp_woe_path, new_var_woe_path]\n 145 |     # tf_model_output = ['cam_tccls']\n 146 |     ume_model_name = exp_idx+\"_\"+seg_name\n 147 |     candidate_vars_map = {\n 148 |         'adaptive_input': candidate_vars[0],\n 149 |         'robust_input': candidate_vars[1],\n 150 |         'adaptive_yg_input': candidate_vars[2],\n 151 |         'new_var':candidate_vars[3],\n 152 |     }\n 153 |     # model_path = f'/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/{exp_idx}_gcp_loss/week-all-best_model'\n 154 |     # custom_objects = {\"penalty_loss\": custom_penalty_loss(pos_scale=5., neg_scale=model_config['neg_scale']),\n 155 |     #                       \"focal_loss\": binary_focal_loss(gamma=2., alpha=.25),\n 156 |     #                       \"binary_crossentropy\":\"binary_crossentropy\"}\n 157 |     # with keras.utils.custom_object_scope(custom_objects):\n 158 |     # tf_model = tf.keras.models.load_model(model_path,custom_objects={'_custom_penalty_loss': custom_penalty_loss})\n 159 |     tf_model = tf.keras.models.load_model(model_path)\n 160 |     packaged = custom_package_woe_and_tf_all(tf_model,\n 161 |                                       tf_model_output,\n 162 |                                       woe_model_path_ls,\n 163 |                                       candidate_vars_map,\n 164 |                                       ume_model_name)\n 165 |     packaged.predict({})\n 166 |     condition_node = ConditionBuilder('cam24_flow_score', \"if(consumer_type == 2.0 or consumer_type == 3.0) {cam_tc_cls} else {cam_yg_cls}\", \n 167 |                                      {'consumer_type': DataType.DOUBLE.getId(),\n 168 |                                      'cam_tc_cls': DataType.DOUBLE.getId(),\n 169 |                                      'cam_yg_cls': DataType.DOUBLE.getId()}).build()\n 170 |     g = packaged.convert_to_graph()\n 171 |     g.add_node(condition_node)\n 172 |     final_spec = g.generate_model_by_graph(model_name=ume_model_name+\"_cond\", model_outputs=[\"cam24_flow_score\"])\n 173 |     final_spec.save(\"/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/\")\n 174 |     return\n 175 | se_files = {\n 176 |     # 'all_seg':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt',\"\"),\n 177 |     'recur':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedRecurring/columns/candidates_top500.txt',\"_seg6\"),\n 178 |     'xo':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedXO/columns/candidates_top500.txt',\"_seg5\"),\n 179 |     'other':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Other/columns/candidates_top500.txt',\"_seg10\"),\n 180 |     'p2pff':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PFF/columns/candidates_top500.txt',\"_seg7\"),\n 181 |     'p2pgs':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PGS/columns/candidates_top500.txt',\"_seg8\"),\n 182 |     'ebaymor':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_EbayMor/columns/candidates_top500.txt',\"_seg9\"),\n 183 |     'tot':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/candidates_top500.txt',\"\"),\n 184 | }\n 185 | def _custom_penalty_loss(y_true, y_pred):   \n 186 |         bce_loss = tf.keras.backend.binary_crossentropy(y_true, y_pred) \n 187 |         y_true = tf.cast(y_true, dtype=tf.int32)\n 188 |         penalty = tf.where(y_true == 1,   \n 189 |                            tf.where(y_pred < 0.15, bce_loss * pos_scale, bce_loss),   \n 190 |                            bce_loss)\n 191 |         penalty = tf.where(y_true == 0,   \n 192 |                            tf.where(y_pred > 0.15, bce_loss * neg_scale, bce_loss),   \n 193 |                            bce_loss)\n 194 |         return tf.reduce_mean(penalty)\n 195 | tf_model_local_path_list\n 196 | tp_list = [\n 197 | #     ('exp8_1','/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/exp8_1_gcp_loss/week-all-best_model'),\n 198 | # ('exp8_2','/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/exp8_2_gcp_loss/week-all-best_model'),\n 199 | # ('exp8_3','/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/exp8_3_gcp_loss/week-all-best_model'),\n 200 | # ('exp8_4','/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/exp8_4_gcp_loss/week-all-best_model'),\n 201 | # ('exp8_5','/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/exp8_5_gcp_loss/week-all-best_model'),\n 202 | # ('exp8_6','/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/exp8_6_gcp_loss/week-all-best_model'),\n 203 | ('exp8_1_re_best','/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_1_gcp_loss/week-all-best_model'),\n 204 | ('exp8_1_re_epoch15','/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_1_gcp_loss/model.15-0.49151716'),\n 205 | ('exp8_3_re_best','/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_3_gcp_loss/week-all-best_model'),\n 206 | ('exp8_3_re_epoch15','/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_3_gcp_loss/model.15-0.51099724'),\n 207 | ('exp8_5_re_best','/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_5_gcp_loss/week-all-best_model'),\n 208 | ('exp8_5_re_epoch15','/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/0116_exp8/exp8_5_gcp_loss/model.15-0.52378106'),\n 209 | ]\n 210 | for exp_idx, model_path in tp_list[:1]:\n 211 |     # exp_idx = f\"exp8_{idx+1}\"\n 212 |     new_vars = se_files['tot']\n 213 |     pack_flow_ume(exp_idx, 'tot', new_vars, ['cam_tc_cls', 'cam_yg_cls'], model_path)\n 214 | for exp_idx, model_path in tp_list[1:]:\n 215 |     # exp_idx = f\"exp8_{idx+1}\"\n 216 |     new_vars = se_files['tot']\n 217 |     pack_flow_ume(exp_idx, 'tot', new_vars, ['cam_tc_cls', 'cam_yg_cls'], model_path)\n 218 | # ## exp7\n 219 | def pack_all_flow_ume(exp_idx, new_vars, tf_model_output):\n 220 |     new_var_shifu_folder = \"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam_20240912_all_in_one\"\n 221 |     set_shifu_col(new_var_shifu_folder, new_vars)\n 222 |     new_var_shifu_norm = ShifuTransformer(new_var_shifu_folder)\n 223 |     new_var_woe = new_var_shifu_norm.create_shifu_transformation_nodes(postfix=\"_new_var\")\n 224 |     g = Graph()\n 225 |     g.add_nodes(new_var_woe)\n 226 |     new_var_norm_ume = g.generate_model_by_graph(model_name=f\"new_var_{seg_name}_{len(new_vars)}_woe\", optimization=True)\n 227 |     new_var_norm_ume.save(path=\"/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/\")\n 228 |     candidate_vars = [list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 229 |                       list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/robust_tf_input.txt',sep='|').columns),\n 230 |                       list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 231 |                      ]+new_vars\n 232 |     candidate_vars[0] = [item+\"_adp\" for item in candidate_vars[0]]\n 233 |     candidate_vars[1] = [item+\"_rob\" for item in candidate_vars[1]]\n 234 |     candidate_vars[2] = [item+\"_adp\" for item in candidate_vars[2]]\n 235 |     # candidate_vars[3] = [item+\"_new_var\" for item in candidate_vars[3]]\n 236 |     new_var_woe_path = f\"/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/new_var_{seg_name}_{len(new_vars)}_woe.m\"\n 237 |     woe_model_path_ls = [rbt_woe_path, adp_woe_path, new_var_woe_path]\n 238 |     # tf_model_output = ['cam_tccls']\n 239 |     ume_model_name = exp_idx+\"_\"+seg_name\n 240 |     candidate_vars_map = {\n 241 |         'adaptive_input': candidate_vars[0],\n 242 |         'robust_input': candidate_vars[1],\n 243 |         'adaptive_yg_input': candidate_vars[2],\n 244 |         'new_var':candidate_vars[3],\n 245 |     }\n 246 |     model_path = f'/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/{exp_idx}_gcp_loss/week-all-best_model'\n 247 |     tf_model = tf.keras.models.load_model(model_path)\n 248 |     packaged = custom_package_woe_and_tf_all(tf_model,\n 249 |                                       tf_model_output,\n 250 |                                       woe_model_path_ls,\n 251 |                                       candidate_vars_map,\n 252 |                                       ume_model_name)\n 253 |     packaged.predict({})\n 254 |     condition_node = ConditionBuilder('cam24_flow_score', \"if(consumer_type == 2.0 or consumer_type == 3.0) {cam_tc_cls} else {cam_yg_cls}\", \n 255 |                                      {'consumer_type': DataType.DOUBLE.getId(),\n 256 |                                      'cam_tc_cls': DataType.DOUBLE.getId(),\n 257 |                                      'cam_yg_cls': DataType.DOUBLE.getId()}).build()\n 258 |     g = packaged.convert_to_graph()\n 259 |     g.add_node(condition_node)\n 260 |     final_spec = g.generate_model_by_graph(model_name=ume_model_name+\"_cond\", model_outputs=[\"cam24_flow_score\"])\n 261 |     final_spec.save(\"/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/\")\n 262 |     return\n 263 | exp_idx = f\"exp7_3\"\n 264 | se_files = {\n 265 |     # 'all_seg':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt',\"\"),\n 266 |     'Tot':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Tot/columns/candidates_top500.txt',\"\"),\n 267 |     'recur':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedRecurring/columns/candidates_top500.txt',\"_seg6\"),\n 268 |     'xo':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_BrandedXO/columns/candidates_top500.txt',\"_seg5\"),\n 269 |     'other':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_Other/columns/candidates_top500.txt',\"_seg10\"),\n 270 |     'p2pff':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PFF/columns/candidates_top500.txt',\"_seg7\"),\n 271 |     'p2pgs':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_P2PGS/columns/candidates_top500.txt',\"_seg8\"),\n 272 |     'ebaymor':get_candidate_vars('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam24_il_EbayMor/columns/candidates_top500.txt',\"_seg9\"),\n 273 | }\n 274 | new_vars = list(se_files.values())\n 275 | all_new_vars = sum(new_vars, [])\n 276 | len(all_new_vars)\n 277 | seg_name = \"all_flow\"\n 278 | new_var_shifu_folder = \"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam_20240912_all_in_one\"\n 279 | set_shifu_col(new_var_shifu_folder, all_new_vars)\n 280 | new_var_shifu_norm = ShifuTransformer(new_var_shifu_folder)\n 281 | new_var_woe = new_var_shifu_norm.create_shifu_transformation_nodes(postfix=\"_new_var\")\n 282 | g = Graph()\n 283 | g.add_nodes(new_var_woe)\n 284 | new_var_norm_ume = g.generate_model_by_graph(model_name=f\"new_var_{seg_name}_{len(all_new_vars)}_woe\", optimization=True)\n 285 | new_var_norm_ume.save(path=\"/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/\")\n 286 | candidate_vars = [list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 287 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/robust_tf_input.txt',sep='|').columns),\n 288 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/columns/adaptive_tf_input.txt',sep='|').columns),\n 289 |                  ]+new_vars\n 290 | candidate_vars[0] = [item+\"_adp\" for item in candidate_vars[0]]\n 291 | candidate_vars[1] = [item+\"_rob\" for item in candidate_vars[1]]\n 292 | candidate_vars[2] = [item+\"_adp\" for item in candidate_vars[2]]\n 293 | for idx in range(3,len(candidate_vars)):\n 294 |     candidate_vars[idx] = [item+\"_new_var\" for item in candidate_vars[idx]]\n 295 | new_var_woe_path = f\"/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/new_var_{seg_name}_{len(all_new_vars)}_woe.m\"\n 296 | woe_model_path_ls = [rbt_woe_path, adp_woe_path, new_var_woe_path]\n 297 | # tf_model_output = ['cam_tccls']\n 298 | ume_model_name = exp_idx+\"_\"+seg_name\n 299 | candidate_vars_map = {\n 300 |     'adaptive_input': candidate_vars[0],\n 301 |     'robust_input': candidate_vars[1],\n 302 |     'adaptive_yg_input': candidate_vars[2],\n 303 |     'new_var_expert_0':candidate_vars[3],\n 304 |     'new_var_expert_1':candidate_vars[4],\n 305 |     'new_var_expert_2':candidate_vars[5],\n 306 |     'new_var_expert_3':candidate_vars[6],\n 307 |     'new_var_expert_4':candidate_vars[7],\n 308 |     'new_var_expert_5':candidate_vars[8],\n 309 |     'new_var_expert_6':candidate_vars[9],\n 310 | }\n 311 | ume_model_name = exp_idx+\"_\"+seg_name\n 312 | model_path = f'/projects/gds-focus/data/catch/IL/CAM_variable/tf_models/{exp_idx}_gcp_loss/week-all-best_model'\n 313 | tf_model = tf.keras.models.load_model(model_path)\n 314 | tf_model_output = ['cam_tc_cls', 'cam_yg_cls']\n 315 | packaged = custom_package_woe_and_tf_all(tf_model,\n 316 |                                   tf_model_output,\n 317 |                                   woe_model_path_ls,\n 318 |                                   candidate_vars_map,\n 319 |                                   ume_model_name)\n 320 | packaged.predict({})\n 321 | condition_node = ConditionBuilder('cam24_flow_score', \"if(consumer_type == 2.0 or consumer_type == 3.0) {cam_tc_cls} else {cam_yg_cls}\", \n 322 |                                  {'consumer_type': DataType.DOUBLE.getId(),\n 323 |                                  'cam_tc_cls': DataType.DOUBLE.getId(),\n 324 |                                  'cam_yg_cls': DataType.DOUBLE.getId()}).build()\n 325 | g = packaged.convert_to_graph()\n 326 | g.add_node(condition_node)\n 327 | final_spec = g.generate_model_by_graph(model_name=ume_model_name+\"_cond\", model_outputs=[\"cam24_flow_score\"])\n 328 | final_spec.save(\"/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/\")\n 329 | # !find /projects/gds-focus/data/catch/IL/CAM_variable/models/umes/ -name \"exp7*\" -print  \n 330 | local_ume_list = [\n 331 |     '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp7_1_all_flow_cond.m',\n 332 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp7_3_all_flow_cond.m',\n 333 |     '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp6_1_recur_cond.m',\n 334 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp6_2_xo_cond.m',\n 335 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp6_3_other_cond.m',\n 336 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp6_4_p2pff_cond.m',\n 337 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp6_5_p2pgs_cond.m',\n 338 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp6_6_ebaymor_cond.m',\n 339 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp6_7_tot_cond.m',\n 340 | ]\n 341 | feature_list = []\n 342 | for ume_path in local_ume_list:\n 343 |     tmp = UMEModel(ume_path)\n 344 |     feature_list.extend(tmp.inputs)\n 345 | len(set(feature_list))\n 346 | write_line_file(\"/projects/gds-focus/data/catch/IL/CAM_variable/files/oot_0111_3505.txt\", list(set(feature_list)))\n 347 | # !find /projects/gds-focus/data/catch/IL/CAM_variable/models/umes/ -name \"*woe*\" -print  \n 348 | woe_ls = [\n 349 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/new_var_650_woe.m',\n 350 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/robust_woe.m',\n 351 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/adaptive_woe.m',\n 352 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/new_var_recur_200_woe.m',\n 353 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/new_var_xo_200_woe.m',\n 354 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/new_var_other_200_woe.m',\n 355 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/new_var_p2pff_200_woe.m',\n 356 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/new_var_p2pgs_200_woe.m',\n 357 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/new_var_ebaymor_200_woe.m',\n 358 | ]\n 359 | for ls in candidate_vars:\n 360 |     for var in ls:\n 361 |         if var.startswith(\"driver_\"):\n 362 | for idx, woe_path in enumerate(woe_ls):\n 363 |     woe_ume = UMEModel(woe_path)\n 364 |     mname = woe_path.split('/')[-1]\n 365 |     inputs = woe_ume.get_inputs()\n 366 |     for var in inputs:\n 367 |         if var.startswith('driver_'):\n 368 | woe_ume = UMEModel('/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/new_var_650_woe.m')\n 369 | woe_ume.print_graph()\n 370 | len(woe_ume.inputs), len(woe_ume.outputs)\n 371 | check = 0\n 372 | for elem in colconfig:\n 373 |     if elem['columnName'] in raw_vars and elem['columnFlag'] is None:\n 374 |     # and elem['columnName'].startswith(\"driver_\"):\n 375 |         check += 1\n 376 |         # print(elem['columnName'])\n 377 | check\n 378 | import re\n 379 | raw_vars = list(set([re.sub(\"_seg[0-9]+\", \"\", x) for x in feature_list]))\n 380 | len(raw_vars)\n 381 | feature_list[-1]\n 382 | feature_list = read_line_file('/projects/gds-focus/data/catch/IL/CAM_variable/files/cam_0912_topse650.txt')\n 383 | feature_list\n 384 | fea_set = set(feature_list)\n 385 | config_path = os.path.join('/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam_20240912_all_in_one', \"ColumnConfig.json\")\n 386 | colconfig = json.load(open(config_path, \"r\"))\n 387 | maps = set(feature_list)\n 388 | # for elem in colconfig:\n 389 | #     if elem[\"columnName\"] not in maps:\n 390 | #         elem[\"finalSelect\"] = False\n 391 | #     else:\n 392 | #         elem[\"finalSelect\"] = True\n 393 | #         fea_set.remove(elem['columnName'])\n 394 | # print(\"{} variables selected\".format(sum(i[\"finalSelect\"] for i in colconfig)))\n 395 | # # with open(config_path, \"w\") as f:\n 396 | # #     json.dump(colconfig, f)\n 397 | for elem in colconfig:\n 398 |     if elem['finalSelect'] == True and elem['columnName'] in maps:\n 399 |     # and elem['columnName'].startswith(\"driver_\"):\n 400 | var_ls = []\n 401 | for elem in colconfig:\n 402 |     var_ls.append(elem['columnName'])\n 403 | len(set(var_ls)), len(var_ls)\n 404 | len(colconfig)\n 405 | for elem in colconfig:\n 406 |     if elem['finalSelect'] == True and elem['columnName'] == 'acct_multi_evt_num_vid_720h_seg9':\n 407 |     # and elem['columnName'].startswith(\"driver_\"):\n 408 | for elem in colconfig:\n 409 |     if 'driver_cc_control' in elem['columnName']:\n 410 |     # and elem['columnName'].startswith(\"driver_\"):\n 411 | for elem in colconfig:\n 412 |     if elem['columnName'] in feature_list and elem['columnFlag'] == 'Candidate':\n 413 |         # print(elem[''])\n 414 | from model_automation.utils.rmr import set_shifu_col\n 415 | from pyScoring.shifu import ShifuTransformer\n 416 | new_var_shifu_folder = \"/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam_20240912_all_in_one\"\n 417 | set_shifu_col(new_var_shifu_folder, feature_list)\n 418 | new_var_shifu_norm = ShifuTransformer(new_var_shifu_folder)\n 419 | new_var_nodes = new_var_shifu_norm.create_shifu_transformation_nodes(postfix=\"_new_var\")\n 420 | # g = Graph()\n 421 | # g.add_nodes(new_var_nodes)\n 422 | # new_var_norm_ume = g.generate_model_by_graph(model_name=f\"new_var_top_650_woe\")\n 423 | # new_var_norm_ume.save(path=\"/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/test/\")\n 424 | '/projects/gds-focus/data/catch/IL/CAM_variable/cam_20240912/shifu/cam_20240912_all_in_one/columns'\n 425 | len(new_var_nodes), new_var_nodes[0].inputs, new_var_nodes[0].outputs\n 426 | 'cpf_id_seg1' in raw_vars\n 427 | for node in new_var_woe:\n 428 |     if 'driver_cc_control' in node.inputs[0] or 'acct_multi_evt_num_vid_720h' in node.inputs[0] or \\\n 429 |         'driver_cc_control' in node.outputs[0] or 'acct_multi_evt_num_vid_720h' in node.outputs[0]:\n 430 | # varlist = [re.sub(\"_seg[0-9]+\", \"\", x) for x in feature_list]\n 431 | # # 2. Scoring for ensemble\n 432 | def base_esm_woe(train_raw_path,\n 433 |         train_woe_path,\n 434 |         meta_cols,\n 435 |         model_path_list):\n 436 |     from pyScoring.model import ModelScorer\n 437 |     from pyScoring import UMEModel\n 438 |     from py_dpu import load_parquet\n 439 |     from automation_utils.gcp.GSUtil import GSUtilHelper\n 440 |     from pyspark.sql.functions import array, col\n 441 |     from pyspark.sql import functions as F\n 442 |     # load and dedup \n 443 |     data = load_parquet(spark, train_raw_path)\n 444 |     # data = data.drop_duplicates(subset=[\"trans_id\"])\n 445 |     esm_filter = f\"\"\"\n 446 |                   driver_is_brm3_4_11_15_bad != 2 \n 447 |                   and driver_is_brm3_4_11_15_bad != '2' \n 448 |                   and driver_is_brm3_4_11_15_bad != 2.0\n 449 |                   and driver_is_sndr_id_train_set == 1\n 450 |                   \"\"\"\n 451 |     # and (driver_is_y_g == '1' or driver_is_y_g == '1.0')\n 452 |     data = data.where(esm_filter)\n 453 |     tmp_file_list = []\n 454 |     input_list = []\n 455 |     model_outputs = []\n 456 |     for file_path in model_path_list:\n 457 |         model_name = file_path.split('/')[-1]\n 458 |         GSUtilHelper.cp(file_path, f'/tmp/model_spec/{model_name}')\n 459 |         tmp_file_list.append(f'/tmp/model_spec/{model_name}')\n 460 |         model = UMEModel(f'/tmp/model_spec/{model_name}')\n 461 |         final_outputs = [model.name+'_'+c for c in model.outputs]\n 462 |         model_outputs.extend(final_outputs)\n 463 |     data = data.withColumn(\"index\", F.monotonically_increasing_id()) # fix\n 464 |     scorer = ModelScorer(spark, validate=False)\n 465 |     # data = scorer.create_score_df(input_df=data, mfile_paths= tmp_file_list, model_outputs= model_outputs, df_column_names = model_outputs)\n 466 |     data = scorer.create_score_df(input_df = data, \n 467 |                                   mfile_paths = tmp_file_list)\n 468 |     # output_list = sum(model_outputs, [])\n 469 |     data = data.select(*(col(c).cast(\"float\").alias(c) for c in model_outputs), *meta_cols).repartition(100)\n 470 |     data.write.format(\"parquet\").mode(\"overwrite\").save(train_woe_path)\n 471 |     return\n 472 | # !ls -t /notebooks/projects/gds-focus/data/catch/IL/CAM24/umes/ | head\n 473 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/umes/\n 474 | # !gsutil cp /projects/gds-focus/data/makang/CAM24/source_code/model_ems/submodel/ems/cam24_ems_sapout.m gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/umes/\n 475 | from model_automation.gcp import dataproc_config\n 476 | from aml import cloud_v1 as cloud\n 477 | gcp_project = \"ccg24-hrzana-gds-focus\"\n 478 | # gcp_project = \"ccg24-hrzana-r-gds-gfs\"\n 479 | client = cloud.TrainingClient(gcp_project=gcp_project)\n 480 | job_id = client.create_spark_job(\n 481 |     func= base_esm_woe,\n 482 |     packages_to_install = ['PyDPU==1.1.0',\n 483 |                            'pyScoring==0.8.0.1.post1',\n 484 |                            'gcsfs',\n 485 |                            'automation_utils==0.3.0',\n 486 |                            'pyjnius<1.5.0'],\n 487 |     # func kwargs\n 488 |     train_raw_path = \"gs://pypl-bkt-rsh-row-std-gds-qpull/madmen/output/cchen16_1726110862\",\n 489 |     train_woe_path = 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/data/ensemble/sim_adp_score/cam24_il_0912_align_v2',\n 490 |     meta_cols = meta_cols,\n 491 |     model_path_list = [\n 492 |         'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/umes/cam24_ems_sapout.m',\n 493 |         'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/umes/exp11_1_week1_esm.m',\n 494 |         # 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/umes/exp10_5_week1_esm.m',\n 495 |     ],\n 496 |     # dataproc config\n 497 |     **dataproc_config['medium']\n 498 | )\n 499 | client.wait_job_for_completion(job_id, \n 500 |                                time_to_sleep=300)\n 501 | client.get_job_driver_logs(job_id)\n 502 | # /projects/xiaopzhang/shared-lib/maglev/SF_SL_RMR/umes/cam24_il_esm_20240829.m\n 503 | gcs_align_scoring_path\n 504 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-art/maglev/SF_SL/20240822/data/training/early_arrive_data_woe/\n 505 | # # 3. Package ensemble Model\n 506 | # !gsutil ls -L $gcs_adp_scoring_path\n 507 | # # !rm -r /projects/gds-focus/data/catch/IL/CAM24/scoring/cam24_il_0822_align_bs1_0829\n 508 | gcs_align_scoring_path = 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/data/ensemble/sim_adp_score/cam24_il_0912_align_v2'\n 509 | run_cmd(f\"gsutil -m cp -r {gcs_align_scoring_path} /projects/gds-focus/data/catch/IL/CAM24/scoring/\")\n 510 | # !ls -t /projects/gds-focus/data/catch/IL/CAM24/scoring/ | head -5\n 511 | # ## Align\n 512 | from model_automation.score_alignment import SAP\n 513 | from pyScoring.model import UMEModel\n 514 | from model_automation.model_ensemble import ModelEnsemble\n 515 | def align_to_rmr(tb_align_score, align_to_score,ens_ume_folder, og_spec_path, data_path=None):\n 516 |     data = pd.read_parquet(data_path)\n 517 |     data[align_to_score] = data[align_to_score].astype(float)\n 518 |     data[tb_align_score] = data[tb_align_score].astype(float)\n 519 |     data['driver_dol_wgt'] = data['driver_dol_wgt'].astype(float)\n 520 |     data_clean = data.loc[(data[align_to_score]>=0) & (data[tb_align_score]>=0),].reset_index(drop=True)\n 521 |     og_spec = UMEModel(og_spec_path)\n 522 |     or_model_output = og_spec.outputs[0]\n 523 |     or_model_name = og_spec.name\n 524 |     pjmp=SAP.SAP_GEN_MAP(input_score_df=data_clean\n 525 |                          ,target_score_df=data_clean\n 526 |                          ,binnumber=80\n 527 |                          ,input_score_name=tb_align_score\n 528 |                          ,target_score_name=align_to_score\n 529 |                          ,input_weight_name='driver_dol_wgt'\n 530 |                          ,target_weight_name='driver_dol_wgt'\n 531 |                         )\n 532 |     new_mfile=SAP.SAP_GEN_SPEC(origin_model_spec=og_spec\n 533 |                                ,map_table=pjmp\n 534 |                                ,new_spec_name=or_model_name+'_sapout'\n 535 |                                ,new_layer_name='saplayer'\n 536 |                                ,new_output_name=or_model_output+'_sapout'\n 537 |                                ,orginal_output_position=0)\n 538 |     new_mfile.save(ens_ume_folder)\n 539 |     return pjmp\n 540 | from model_automation.score_alignment import SAP\n 541 | from pyScoring.model import UMEModel\n 542 | from model_automation.model_ensemble import ModelEnsemble\n 543 | # add condition node here\n 544 | from pyScoring import ConditionBuilder, RegressionBuilder, UMEModel,ReNameBuilder,ModeltoNodeBuilder,UnNullifyBuilder,ConstantBuilder,AddBuilder\n 545 | from pyScoring.backend import DataType\n 546 | from pyScoring.graph import Graph\n 547 | def test(tb_align_score, align_to_score,ens_ume_folder, og_spec_path, data_path=None):\n 548 |     data = pd.read_parquet(data_path)\n 549 |     data[align_to_score] = data[align_to_score].astype(float)\n 550 |     data[tb_align_score] = data[tb_align_score].astype(float)\n 551 |     data['driver_dol_wgt'] = data['driver_dol_wgt'].astype(float)\n 552 |     data_clean = data.loc[(data[align_to_score]>=0) & (data[tb_align_score]>=0),].reset_index(drop=True)\n 553 |     og_spec = UMEModel(og_spec_path)\n 554 |     or_model_output = og_spec.outputs[0]\n 555 |     or_model_name = og_spec.name\n 556 |     pjmp=SAP.SAP_GEN_MAP(input_score_df=data_clean\n 557 |                          ,target_score_df=data_clean\n 558 |                          ,binnumber=80\n 559 |                          ,input_score_name=tb_align_score\n 560 |                          ,target_score_name=align_to_score\n 561 |                          ,input_weight_name='driver_dol_wgt'\n 562 |                          ,target_weight_name='driver_dol_wgt'\n 563 |                         )\n 564 |     new_mfile=SAP.SAP_GEN_SPEC(origin_model_spec=og_spec\n 565 |                                ,map_table=pjmp\n 566 |                                ,new_spec_name=or_model_name+'_sapout_tmp'\n 567 |                                ,new_layer_name='saplayer'\n 568 |                                ,new_output_name=or_model_output+'_sapout_tmp'\n 569 |                                ,orginal_output_position=0)\n 570 |     new_mfile.save(ens_ume_folder)\n 571 |     # add recurring fix here\n 572 |     final_spec = UMEModel(ens_ume_folder+\"/\"+or_model_name+'_sapout_tmp.m')\n 573 |     unnullNode = UnNullifyBuilder('usd_amount_unnull', 'usd_amount', -999.0).build()\n 574 |     recur_condition_node = ConditionBuilder('CAM24_IL_EMS_ModelScore_new', \"if ((usd_amount_unnull >= 0.0 and usd_amount_unnull <= 5000.0) and (atom22_product_flow == 'UDS_WPS_Recurring' or atom22_product_flow == 'UDS_MSEC_Recurring' or atom22_product_flow == 'Recurring') )  {lowAsp_Recurring_model_score_fix} else {other_model_score}\", \n 575 |                                      {'atom22_product_flow': DataType.STRING.getId(),\n 576 |                                      'usd_amount_unnull': DataType.DOUBLE.getId(),\n 577 |                                      'other_model_score': DataType.DOUBLE.getId(),\n 578 |                                      'lowAsp_Recurring_model_score_fix': DataType.DOUBLE.getId()}).build()\n 579 |     recur_rename_nodes = []\n 580 |     recur_rename_nodes.append(ReNameBuilder('other_model_score', final_spec.get_outputs()[0]).build())\n 581 |     recur_rename_nodes.append(ReNameBuilder('lowAsp_Recurring_model_score', final_spec.get_outputs()[0]).build())\n 582 |     recur_g = final_spec.convert_to_graph()\n 583 |     constantNode = ConstantBuilder(\"fix_score\", -50.0).build()\n 584 |     AddNode = AddBuilder(['lowAsp_Recurring_model_score', 'fix_score'], 'lowAsp_Recurring_model_score_fix').build()\n 585 |     recur_g.add_node(constantNode)\n 586 |     recur_g.add_node(AddNode)\n 587 |     recur_g.add_nodes(recur_rename_nodes)\n 588 |     recur_g.add_node(recur_condition_node)\n 589 |     recur_g.add_node(unnullNode)\n 590 |     fix_spec = recur_g.generate_model_by_graph(model_name=or_model_name+'_test')\n 591 |     fix_spec.save(ens_ume_folder)\n 592 |     return\n 593 | test(tb_align_score=source.name+\"_\"+source.outputs[0], \n 594 |              align_to_score='CAM24_EMS_sapout_CAM24_EMS_SCORE_sapout',\n 595 |              ens_ume_folder='/projects/gds-focus/data/catch/IL/CAM24/umes',\n 596 |              og_spec_path=source_path,\n 597 |              data_path=local_align_scoring_path)\n 598 | # !ls -lt /projects/gds-focus/data/catch/IL/CAM24/umes/\n 599 | test_ume = UMEModel('/projects/gds-focus/data/catch/IL/CAM24/umes/exp11_1_week1_esm_test.m')\n 600 | test_ume.name, test_ume.outputs\n 601 | target = UMEModel('/projects/gds-focus/data/makang/CAM24/source_code/model_ems/submodel/ems/cam24_ems_sapout.m')\n 602 | target.name, target.outputs\n 603 | # !ls /projects/gds-focus/data/catch/IL/CAM24/umes/exp10_esm/\n 604 | # source = UMEModel(\"/projects/gds-focus/data/junrzhang/CAM_IL/Filter_YG_Exp/cam24_il_ems_model.m\")\n 605 | # source = UMEModel(\"/projects/gds-focus/data/catch/IL/CAM24/umes/exp11_esm/exp10_4_week1_esm.m\")\n 606 | source_path = \"/projects/gds-focus/data/catch/IL/CAM24/umes/exp10_esm/exp11_1_week1_esm.m\"\n 607 | source = UMEModel(source_path)\n 608 | source.name, source.outputs\n 609 | local_align_scoring_path = '/projects/gds-focus/data/catch/IL/CAM24/scoring/cam24_il_0912_align_v2'\n 610 | # source_df_raw = pd.read_parquet(local_align_scoring_path)\n 611 | # ume_adp_local_path_list = [os.path.join(ume_adp_local_dir, filepath) for filepath in os.listdir(ume_adp_local_dir)]\n 612 | tag_column = 'driver_is_brm3_4_11_15_bad' # fix\n 613 | weight_column = 'driver_train_wgt_3' # need experiment\n 614 | # ems_model = UMEModel(adaptive_model_path)\n 615 | # align to RMR \n 616 | pjmp = align_to_rmr(tb_align_score=source.name+\"_\"+source.outputs[0], \n 617 |              align_to_score='CAM24_EMS_sapout_CAM24_EMS_SCORE_sapout',\n 618 |              ens_ume_folder='/projects/gds-focus/data/catch/IL/CAM24/umes',\n 619 |              og_spec_path=source_path,\n 620 |              data_path=local_align_scoring_path)\n 621 | tb_align_score='cam24_il_ems_model_bs1_CAM24_IL_EMS_bs1_ModelScore'\n 622 | align_to_score='CAM24_EMS_V2_sapout_CAM24_EMS_SCORE_V2_sapout'\n 623 | ens_ume_folder='/projects/gds-focus/data/catch/IL/CAM24/umes'\n 624 | og_spec_path='/projects/gds-focus/data/junrzhang/CAM_IL/Filter_YG_Exp/cam24_il_ems_model_bs1.m'\n 625 | data_path='/projects/gds-focus/data/catch/IL/CAM24/scoring/cam24_il_0822_align_bs1_0829'\n 626 | data = pd.read_parquet(data_path)\n 627 | data[align_to_score] = data[align_to_score].astype(float)\n 628 | data[tb_align_score] = data[tb_align_score].astype(float)\n 629 | data['driver_dol_wgt'] = data['driver_dol_wgt'].astype(float)\n 630 | # data_clean = data.loc[(data[align_to_score]>=0) & (data[tb_align_score]>=0),].reset_index(drop=True)\n 631 | # print(data_clean.shape)\n 632 | # og_spec = UMEModel(og_spec_path)\n 633 | # or_model_output = og_spec.outputs[0]\n 634 | # or_model_name = og_spec.name\n 635 | condition_node = ConditionBuilder('CAM24_IL_EMS_ModelScore', \"if(consumer_type == 2.0 or consumer_type == 3.0) {tc_model_score} else {yg_model_score}\", \n 636 |                                  {'consumer_type': DataType.DOUBLE.getId(),\n 637 |                                  'tc_model_score': DataType.DOUBLE.getId(),\n 638 |                                  'yg_model_score': DataType.DOUBLE.getId()}).build()\n 639 | pjmp=SAP.SAP_GEN_MAP(input_score_df=data_clean\n 640 |      ,target_score_df=data_clean\n 641 |      ,binnumber=80\n 642 |      ,input_score_name=tb_align_score\n 643 |      ,target_score_name=align_to_score\n 644 |      ,input_weight_name='driver_dol_wgt'\n 645 |      ,target_weight_name='driver_dol_wgt'\n 646 |     )\n 647 | data.query(f\" {tb_align_score} == 0.0\")['driver_cnsr_seg'].value_counts()\n 648 | data.query(f\" {tb_align_score} == 0.0\")['driver_is_brm3_4_11_15_bad'].value_counts()\n 649 | 72155/(72155+11)\n 650 | data[data.cam24_il_ems_model_bs1_CAM24_IL_EMS_bs1_ModelScore == 0.0].shape\n 651 | 72166/7743324\n 652 | 7743324/1000, 7743324/80\n 653 | # # 4. Final UME scoring\n 654 | # !gsutil ls -l gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/IL_CAM24/ | grep '20240926'\n 655 | # !gsutil cp gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/IL_CAM24/cam24_il_esm_20240926_sapout_final.m gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/umes/\n 656 | # !find /projects/gds-focus/data/catch/IL/CAM_variable/models/umes/ -name 'exp8*re*' -print\n 657 | gcs_ume_path = \"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/umes\"\n 658 | local_ume_list = [\n 659 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp8_1_re_best_tot_cond.m',\n 660 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp8_1_re_epoch15_tot_cond.m',\n 661 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp8_3_re_best_tot_cond.m',\n 662 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp8_3_re_epoch15_tot_cond.m',\n 663 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp8_5_re_best_tot_cond.m',\n 664 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp8_5_re_epoch15_tot_cond.m',\n 665 | ]\n 666 | gcs_ume_path_list = [\n 667 |     'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/umes/cam24_il_esm_20240926_sapout_final.m',\n 668 | ]\n 669 | for ume in local_ume_list:\n 670 |     run_cmd(f\"gsutil -m cp -r {ume} {gcs_ume_path}/\")\n 671 |     gcs_ume_path_list.append(f\"{gcs_ume_path}/{ume.split('/')[-1]}\")\n 672 | gcs_ume_path_list\n 673 | scoring_part = {\n 674 |     'part_1':[\n 675 |         'gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/IL_CAM24/cam24_il_esm_20240926_sapout.m',\n 676 |  'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/umes/exp8_1_re_best_tot_cond.m',\n 677 |  'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/umes/exp8_1_re_epoch15_tot_cond.m',\n 678 |  'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/umes/exp8_3_re_best_tot_cond.m',\n 679 |     ],\n 680 |     'part_2':[\n 681 |         'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/umes/exp8_3_re_epoch15_tot_cond.m',\n 682 |  'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/umes/exp8_5_re_best_tot_cond.m',\n 683 |  'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/umes/exp8_5_re_epoch15_tot_cond.m'\n 684 |     ],\n 685 | }\n 686 | def scoring_woe(input_raw_path,\n 687 |         output_woe_path,\n 688 |         model_path_list):\n 689 |     from pyScoring.model import ModelScorer\n 690 |     from pyScoring import UMEModel\n 691 |     from py_dpu import load_parquet, load_pig\n 692 |     from automation_utils.gcp.GSUtil import GSUtilHelper\n 693 |     from pyspark.sql.functions import array, col\n 694 |     from pyspark.sql import functions as F\n 695 |     # load and dedup \n 696 |     data = load_parquet(spark, input_raw_path)\n 697 |     data = data.drop_duplicates(subset=[\"driver_trans_id\"])\n 698 |     # filters = \"\"\"\n 699 |     #     driver_pop_tag == 1.0 or driver_cc_control == '1' or driver_cc_control == '1.0'\n 700 |     # \"\"\"\n 701 |     # filters = \"\"\"\n 702 |     #     driver_product_flow_rollup == 'Branded Recurring'\n 703 |     # \"\"\"\n 704 |     # data = data.where(filters)\n 705 |     # data = data.withColumn(\"driver_pmt_start_date\", F.to_date(col(\"driver_pmt_start_date\")))  \n 706 |     # data = data.filter((col(\"driver_pmt_start_date\") >= \"2024-09-01\") & (col(\"driver_pmt_start_date\") <= \"2024-09-15\"))  \n 707 |     tmp_file_list = []\n 708 |     input_list = []\n 709 |     model_outputs = []\n 710 |     features_all = []\n 711 |     for file_path in model_path_list:\n 712 |         model_name = file_path.split('/')[-1]\n 713 |         GSUtilHelper.cp(file_path, f'/tmp/model_spec/{model_name}')\n 714 |         tmp_file_list.append(f'/tmp/model_spec/{model_name}')\n 715 |         model = UMEModel(f'/tmp/model_spec/{model_name}')\n 716 |         final_outputs = [model.name+'_'+c for c in model.outputs]\n 717 |         model_outputs.extend(final_outputs)\n 718 |         features_all.extend(model.get_inputs())\n 719 |     if len(set(features_all) - set(data.columns)) > 0:\n 720 |         assert len(set(features_all) - set(data.columns)) == 0\n 721 |     data = data.withColumn(\"index\", F.monotonically_increasing_id()) # fix\n 722 |     scorer = ModelScorer(spark, validate=False)\n 723 |     data = scorer.create_score_df(input_df = data, \n 724 |                                   mfile_paths = tmp_file_list)\n 725 |     meta_cols = [c for c in data.columns if c.startswith(\"driver_\")]\n 726 |     data = data.select(*(col(c).cast(\"float\").alias(c) for c in model_outputs), *meta_cols).repartition(400)\n 727 |     data.write.format(\"parquet\").mode(\"overwrite\").save(output_woe_path)\n 728 |     return\n 729 | from model_automation.gcp import dataproc_config\n 730 | from aml import cloud_v1 as cloud\n 731 | gcp_project = \"ccg24-hrzana-gds-focus\"\n 732 | client = cloud.TrainingClient(gcp_project=gcp_project)\n 733 | # adjust propertity\n 734 | dataproc_config['large']['spark_properties']['spark.driver.maxResultSize'] = '48g'\n 735 | dataproc_config['large']['spark_properties']['spark.yarn.driver.memoryOverhead'] = '4096'\n 736 | dataproc_config['large']['spark_properties']['spark.yarn.executor.memoryOverhead'] = '4096'\n 737 | # job_ids = {}\n 738 | for part_idx, gcs_ume_path_list in scoring_part.items():\n 739 |     job_ids[part_idx] = client.create_spark_job(\n 740 |         func= scoring_woe,\n 741 |         packages_to_install = ['PyDPU==1.1.0',\n 742 |                                'pyScoring==0.8.0.1.post1',\n 743 |                                'gcsfs',\n 744 |                                'automation_utils==0.3.0',\n 745 |                                'pyjnius<1.5.0'],\n 746 |         input_raw_path = \"gs://pypl-bkt-rsh-row-std-gds-qpull/madmen/output/cchen16_1736908971\",\n 747 |         output_woe_path = f\"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/final_scoring/cam_il_model_scoring_0116_{part_idx}\",\n 748 |         model_path_list = gcs_ume_path_list,\n 749 |         # dataproc config\n 750 |         **dataproc_config['large']\n 751 |     )\n 752 | for k,v in job_ids.items():\n 753 | job_id = client.create_spark_job(\n 754 |     func= scoring_woe,\n 755 |     packages_to_install = ['PyDPU==1.1.0',\n 756 |                            'pyScoring==0.8.0.1.post1',\n 757 |                            'gcsfs',\n 758 |                            'automation_utils==0.3.0',\n 759 |                            'pyjnius<1.5.0'],\n 760 |     input_raw_path = \"gs://pypl-bkt-rsh-row-std-gds-qpull/madmen/output/cchen16_1736571579\",\n 761 |     output_woe_path = f\"gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/final_scoring/cam_il_model_scoring_0114_exp9\",\n 762 |     model_path_list = gcs_ume_path_list,\n 763 |     # dataproc config\n 764 |     **dataproc_config['large']\n 765 | )\n 766 | client.wait_job_for_completion(job_ids['part_2'],\n 767 |                                time_to_sleep=300)\n 768 | client.wait_job_for_completion(job_ids['part_1'],\n 769 |                                time_to_sleep=300)\n 770 | client.get_job_logs(job_ids['part_1'])\n 771 | # !ls -tl /projects/cchen16/shared-lib/maglev/SF_SL_RMR/umes/ | grep '0926'\n 772 | local_ume_list = [\n 773 | '/projects/gds-focus/data/catch/IL/CAM_variable/models/umes/exp8_5_re_epoch15_tot_cond.m',\n 774 | '/projects/cchen16/shared-lib/maglev/SF_SL_RMR/umes/cam24_il_esm_20240926_sapout_final.m'\n 775 | ]\n 776 | features_all = []\n 777 | for file_path in local_ume_list:\n 778 |     model = UMEModel(file_path)\n 779 |     input_vars = model.get_inputs()\n 780 |     for var in input_vars:\n 781 |         if var.startswith(\"driver_\"):\n 782 |     features_all.extend(model.get_inputs())\n 783 | len(set(features_all))\n 784 | len(set(features_all))\n 785 | oot_cols = read_line_file('/projects/gds-focus/data/catch/IL/CAM_variable/files/oot_0111_3505.txt')\n 786 | set(features_all)-set(oot_cols)\n 787 | write_line_file('/projects/gds-focus/data/catch/IL/CAM_variable/files/oot_1127_2747.txt', list(set(features_all)))\n 788 | # ## Copy scoring back to local\n 789 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/final_scoring/ | grep 'part'\n 790 | scoring_ls = [\n 791 | 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/final_scoring/cam_il_model_scoring_0116_part_1',\n 792 | 'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_variable/data/final_scoring/cam_il_model_scoring_0116_part_2',\n 793 | ]\n 794 | local_adp_scoring_dir = '/projects/gds-focus/data/catch/IL/CAM_variable/data/scoring'\n 795 | # gcs_final_scoring_path = f\"{gcs_root_dir}/data/final_scoring/cam_new_var_0912_exp1_1_8_scoring\"\n 796 | for gcs_final_scoring_path in scoring_ls:\n 797 |     run_cmd(f\"gsutil -m cp -r {gcs_final_scoring_path} {local_adp_scoring_dir}/\")\n 798 | # # !rm -r /projects/gds-focus/data/catch/IL/CAM_variable/data/scoring/cam_il_model_scoring_0116_part_2\n 799 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/data/final_scoring/cam_rmr_0822 | head\n 800 | # !gsutil -m cp -r gs://pypl-bkt-rsh-row-std-gds-qpull/out/cchen16_1736152793/cchen16_1736152793 /projects/gds-focus/data/catch/IL/CAM24/scoring/\n 801 | # !gsutil du -h -s gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/data/replay_data_ct_mmoe_embedding_woe/\n 802 | gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/data/replay_data_ct_mmoe_embedding_woe\n 803 | # !cat /projects/jianhe3/shared-lib/maglev/SF_SL_RMR/gcp_logs/exp9_2_trainer_all_finetune_595b44cb-62f2-37f9-a8dd-3892f061b47f.log",
    "rmr_agent/repos/CAM_variable_research/5_eval.py": "   1 | from model_automation.model_evaluation import model_eval\n   2 | import pandas as pd\n   3 | import sys\n   4 | from py_dpu import load_pig\n   5 | import os\n   6 | pd.set_option('display.max_rows', 500)\n   7 | pd.set_option('display.max_columns', 500)\n   8 | pd.set_option('display.width', 1000)\n   9 | # !ls -htl /projects/gds-focus/data/catch/IL/CAM_variable/data/scoring/ \n  10 | score_name = f\"cam_il_model_scoring_0116\"\n  11 | local_root_dir = \"/projects/gds-focus/data/catch/IL/CAM_variable\"\n  12 | local_final_scoring_path = f\"{local_root_dir}/data/scoring/{score_name}\"\n  13 | outputFile = f\"{local_root_dir}/perf_output/{score_name}_perf.csv\"\n  14 | if not os.path.exists(outputFile):\n  15 |     os.makedirs(os.path.dirname(outputFile), exist_ok = True)\n  16 | df1 = pd.read_parquet(local_final_scoring_path+\"_part_1\")\n  17 | df2 = pd.read_parquet(local_final_scoring_path+\"_part_2\")\n  18 | df1.shape, df2.shape\n  19 | list(df2.columns[:5])\n  20 | df = df1.merge(df2[[\n  21 |      'driver_trans_id_fix',\n  22 |      'exp8_3_re_epoch15_tot_cond_cam24_flow_score',\n  23 |      'exp8_5_re_best_tot_cond_cam24_flow_score',\n  24 |      'exp8_5_re_epoch15_tot_cond_cam24_flow_score',\n  25 | ]], on='driver_trans_id_fix', how='inner')\n  26 | df.shape\n  27 | df = pd.read_parquet(local_final_scoring_path)\n  28 | # df = pd.read_csv(local_final_scoring_path, sep='\\x07')\n  29 | df.shape\n  30 | df.driver_pmt_start_date.min(), df.driver_pmt_start_date.max()\n  31 | # df.columns = [\"driver_\"+c for c in df.columns]\n  32 | df['driver_pop_tag'].value_counts()\n  33 | list(df.columns)\n  34 | df = df.rename(columns={\n  35 |     # 'CAM24_EMS_V2_sapout_CAM24_EMS_SCORE_V2_sapout':'cam24_09_rmr',\n  36 |     'driver_madmen_is_cc_bad':'driver_is_brm3_4_bad',\n  37 |     'driver_bad3_4_nloss': 'driver_nloss',\n  38 |     'cam24_il_esm_20240926_sapout_CAM24_IL_EMS_ModelScore_sapout': 'cam_il_0926_sapout',\n  39 | })\n  40 | score_list = [\n  41 | 'cam_il_0926_sapout',\n  42 | 'exp8_1_re_best_tot_cond_cam24_flow_score',\n  43 |  'exp8_1_re_epoch15_tot_cond_cam24_flow_score',\n  44 |  'exp8_3_re_best_tot_cond_cam24_flow_score',\n  45 | 'exp8_3_re_epoch15_tot_cond_cam24_flow_score',\n  46 |  'exp8_5_re_best_tot_cond_cam24_flow_score',\n  47 |  'exp8_5_re_epoch15_tot_cond_cam24_flow_score'\n  48 | # 'cam24_09_rmr',\n  49 | ]\n  50 | # ### Add dimension columns\n  51 | import datetime\n  52 | def get_mth_cutoff(dt):\n  53 |     dt = datetime.datetime.strptime(dt, '%Y-%m-%d').date()\n  54 |     if datetime.date(2024, 7, 12) <= dt <= datetime.date(2024, 8, 11):\n  55 |         return '0712-0811'\n  56 |     elif datetime.date(2024, 8, 12) <= dt <= datetime.date(2024, 9, 12):\n  57 |         return '0812-0911'\n  58 |     elif datetime.date(2024, 9, 13) <= dt <= datetime.date(2024, 10, 12):\n  59 |         return '0912-1012'\n  60 |     else:\n  61 |         return '0912-1012'\n  62 | import datetime\n  63 | def get_week_cutoff(dt):\n  64 |     dt = datetime.datetime.strptime(dt, '%Y-%m-%d').date()\n  65 |     if datetime.date(2024, 7, 28) <= dt <= datetime.date(2024, 8, 19):\n  66 |         return '0728-0819'\n  67 |     elif datetime.date(2024, 8, 20) <= dt <= datetime.date(2024, 9, 12):\n  68 |         return '0820-0912'\n  69 |     elif datetime.date(2024, 9, 13) <= dt <= datetime.date(2024, 9, 26):\n  70 |         return '0913-0926'\n  71 |     elif datetime.date(2024, 9, 27) <= dt <= datetime.date(2024, 10, 13):\n  72 |         return '0927-1013'\n  73 |     else:\n  74 |         return '1014-1026'\n  75 | df[\"mth_period\"] = df['driver_pmt_start_date'].apply(lambda x:get_mth_cutoff(x))\n  76 | df[\"mth_period\"].value_counts().sort_index()\n  77 | # # score_list = score_parts[part_idx]\n  78 | # used_cols = score_list +\\\n  79 | # [\n  80 | # # \"driver_eval_t11_to_t17_week_id\",\n  81 | # # \"driver_madmen_monthly\",\n  82 | # \"driver_cnsr_seg\",\n  83 | # \"driver_product_flow_rollup\",\n  84 | # \"driver_is_brm3_4_bad\",\n  85 | # 'driver_dol_wgt',\n  86 | # \"driver_nloss\",\n  87 | # 'driver_unit_wgt',\n  88 | # 'driver_pop_tag',\n  89 | # # 'driver_usd_amt',\n  90 | # # 'driver_cc_control',\n  91 | # 'driver_cam22_ems_score'\n  92 | # # 'mth_period',\n  93 | # # 'driver_is_sndr_id_train_set',\n  94 | # # 'driver_is_y_g'\n  95 | # ]\n  96 | for col in [\n  97 | \"driver_is_brm3_4_bad\",\n  98 | 'driver_dol_wgt',\n  99 | 'driver_nloss',\n 100 | 'driver_unit_wgt',\n 101 | # 'driver_usd_amt',\n 102 | 'driver_cam22_ems_score'\n 103 | ]:\n 104 |     df[col] = df[col].astype(float)\n 105 | # # Customized Parameters\n 106 | # # !awk -F '\\a' '{print $9}' /projects/cchen16/shared-lib/maglev/SF_SL_RMR/evaluation.csv | sort | uniq  \n 107 | param = {\n 108 |     \"df\": df,\n 109 |     # \"dataPath\": \"/projects/cchen16/shared-lib/maglev/SF_SL_RMR/eval_data.csv\",\n 110 |     \"delimiter\": \"\\u0007\",\n 111 |     \"scoreList\": score_list,\n 112 |     # \"filter\": \" driver_pop_tag == 'cg' \",\n 113 |     \"dimList\": [\n 114 |                 # 'pop_tag',\n 115 |                 # 'driver_pop_tag*driver_product_flow_rollup*mth_period',\n 116 |                 'driver_product_flow_rollup*driver_cnsr_seg',\n 117 |                 'driver_product_flow_rollup*driver_pop_tag',\n 118 |                 'driver_cnsr_seg*driver_pop_tag',\n 119 |                 # 'madmen_monthly*cnsr_seg',\n 120 |                 # 'driver_madmen_monthly*driver_product_flow_rollup',\n 121 |                 # 'eval_t11_to_t17_week_id*cnsr_seg',\n 122 |                 # 'eval_t11_to_t17_week_id*product_flow_rollup',\n 123 |                 # 'driver_product_flow_rollup*driver_pop_tag'\n 124 |                ],\n 125 |     \"OP\": 1000,\n 126 |     \"TopOP\": 100,\n 127 |     \"badList\": ['driver_is_brm3_4_bad'],\n 128 |     \"xWeight\": ['driver_dol_wgt', 'driver_dol_wgt'],\n 129 |     \"yWeight\": ['driver_dol_wgt', 'driver_nloss'],\n 130 |     'weightAlias' : ['driver_dol_wgt', 'dol_wgt_nloss']\n 131 | }\n 132 | res2 = model_eval(\"evalkit\", param)\n 133 | res2 = res2.dropna(subset=['score_value'], axis=0)\n 134 | for xw, yw, aw in zip(param['xWeight'], param['yWeight'], param['weightAlias']):\n 135 |     res2['weight_alias']=res2['weight_alias'].replace({xw+'-'+yw: aw})\n 136 | res2.shape\n 137 | res2['dimension_value'].unique()\n 138 | res2.to_csv(outputFile, index=False, sep='\\u0007')\n 139 | # res2 = pd.read_csv(\"/projects/gds-focus/data/catch/IL/CAM_variable/perf_output/cam_il_model_scoring_0108_perf.csv\",sep='\\x07')\n 140 | # ## base check\n 141 | pd.concat([res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 2 and dimension_value.str.startswith('T*cg').values \")\\\n 142 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 143 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 6 and dimension_value.str.startswith('C*cg').values \")\\\n 144 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 145 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('G*cg').values \")\\\n 146 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 147 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('Y*cg').values \")\\\n 148 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 149 |           res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('P2P-F&F*cg').values \")\\\n 150 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 151 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 10 and dimension_value.str.startswith('P2P-G&S*cg').values \")\\\n 152 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 153 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 10 and dimension_value.str.startswith('Branded XO*cg').values \")\\\n 154 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 155 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('Branded Recurring*cg').values \")\\\n 156 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 157 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('eBay MOR*cg').values \")\\\n 158 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 159 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('Other*cg').values \")\\\n 160 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 161 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 7 and dimension_value.str.startswith('cg').values \")\\\n 162 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T\n 163 |           ]\n 164 |     ).T.reset_index()\n 165 | pd.concat([res2.query(\"weight_alias == 'driver_dol_wgt' and op == 2 and dimension_value.str.startswith('T*cg').values \")\\\n 166 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 167 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 6 and dimension_value.str.startswith('C*cg').values \")\\\n 168 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 169 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 30 and dimension_value.str.startswith('G*cg').values \")\\\n 170 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 171 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 70 and dimension_value.str.startswith('Y*cg').values \")\\\n 172 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 173 |           res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('P2P-F&F*cg').values \")\\\n 174 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 175 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('P2P-G&S*cg').values \")\\\n 176 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 177 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('Branded XO*cg').values \")\\\n 178 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 179 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('Branded Recurring*cg').values \")\\\n 180 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 181 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('eBay MOR*cg').values \")\\\n 182 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 183 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('Other*cg').values \")\\\n 184 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 185 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 7 and dimension_value.str.startswith('cg').values \")\\\n 186 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T\n 187 |           ]\n 188 |     ).T.reset_index()\n 189 | # ## Flow check\n 190 | # flow = 'eBay MOR'\n 191 | # flow = 'Recurring'\n 192 | # flow = 'P2P-F&F'\n 193 | # flow = 'P2P-G&S'\n 194 | flow = 'Branded XO'\n 195 | pd.concat([res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 2 and dimension_value.str.endswith('{flow}*T').values \")\\\n 196 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 197 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 6 and dimension_value.str.endswith('{flow}*C').values \")\\\n 198 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 199 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.endswith('{flow}*G').values \")\\\n 200 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 201 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.endswith('{flow}*Y').values \")\\\n 202 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 203 |           ]\n 204 |     ).T.reset_index()\n 205 | # !find {local_final_scoring_path}/ '*part*' -print \n 206 | tmp = pd.read_csv('/projects/gds-focus/data/catch/IL/CAM24/perf_output/cam_il_oot_2312_2405_perf.csv', sep='\\x07')\n 207 | tmp['score_name'].unique()\n 208 | tmp['dimension_value'].unique()\n 209 | tmp2 = tmp[(tmp.score_name == 'CAM24_DeepFM_new_transfer_202305_202403_cam24_deepfm_new_transfer_202305_202403_cls_src')]\n 210 | pd.concat([tmp2.query(\"weight_alias == 'dol_wgt_nloss' and op == 2 and dimension_value.str.startswith('2024-02-01*T').values \"),\n 211 |            tmp2.query(\"weight_alias == 'dol_wgt_nloss' and op == 6 and dimension_value.str.startswith('2024-02-01*C').values \"),\n 212 |            tmp2.query(\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('2024-02-01*G').values \"),\n 213 |            tmp2.query(\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('2024-02-01*Y').values \"),\n 214 |            tmp2.query(\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('2024-02-01*P2P-F&F').values \"),\n 215 |            tmp2.query(\"weight_alias == 'dol_wgt_nloss' and op == 10 and dimension_value.str.startswith('2024-02-01*P2P-G&S').values \"),\n 216 |            tmp2.query(\"weight_alias == 'dol_wgt_nloss' and op == 10 and dimension_value.str.startswith('2024-02-01*Branded XO').values \"),\n 217 |            tmp2.query(\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('2024-02-01*Branded Recurring').values \"),\n 218 |            tmp2.query(\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('2024-02-01*eBay MOR').values \"),\n 219 |            tmp2.query(\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('2024-02-01*Other').values \")\n 220 |           ]\n 221 |     )\n 222 | # ## CO check\n 223 | # res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('G*cg').values \")## CO check\n 224 | res2.query(\"weight_alias == 'driver_dol_wgt' and op == 6 and dimension_value.str.startswith('C*cg').values \")## CO check\n 225 | res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('Y*cg').values \")## CO check\n 226 | for score in score_list:\n 227 |     df[score] = pd.to_numeric(df[score])\n 228 | df.query(\" exp6_7_tot_cond_cam24_flow_score >= 0.1 and driver_pop_tag == 'cg' \")['driver_is_brm3_4_bad'].value_counts()\n 229 | df['driver_is_brm3_4_bad'].value_counts()\n 230 | s1 = df.query(\" driver_cnsr_seg == 'Y' and exp7_1_all_flow_cond_cam24_flow_score >= 0.10475\")\n 231 | s1.groupby(['driver_is_brm3_4_bad','driver_product_flow_rollup'])\\\n 232 |   .agg({\"driver_trans_id\":\"count\"}).pivot_table(index='driver_product_flow_rollup', columns='driver_is_brm3_4_bad', values='driver_trans_id')\n 233 | s1 = df.query(\" driver_cnsr_seg == 'Y' and exp6_7_tot_cond_cam24_flow_score >= 0.10475 \")\n 234 | s1.groupby(['driver_is_brm3_4_bad','driver_product_flow_rollup'])\\\n 235 |   .agg({\"driver_trans_id\":\"count\"}).pivot_table(index='driver_product_flow_rollup', columns='driver_is_brm3_4_bad', values='driver_trans_id')\n 236 | s2 = df.query(\" driver_product_flow_rollup == 'Branded XO' and exp5_6_ebaymor_cond_cam24_flow_score >= 0.03693\")\n 237 | s2.groupby('driver_is_brm3_4_bad')\\\n 238 | .agg({\"driver_trans_id\":\"count\",\"driver_nloss\":\"sum\",\"driver_dol_wgt\":\"sum\"})\n 239 | 44/1219\n 240 | len(set(s1[s1.driver_is_brm3_4_bad == 1.0]['driver_trans_id'].values.tolist())&set(s2[s2.driver_is_brm3_4_bad == 1.0]['driver_trans_id'].values.tolist()))\n 241 | # ## Perf check\n 242 | # !echo {local_root_dir}/perf_output\n 243 | # !find /projects/gds-focus/data/catch/IL/CAM_variable/perf_output/ -name '*part*' -print\n 244 | res2 = pd.concat([\n 245 | pd.read_csv('/projects/gds-focus/data/catch/IL/CAM_variable/perf_output/cam_new_var_0912_scoring_part1_scoring_perf.csv',sep='\\x07'),\n 246 | pd.read_csv('/projects/gds-focus/data/catch/IL/CAM_variable/perf_output/cam_new_var_0912_scoring_part2_scoring_perf.csv',sep='\\x07'),\n 247 | pd.read_csv('/projects/gds-focus/data/catch/IL/CAM_variable/perf_output/cam_new_var_0912_scoring_part3_scoring_perf.csv',sep='\\x07'),\n 248 | pd.read_csv('/projects/gds-focus/data/catch/IL/CAM_variable/perf_output/cam_new_var_0912_scoring_part4_scoring_perf.csv',sep='\\x07'),\n 249 | pd.read_csv('/projects/gds-focus/data/catch/IL/CAM_variable/perf_output/cam_new_var_0912_scoring_part5_scoring_perf.csv',sep='\\x07'),\n 250 | ])\n 251 | res2.shape\n 252 | df['mth_period'].unique()\n 253 | res2['score_name'].unique()\n 254 | res2['dimension_value'].unique()\n 255 | res2 = res2[~res2.score_name.str.contains(\"tc_cls\")]\n 256 | res2 = res2[~res2.score_name.str.contains(\"exp2\")]\n 257 | res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('0812-0911*').values and dimension_value.str.endswith('*G').values\")\\\n 258 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').reset_index()\n 259 | flow = 'Other'\n 260 | res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('0812-0911*').values and dimension_value.str.endswith('*G').values\")\\\n 261 | tmp\n 262 | tmp = df[df.driver_is_brm3_4_bad == 1.0].groupby(['driver_product_flow_rollup', 'driver_cnsr_seg'])['driver_dol_wgt'].sum().reset_index()\n 263 | tmp2 = tmp.pivot_table(columns='driver_product_flow_rollup', index='driver_cnsr_seg', values='driver_dol_wgt')/(tmp['driver_dol_wgt'].sum(axis=0))\n 264 | tmp2.applymap(lambda x: f\"{x*100:.2f}%\" if not pd.isna(x) else '0.0%')\n 265 | flow = 'P2P-F&F'\n 266 | res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('0912-1012*').values and dimension_value.str.endswith('*G').values\")\\\n 267 | flow = 'eBay MOR'\n 268 | res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('0912-1012*{flow}').values and dimension_value.str.endswith('*Y').values\")\\\n 269 | # res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('0912-1012*{flow}').values and dimension_value.str.endswith('*G').values\")\\\n 270 | flow = 'Branded XO'\n 271 | res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 50 and dimension_value.str.startswith('0912-1012*').values and dimension_value.str.endswith('*Y').values\")\\\n 272 | flow = 'Other'\n 273 | res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('0912-1012*').values and dimension_value.str.endswith('*Y').values\")\\\n 274 | flow = 'P2P-G&S'\n 275 | res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 45 and dimension_value.str.startswith('0912-1012*').values and dimension_value.str.endswith('*Y').values\")\\\n 276 | flow = 'Branded XO'\n 277 | res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 50 and dimension_value.str.startswith('0912-1012*').values and dimension_value.str.endswith('*Y').values\")\\\n 278 | flow = 'eBay MOR'\n 279 | res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('0912-1012*').values and dimension_value.str.endswith('*Y').values\")\\\n 280 | flow = 'eBay MOR'\n 281 | res2.query(\"weight_alias == 'driver_dol_wgt' and op == 70 and dimension_value.str.startswith('0912-1012*').values and dimension_value.str.endswith('*Y').values\")\\\n 282 | res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('0912-1012*').values and dimension_value.str.endswith('*G').values\")\\\n 283 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').reset_index()\n 284 | res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('0912-1012*').values and dimension_value.str.endswith('*Y').values\")\\\n 285 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').reset_index()\n 286 | pd.concat([res2.query(\"weight_alias == 'driver_dol_wgt' and op == 2 and dimension_value.str.startswith('T*cg').values \")\\\n 287 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 288 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 6 and dimension_value.str.startswith('C*cg').values \")\\\n 289 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 290 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 30 and dimension_value.str.startswith('G*cg').values \")\\\n 291 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 292 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 70 and dimension_value.str.startswith('Y*cg').values \")\\\n 293 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 294 |           res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('P2P-F&F*cg').values \")\\\n 295 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 296 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('P2P-G&S*cg').values \")\\\n 297 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 298 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('Branded XO*cg').values \")\\\n 299 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 300 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('Branded Recurring*cg').values \")\\\n 301 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 302 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('eBay MOR*cg').values \")\\\n 303 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 304 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('Other*cg').values \")\\\n 305 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 306 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 7 and dimension_value.str.startswith('cg').values \")\\\n 307 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T\n 308 |           ]\n 309 |     ).T.reset_index()\n 310 | seg_ls = [('T',2),('C',6),('G',30),('Y',70)]\n 311 | flow_ls = [('P2P-F&F',15), ('P2P-G&S', 10), ('Branded XO',10), ('Branded Recurring',5), ('eBay MOR',5), ('Other',15)]\n 312 | all_seg = seg_ls+flow_ls\n 313 | wgt = 'dol_wgt_nloss'\n 314 | # wgt = 'driver_dol_wgt'\n 315 | period = '0912-1012'\n 316 | value_ls = []\n 317 | pivot = pd.DataFrame([])\n 318 | for seg, op in all_seg:\n 319 |     tmp_value = f\"cg*{seg}*{period}\"\n 320 |     tmp_pivot = res2.query(f\"weight_alias == '{wgt}' and op == {op} and dimension_value.str.startswith('{tmp_value}').values \")\\\n 321 |                     .pivot(index='score_name', \n 322 |                            columns='dimension_value', \n 323 |                            values='catch_rate').T\n 324 |     pivot = pd.concat([pivot, tmp_pivot])\n 325 | pivot.T.reset_index()\n 326 | res2['dimension_value'].unique()\n 327 | seg_ls = [('T',2),('C',6),('G',30),('Y',70)]\n 328 | flow_ls = [('P2P-F&F',15), ('P2P-G&S', 10), ('Branded XO',10), ('Branded Recurring',5), ('eBay MOR',5), ('Other',15)]\n 329 | all_seg = seg_ls+flow_ls\n 330 | wgt = 'dol_wgt_nloss'\n 331 | # wgt = 'driver_dol_wgt'\n 332 | period = '0912-1012'\n 333 | value_ls = []\n 334 | pivot = pd.DataFrame([])\n 335 | score_names = ['cam_il_0912_sapout','exp1_2_cam_cls', 'exp1_3_recur_cam_cls', 'exp1_3_recur_cam_cls','exp3_2_tot_cam_yg_cls','exp3_3_recur_cam_yg_cls']\n 336 | for seg, op in all_seg:\n 337 |     tmp_value = f\"{period}*{seg}\"\n 338 |     tmp_pivot = res2.query(f\"weight_alias == '{wgt}' and op == {op} and dimension_value.str.startswith('{tmp_value}').values \")\\\n 339 |                     .pivot(index='score_name', \n 340 |                            columns='dimension_value', \n 341 |                            values='catch_rate').T\n 342 |     pivot = pd.concat([pivot, tmp_pivot])\n 343 | pivot.T.reset_index()\n 344 | seg_ls = [('T',2),('C',6),('G',30),('Y',70)]\n 345 | flow_ls = [('P2P-F&F',15), ('P2P-G&S', 10), ('Branded XO',10), ('Branded Recurring',5), ('eBay MOR',5), ('Other',15)]\n 346 | all_seg = seg_ls+flow_ls\n 347 | wgt = 'dol_wgt_nloss'\n 348 | # wgt = 'driver_dol_wgt'\n 349 | period = '0812-0911'\n 350 | value_ls = []\n 351 | pivot = pd.DataFrame([])\n 352 | for seg, op in all_seg:\n 353 |     tmp_value = f\"{period}*{seg}*Y\"\n 354 |     tmp_pivot = res2.query(f\"weight_alias == '{wgt}' and op == {op} and dimension_value.str.startswith('{tmp_value}').values \")\\\n 355 |                     .pivot(index='score_name', \n 356 |                            columns='dimension_value', \n 357 |                            values='catch_rate').T\n 358 |     pivot = pd.concat([pivot, tmp_pivot])\n 359 | pivot.T.reset_index()\n 360 | tmp3 = res2[res2.score_name == 'driver_RMR_MODEL_SF_2_model_score1']\n 361 | pd.concat([tmp3.query(\"weight_alias == 'driver_dol_wgt' and op == 2 and dimension_value.str.startswith('cg*T*0912-1012').values \"),\n 362 |            tmp3.query(\"weight_alias == 'driver_dol_wgt' and op == 6 and dimension_value.str.startswith('cg*C*0912-1012').values \"),\n 363 |            tmp3.query(\"weight_alias == 'driver_dol_wgt' and op == 30 and dimension_value.str.startswith('cg*G*0912-1012').values \"),\n 364 |            tmp3.query(\"weight_alias == 'driver_dol_wgt' and op == 70 and dimension_value.str.startswith('cg*Y*0912-1012').values \"),\n 365 |           tmp3.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('cg*P2P-F&F*0912-1012').values \"),\n 366 |            tmp3.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('cg*P2P-G&S*0912-1012').values \"),\n 367 |            tmp3.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('cg*Branded XO*0912-1012').values \"),\n 368 |            tmp3.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('cg*Branded Recurring*0912-1012').values \"),\n 369 |            tmp3.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('cg*eBay MOR*0912-1012').values \"),\n 370 |            tmp3.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('cg*Other*0912-1012').values \"),\n 371 |           ]\n 372 |     )\n 373 | pd.concat([res2.query(\"weight_alias == 'driver_dol_wgt' and op == 2 and dimension_value.str.startswith('T*cg').values \")\\\n 374 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 375 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 6 and dimension_value.str.startswith('C*cg').values \")\\\n 376 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 377 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 30 and dimension_value.str.startswith('G*cg').values \")\\\n 378 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 379 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 70 and dimension_value.str.startswith('Y*cg').values \")\\\n 380 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 381 |           res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('P2P-F&F*cg').values \")\\\n 382 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 383 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('P2P-G&S*cg').values \")\\\n 384 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 385 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('Branded XO*cg').values \")\\\n 386 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 387 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('Branded Recurring*cg').values \")\\\n 388 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 389 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('eBay MOR*cg').values \")\\\n 390 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 391 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('Other*cg').values \")\\\n 392 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 393 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 7 and dimension_value.str.startswith('cg').values \")\\\n 394 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T\n 395 |           ]\n 396 |     ).T.reset_index()\n 397 | pd.concat([res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 2 and dimension_value.str.startswith('T*{flow}').values \")\\\n 398 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 399 | res2['dimension_value'].unique()\n 400 | flow = 'eBay MOR'\n 401 | pd.concat([res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 2 and dimension_value.str.endswith('T*{flow}*0912-1012').values \")\\\n 402 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 403 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 6 and dimension_value.str.endswith('C*{flow}*0912-1012').values \")\\\n 404 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 405 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.endswith('G*{flow}*0912-1012').values \")\\\n 406 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 407 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.endswith('Y*{flow}*0912-1012').values \")\\\n 408 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 409 |           ]\n 410 |     ).T.reset_index()\n 411 | flow_ls = [('P2P-F&F',15), ('P2P-G&S', 10), ('Branded XO',10), ('Branded Recurring',5), ('eBay MOR',5), ('Other',15)]\n 412 | flow = 'P2P-F&F'\n 413 | pd.concat([\n 414 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.endswith('G*{flow}*0812-0911').values \")\\\n 415 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 416 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.endswith('Y*{flow}*0812-0911').values \")\\\n 417 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 418 |           ]\n 419 |     ).T.reset_index()\n 420 |  res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.endswith('{flow}').values \")\\\n 421 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate')\n 422 |  res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.endswith('{flow}').values \")\\\n 423 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate')\n 424 | flow = 'eBay MOR'\n 425 | pd.concat([res2.query(f\"weight_alias == 'driver_dol_wgt' and op ==5 and dimension_value.str.endswith('T*{flow}').values \")\\\n 426 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 427 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op ==5 and dimension_value.str.endswith('C*{flow}').values \")\\\n 428 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 429 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.endswith('G*{flow}').values \")\\\n 430 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 431 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.endswith('Y*{flow}').values \")\\\n 432 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 433 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.endswith('{flow}').values \")\\\n 434 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 435 |           ]\n 436 |     ).T.reset_index()\n 437 | flow = 'Branded Recurring'\n 438 | pd.concat([res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('T*{flow}').values \")\\\n 439 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 440 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 6 and dimension_value.str.startswith('C*{flow}').values \")\\\n 441 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 442 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('G*{flow}').values \")\\\n 443 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 444 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('Y*{flow}').values \")\\\n 445 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 446 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('{flow}').values \")\\\n 447 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 448 |           ]\n 449 |     ).T.reset_index()\n 450 | flow = 'Branded Recurring'\n 451 | pd.concat([res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 2 and dimension_value.str.startswith('T*{flow}').values \")\\\n 452 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 453 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 6 and dimension_value.str.startswith('C*{flow}').values \")\\\n 454 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 455 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 30 and dimension_value.str.startswith('G*{flow}').values \")\\\n 456 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 457 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 70 and dimension_value.str.startswith('Y*{flow}').values \")\\\n 458 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 459 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('{flow}').values \")\\\n 460 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 461 |           ]\n 462 |     ).T.reset_index()\n 463 | flow = 'Branded Recurring'\n 464 | pd.concat([res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 2 and dimension_value.str.startswith('T*{flow}').values \")\\\n 465 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 466 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 6 and dimension_value.str.startswith('C*{flow}').values \")\\\n 467 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 468 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 30 and dimension_value.str.startswith('G*{flow}').values \")\\\n 469 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 470 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 70 and dimension_value.str.startswith('Y*{flow}').values \")\\\n 471 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 472 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('{flow}').values \")\\\n 473 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 474 |           ]\n 475 |     ).T.reset_index()\n 476 | flow = 'P2P-G&S'\n 477 | pd.concat([res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 2 and dimension_value.str.startswith('T*{flow}').values \")\\\n 478 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 479 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 6 and dimension_value.str.startswith('C*{flow}').values \")\\\n 480 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 481 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('G*{flow}').values \")\\\n 482 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 483 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('Y*{flow}').values \")\\\n 484 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 485 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('{flow}').values \")\\\n 486 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 487 |           ]\n 488 |     ).T.reset_index()\n 489 | flow = 'Branded XO'\n 490 | pd.concat([res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 2 and dimension_value.str.startswith('T*{flow}').values \")\\\n 491 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 492 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 6 and dimension_value.str.startswith('C*{flow}').values \")\\\n 493 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 494 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('G*{flow}').values \")\\\n 495 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 496 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('Y*{flow}').values \")\\\n 497 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 498 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('{flow}').values \")\\\n 499 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 500 |           ]\n 501 |     ).T.reset_index()\n 502 | flow = 'P2P-F&F'\n 503 | pd.concat([res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 2 and dimension_value.str.startswith('T*{flow}').values \")\\\n 504 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 505 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 6 and dimension_value.str.startswith('C*{flow}').values \")\\\n 506 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 507 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('G*{flow}').values \")\\\n 508 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 509 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('Y*{flow}').values \")\\\n 510 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 511 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('{flow}').values \")\\\n 512 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 513 |           ]\n 514 |     ).T.reset_index()\n 515 | flow = 'Other'\n 516 | pd.concat([res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 2 and dimension_value.str.startswith('T*{flow}').values \")\\\n 517 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 518 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 6 and dimension_value.str.startswith('C*{flow}').values \")\\\n 519 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 520 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('G*{flow}').values \")\\\n 521 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 522 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('Y*{flow}').values \")\\\n 523 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 524 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('{flow}').values \")\\\n 525 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 526 |           ]\n 527 |     ).T.reset_index()\n 528 | flow = 'Branded XO'\n 529 | pd.concat([res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 2 and dimension_value.str.startswith('T*{flow}').values \")\\\n 530 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 531 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 6 and dimension_value.str.startswith('C*{flow}').values \")\\\n 532 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 533 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 30 and dimension_value.str.startswith('G*{flow}').values \")\\\n 534 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 535 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 70 and dimension_value.str.startswith('Y*{flow}').values \")\\\n 536 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 537 |            res2.query(f\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('{flow}').values \")\\\n 538 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 539 |           ]\n 540 |     ).T.reset_index()\n 541 | pd.concat([res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 2 and dimension_value.str.startswith('T*cg').values \")\\\n 542 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 543 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 6 and dimension_value.str.startswith('C*cg').values \")\\\n 544 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 545 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('G*cg').values \")\\\n 546 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 547 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('Y*cg').values \")\\\n 548 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 549 |           res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('P2P-F&F*cg').values \")\\\n 550 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 551 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 10 and dimension_value.str.startswith('P2P-G&S*cg').values \")\\\n 552 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 553 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 10 and dimension_value.str.startswith('Branded XO*cg').values \")\\\n 554 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 555 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('Branded Recurring*cg').values \")\\\n 556 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 557 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('eBay MOR*cg').values \")\\\n 558 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 559 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('Other*cg').values \")\\\n 560 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 561 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 7 and dimension_value.str.startswith('cg').values \")\\\n 562 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T\n 563 |           ]\n 564 |     ).T.reset_index()\n 565 | res2.query(\"weight_alias == 'dol_wgt_nloss' and op >= 10 and dimension_value.str.startswith('Branded XO*ncg').values \")\n 566 | pd.concat([res2.query(\"weight_alias == 'dol_wgt_nloss' and op <= 10 and dimension_value.str.startswith('Branded XO*ncg').values \")\\\n 567 |     .pivot(index='op', columns='score_name', values='score_value').T\n 568 | ])\n 569 | pd.concat([res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 2 and dimension_value.str.startswith('T*ncg').values \")\\\n 570 |     .pivot(index='score_name', columns='dimension_value', values='action_rate').T,\n 571 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 6 and dimension_value.str.startswith('C*ncg').values \")\\\n 572 |     .pivot(index='score_name', columns='dimension_value', values='action_rate').T,\n 573 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('G*ncg').values \")\\\n 574 |     .pivot(index='score_name', columns='dimension_value', values='action_rate').T,\n 575 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('Y*ncg').values \")\\\n 576 |     .pivot(index='score_name', columns='dimension_value', values='action_rate').T,\n 577 |           res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('P2P-F&F*ncg').values \")\\\n 578 |     .pivot(index='score_name', columns='dimension_value', values='action_rate').T,\n 579 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 10 and dimension_value.str.startswith('P2P-G&S*ncg').values \")\\\n 580 |     .pivot(index='score_name', columns='dimension_value', values='action_rate').T,\n 581 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 10 and dimension_value.str.startswith('Branded XO*ncg').values \")\\\n 582 |     .pivot(index='score_name', columns='dimension_value', values='action_rate').T,\n 583 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('Branded Recurring*ncg').values \")\\\n 584 |     .pivot(index='score_name', columns='dimension_value', values='action_rate').T,\n 585 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('eBay MOR*ncg').values \")\\\n 586 |     .pivot(index='score_name', columns='dimension_value', values='action_rate').T,\n 587 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('Other*ncg').values \")\\\n 588 |     .pivot(index='score_name', columns='dimension_value', values='action_rate').T,\n 589 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 7 and dimension_value.str.startswith('ncg').values \")\\\n 590 |     .pivot(index='score_name', columns='dimension_value', values='action_rate').T\n 591 |           ]\n 592 |     ).T.reset_index()\n 593 | pd.concat([res2.query(\"weight_alias == 'driver_dol_wgt' and op == 2 and dimension_value.str.startswith('T*cg').values \")\\\n 594 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 595 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 6 and dimension_value.str.startswith('C*cg').values \")\\\n 596 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 597 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 30 and dimension_value.str.startswith('G*cg').values \")\\\n 598 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 599 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 70 and dimension_value.str.startswith('Y*cg').values \")\\\n 600 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 601 |           res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('P2P-F&F*cg').values \")\\\n 602 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 603 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('P2P-G&S*cg').values \")\\\n 604 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 605 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('Branded XO*cg').values \")\\\n 606 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 607 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('Branded Recurring*cg').values \")\\\n 608 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 609 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('eBay MOR*cg').values \")\\\n 610 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 611 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('Other*cg').values \")\\\n 612 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 613 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 7 and dimension_value.str.startswith('cg*cg').values \")\\\n 614 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T\n 615 |           ]\n 616 |     ).T.reset_index()\n 617 | pd.concat([res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 2 and dimension_value.str.startswith('T').values \")\\\n 618 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 619 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 6 and dimension_value.str.startswith('C').values \")\\\n 620 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 621 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('G').values \")\\\n 622 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 623 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('Y').values \")\\\n 624 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 625 |           res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('P2P-F&F').values \")\\\n 626 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 627 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 10 and dimension_value.str.startswith('P2P-G&S').values \")\\\n 628 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 629 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 10 and dimension_value.str.startswith('Branded XO').values \")\\\n 630 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 631 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('Branded Recurring').values \")\\\n 632 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 633 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('eBay MOR').values \")\\\n 634 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 635 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('Other').values \")\\\n 636 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 637 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 7 and dimension_value.str.startswith('cg').values \")\\\n 638 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T\n 639 |           ]\n 640 |     ).T.reset_index()\n 641 | res2.query(\"op < 5 and dimension_value.str.startswith('T*cg').values and weight_alias == 'driver_dol_wgt' \")\\\n 642 |     .pivot(index='op', columns='score_name', values=['catch_rate', 'fpr', 'score_value']).reset_index()\n 643 | res2.query(\"op == 2 and dimension_value.str.startswith('T*cg').values and weight_alias == 'driver_dol_wgt' \")\\\n 644 |     .pivot(index='score_name', columns='dimension_value', values=['catch_rate', 'fpr', 'action_rate']).reset_index()\n 645 | # ### multi-dimension check\n 646 | # !head -1 /projects/cchen16/shared-lib/maglev/SF_SL_RMR/evaluation.csv | tr '\\a' '\\n'\n 647 | res2['dimension_value'].unique()\n 648 | res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.endswith('Other*cg').values \")\\\n 649 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').reset_index()\n 650 | res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.endswith('Other*cg').values \")\\\n 651 |     .pivot(index='score_name', columns='dimension_value', values='score_value').reset_index()\n 652 | df.dtypes\n 653 | df.query(f\"driver_madmen_monthly == '2024-06-01' and driver_RMR_MODEL_SF_SL_1_model_score1 >= '730.66' and driver_pop_tag == 'cg' \").sort_values(by='driver_RMR_MODEL_SF_SL_1_model_score1', ascending=False)\n 654 |     # .pivot(index='score_name', columns='dimension_value', values='catch_rate')\n 655 | df.query(f\"driver_madmen_monthly == '2024-06-01' and driver_RMR_MODEL_SF_SL_2_model_score1 >= '720.90' and driver_pop_tag == 'cg' and driver_ \")\\\n 656 |     .groupby(['driver_cnsr_seg', 'driver_is_brm3_4_bad'])\\\n 657 |     .agg({'driver_madmen_monthly':'count', 'driver_dol_wgt':'sum'}).unstack()\n 658 | df.query(f\"driver_madmen_monthly == '2024-06-01' and driver_RMR_MODEL_SF_SL_1_model_score1 >= '730.66' and driver_pop_tag == 'cg' \")\\\n 659 |     .groupby(['driver_cnsr_seg', 'driver_is_brm3_4_bad'])\\\n 660 |     .agg({'driver_dol_wgt':'sum', 'driver_madmen_monthly':'count'}).reset_index()\\\n 661 |     .pivot(index='driver_cnsr_seg', columns='driver_is_brm3_4_bad', values=['driver_dol_wgt','driver_madmen_monthly'])\n 662 | df.query(f\"driver_madmen_monthly == '2024-06-01' and driver_RMR_MODEL_SF_SL_2_model_score1 >= '720.90' and driver_pop_tag == 'cg' and \")\\\n 663 |     .groupby(['driver_cnsr_seg', 'driver_is_brm3_4_bad'])\\\n 664 |     .agg({'driver_dol_wgt':'sum', 'driver_madmen_monthly':'count'}).reset_index()\\\n 665 |     .pivot(index='driver_cnsr_seg', columns='driver_is_brm3_4_bad', values=['driver_dol_wgt','driver_madmen_monthly'])\n 666 | df.query(f\"driver_madmen_monthly == '2024-06-01' and driver_RMR_MODEL_SF_SL_2_model_score1 >= '720.90' \\\n 667 |             and driver_RMR_MODEL_SF_SL_1_model_score1 < '730.66' and driver_pop_tag == 'cg' \\\n 668 |             and driver_cnsr_seg == 'C' and driver_product_flow_rollup == 'Other' \").sort_values(by='driver_pmt_start_date', ascending=False)\n 669 | df.query(f\"driver_madmen_monthly == '2024-08-01' and driver_RMR_MODEL_SF_SL_1_model_score1 < '730.66' and driver_pop_tag == 'cg' \")\\\n 670 |     .groupby(['driver_cnsr_seg', 'driver_is_brm3_4_bad'])\\\n 671 |     .agg({'driver_dol_wgt':'sum', 'driver_madmen_monthly':'count'}).reset_index()\\\n 672 |     .pivot(index='driver_cnsr_seg', columns='driver_is_brm3_4_bad', values=['driver_dol_wgt','driver_madmen_monthly'])\n 673 | other_0919\n 674 | res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.endswith('Other*cg').values \")\\\n 675 |     .pivot(index='score_name', columns='dimension_value', values='score_value').reset_index()\n 676 | pd.concat([res2.query(\"weight_alias == 'driver_dol_wgt' and op == 2 and dimension_value.str.startswith('T*cg').values \")\\\n 677 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 678 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 6 and dimension_value.str.startswith('C*cg').values \")\\\n 679 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 680 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 30 and dimension_value.str.startswith('G*cg').values \")\\\n 681 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 682 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 70 and dimension_value.str.startswith('Y*cg').values \")\\\n 683 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 684 |           res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('P2P-F&F*cg').values \")\\\n 685 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 686 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('P2P-G&S*cg').values \")\\\n 687 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 688 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('Branded XO*cg').values \")\\\n 689 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 690 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('Branded Recurring*cg').values \")\\\n 691 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 692 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('eBay MOR*cg').values \")\\\n 693 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 694 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('Other*cg').values \")\\\n 695 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 696 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 7 and dimension_value.str.startswith('cg').values \")\\\n 697 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T\n 698 |           ]\n 699 |     ).T.reset_index()\n 700 | pd.concat([res2.query(\"weight_alias == 'driver_dol_wgt' and op == 2 and dimension_value.str.startswith('T*ncg').values \")\\\n 701 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 702 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 6 and dimension_value.str.startswith('C*ncg').values \")\\\n 703 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 704 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 30 and dimension_value.str.startswith('G*ncg').values \")\\\n 705 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 706 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 70 and dimension_value.str.startswith('Y*ncg').values \")\\\n 707 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 708 |           res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('P2P-F&F*ncg').values \")\\\n 709 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 710 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('P2P-G&S*ncg').values \")\\\n 711 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 712 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 10 and dimension_value.str.startswith('Branded XO*ncg').values \")\\\n 713 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 714 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('Branded Recurring*ncg').values \")\\\n 715 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 716 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 5 and dimension_value.str.startswith('eBay MOR*ncg').values \")\\\n 717 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 718 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.startswith('Other*ncg').values \")\\\n 719 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T,\n 720 |            res2.query(\"weight_alias == 'driver_dol_wgt' and op == 7 and dimension_value.str.startswith('ncg').values \")\\\n 721 |     .pivot(index='score_name', columns='dimension_value', values='fpr').T\n 722 |           ]\n 723 |     ).T.reset_index()\n 724 | pd.concat([res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 2 and dimension_value.str.startswith('T*cg').values \")\\\n 725 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 726 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 6 and dimension_value.str.startswith('C*cg').values \")\\\n 727 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 728 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('G*cg').values \")\\\n 729 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 730 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('Y*cg').values \")\\\n 731 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 732 |           res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('P2P-F&F*cg').values \")\\\n 733 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 734 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 10 and dimension_value.str.startswith('P2P-G&S*cg').values \")\\\n 735 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 736 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 10 and dimension_value.str.startswith('Branded XO*cg').values \")\\\n 737 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 738 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('Branded Recurring*cg').values \")\\\n 739 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 740 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 5 and dimension_value.str.startswith('eBay MOR*cg').values \")\\\n 741 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 742 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 15 and dimension_value.str.startswith('Other*cg').values \")\\\n 743 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 744 |            res2.query(\"weight_alias == 'dol_wgt_nloss' and op == 7 and dimension_value.str.startswith('cg').values \")\\\n 745 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T\n 746 |           ]\n 747 |     ).T.reset_index()\n 748 | pd.concat([res2.query(\"op == 2 and dimension_value.str.startswith('T*cg').values \")\\\n 749 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 750 |            res2.query(\"op == 6 and dimension_value.str.startswith('C*cg').values \")\\\n 751 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 752 |            res2.query(\"op == 30 and dimension_value.str.startswith('G*cg').values \")\\\n 753 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 754 |            res2.query(\"op == 70 and dimension_value.str.startswith('Y*cg').values \")\\\n 755 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 756 |           res2.query(\"op == 15 and dimension_value.str.startswith('P2P-F&F*cg').values \")\\\n 757 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 758 |            res2.query(\"op == 10 and dimension_value.str.startswith('P2P-G&S*cg').values \")\\\n 759 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 760 |            res2.query(\"op == 10 and dimension_value.str.startswith('Branded XO*cg').values \")\\\n 761 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 762 |            res2.query(\"op == 5 and dimension_value.str.startswith('Branded Recurring*cg').values \")\\\n 763 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 764 |            res2.query(\"op == 5 and dimension_value.str.startswith('eBay MOR*cg').values \")\\\n 765 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 766 |            res2.query(\"op == 15 and dimension_value.str.startswith('Other*cg').values \")\\\n 767 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T\n 768 |           ]\n 769 |     ).T.reset_index()\n 770 | pd.concat([res2.query(\"op == 15 and dimension_value.str.startswith('P2P-F&F*cg').values \")\\\n 771 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 772 |            res2.query(\"op == 10 and dimension_value.str.startswith('P2P-G&S*cg').values \")\\\n 773 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 774 |            res2.query(\"op == 10 and dimension_value.str.startswith('Branded XO*cg').values \")\\\n 775 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 776 |            res2.query(\"op == 5 and dimension_value.str.startswith('Branded Recurring*cg').values \")\\\n 777 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 778 |            res2.query(\"op == 5 and dimension_value.str.startswith('eBay MOR*cg').values \")\\\n 779 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 780 |            res2.query(\"op == 15 and dimension_value.str.startswith('Other*cg').values \")\\\n 781 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T\n 782 |           ]\n 783 |          )\n 784 | res2.query(f\"op < 100 and dimension_value.str.startswith('eval_t11_17_1*Y*eBay MOR').values \")\n 785 |     # .pivot(index='score_name', columns='dimension_value', values='catch_rate')\n 786 | def plot_perf(key_dim,score_list,wgt,OP,metric,bad,perf,plot_op_range):\n 787 |     import pandas as pd\n 788 |     import numpy as np\n 789 |     import matplotlib.pyplot as plt\n 790 |     counter=0\n 791 |     for score in score_list:\n 792 |         y1 = perf[(perf['weight_alias'] == wgt) \\\n 793 |           & (perf['score_name'] == score) \\\n 794 |           & (perf['op'] == OP) \\\n 795 |           & (perf['tagging'] == bad) \\\n 796 |           & ([i.startswith(key_dim) for i in perf['dimension_value']])][metric]\n 797 |         #print(y1)\n 798 |         x1 = perf[(perf['weight_alias'] == wgt) \\\n 799 |           & (perf['score_name'] == score) \\\n 800 |           & (perf['op'] == OP) \\\n 801 |           & (perf['tagging'] == bad) \\\n 802 |           & ([i.startswith(key_dim) for i in perf['dimension_value']])]['dimension_value']\n 803 |         #print(y1)\n 804 |         y2 = perf[(perf['weight_alias'] == wgt) \\\n 805 |           & (perf['score_name'] == score) \\\n 806 |           & (perf['tagging'] == bad) \\\n 807 |           & ([i.startswith(key_dim) for i in perf['dimension_value']])\n 808 |           & (perf['op'] <= plot_op_range)][metric]\n 809 |         x2 = perf[(perf['weight_alias'] == wgt) \\\n 810 |           & (perf['score_name'] == score) \\\n 811 |           & (perf['tagging'] == bad) \\\n 812 |           & ([i.startswith(key_dim) for i in perf['dimension_value']])\n 813 |           & (perf['op'] <= plot_op_range)]['op']\n 814 |         plt.plot(x2,y2, label = score)\n 815 |         #print(x2)\n 816 |         #print(y2)\n 817 |         df = pd.DataFrame(y1)\n 818 |         x1=list(x1)\n 819 |         x1=[\"$op\"+metric+i for i in x1]\n 820 |         df.index=x1\n 821 |         df.columns=[score]\n 822 |         df.transpose()\n 823 |         if counter==0:\n 824 |             last_df=df\n 825 |             last_name=score\n 826 |         else:\n 827 |             df=pd.merge(df, last_df, how='left', left_index=True,right_index=True,sort=True,\n 828 |                      suffixes=('-'+score,'-'+last_name), copy=True, indicator=False,validate=None)\n 829 |             last_df=df\n 830 |             last_name=score\n 831 |         counter=counter+1\n 832 |     plt.legend(loc=\"best\", bbox_to_anchor=(1.05,1.0),borderaxespad = 0.) \n 833 |     plt.title(metric+\" on \"+key_dim+\" with weight=\"+wgt+\" bad=\"+bad,loc ='right') \n 834 |     a=df.transpose()\n 835 |     df=df.transpose().sort_values(a.columns[0],ascending=False)#.transpose()\n 836 |     return df\n 837 | score_name = ['cam24_il_ems_model_CAM24_IL_EMS_ModelScore', 'cam24_il_ems_model_sapout_CAM24_IL_EMS_ModelScore_sapout']\n 838 | plot_perf('Branded Recurring*cg',score_name,'dol_wgt_nloss',5,'catch_rate','driver_is_brm3_4_bad',res2,100)\n 839 | plot_perf('Branded XO*cg',score_name,'dol_wgt_nloss',10,'catch_rate','driver_is_brm3_4_bad',res2,40)\n 840 | plot_perf('P2P-G&S*cg',score_name,'dol_wgt_nloss',10,'catch_rate','driver_is_brm3_4_bad',res2,40)\n 841 | plot_perf('P2P-F&F*cg',score_name,'dol_wgt_nloss',15,'catch_rate','driver_is_brm3_4_bad',res2,40)\n 842 | plot_perf('P2P-G&S*cg',score_name,'dol_wgt_nloss',10,'catch_rate','driver_is_brm3_4_bad',res2,40)\n 843 | plot_perf('eBay MOR*cg',score_name,'dol_wgt_nloss',5,'catch_rate','driver_is_brm3_4_bad',res2,40)\n 844 | plot_perf('Other*cg',score_name,'dol_wgt_nloss',15,'catch_rate','driver_is_brm3_4_bad',res2,40)\n 845 | import matplotlib.pyplot as plt\n 846 | import seaborn as sns\n 847 | pd.set_option('display.max_rows', 500)\n 848 | pd.set_option('display.max_columns', 500)\n 849 | pd.set_option('display.width', 1000)\n 850 | sns.set_palette('husl')\n 851 | custom_palette = sns.color_palette(\"husl\", n_colors=2)\n 852 | plt.figure(figsize=(10, 6))\n 853 | sns.lineplot(, bins=50, kde=True, label=f'Flow {group_name}', alpha=0.3, color=custom_palette[i])\n 854 | plt.title('Score Distribution by Group')\n 855 | plt.xlabel('Score')\n 856 | plt.ylabel('Frequency')\n 857 | plt.legend()\n 858 | plt.grid(True)\n 859 | plt.show()\n 860 | import matplotlib.pyplot as plt\n 861 | import seaborn as sns\n 862 | pd.set_option('display.max_rows', 500)\n 863 | pd.set_option('display.max_columns', 500)\n 864 | pd.set_option('display.width', 1000)\n 865 | sns.set_palette('husl')\n 866 | custom_palette = sns.color_palette(\"husl\", n_colors=len(df['driver_cnsr_seg'].unique()))\n 867 | # Plotting the distributions\n 868 | plt.figure(figsize=(10, 6))\n 869 |     sns.histplot(group_data['CAM24_DeepFM_new_transfer_202305_202403_cam24_deepfm_new_transfer_202305_202403_cls_src'], bins=50, kde=True, label=f'Group {group_name}', alpha=0.3, color=custom_palette[i])\n 870 | plt.title('Score Distribution by Group')\n 871 | plt.xlabel('Score')\n 872 | plt.ylabel('Frequency')\n 873 | plt.legend()\n 874 | plt.grid(True)\n 875 | plt.show()\n 876 | res2.query(\"op == 2 and dimension_value.str.startswith('eval_t11_17_1*T').values \")\\\n 877 | res2.query(\"op == 6 and dimension_value.str.startswith('eval_t11_17_1*C').values \")\\\n 878 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate')\n 879 | res2.query(\"op == 6 and dimension_value.str.startswith('eval_t11_17_1*C').values \")\\\n 880 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate')\n 881 | res2.query(\"op == 6 and dimension_value.str.startswith('eval_t11_17_1*C').values \")\\\n 882 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate')\n 883 | # ## Score check\n 884 | # %config PPMagics.domain=\"ccg24-hrzana-r-gds-gfs\"\n 885 | # %%ppbq\n 886 | select *\n 887 | from pypl-edw.pp_scratch.cam24_il_strategy_20240919\n 888 | where madmen_monthly = '2024-06-01' \n 889 | and RMR_MODEL_SF_SL_2_model_score1 >= '720.90'\n 890 | and RMR_MODEL_SF_SL_1_model_score1 < '730.66'\n 891 | and is_brm3_4_bad = '1.0'\n 892 | and pop_tag = 'cg'\n 893 | and product_flow_rollup = 'Other'\n 894 | order by pmt_start_date\n 895 | ;\n 896 | res2.query(\"weight_alias == 'driver_dol_wgt' and op == 15 and dimension_value.str.endswith('Other*cg').values \")\\\n 897 |     .pivot(index='score_name', columns='dimension_value', values='score_value').reset_index()\n 898 | # %%ppbq\n 899 | select *\n 900 | from pypl-edw.pp_scratch.cam24_il_strategy_20240919\n 901 | where madmen_monthly = '2024-08-01' \n 902 | and RMR_MODEL_SF_SL_2_model_score1 >= '718.93'\n 903 | and RMR_MODEL_SF_SL_1_model_score1 < '724.78'\n 904 | # and is_brm3_4_bad = '1.0'\n 905 | and pop_tag = 'cg'\n 906 | and product_flow_rollup = 'Other'\n 907 | order by pmt_start_date\n 908 | ;\n 909 | # %%ppbq\n 910 | select b.* \n 911 | from (select *\n 912 |         from pypl-edw.pp_scratch.cam24_il_strategy_20240919\n 913 |         where madmen_monthly = '2024-08-01' \n 914 |         and RMR_MODEL_SF_SL_2_model_score1 >= '718.93'\n 915 |         and RMR_MODEL_SF_SL_1_model_score1 < '724.78'\n 916 |         # and is_brm3_4_bad = '1.0'\n 917 |         and pop_tag = 'cg'\n 918 |         and product_flow_rollup = 'Other'\n 919 |         order by pmt_start_date) as a\n 920 | inner join pypl-edw.pp_scratch_risk.PPFM_donation_trend b\n 921 | on cast(a.trans_id as Numeric) = b.trans_id\n 922 | order by pmt_start_date\n 923 | ;\n 924 | # %%ppbq\n 925 | select * \n 926 | from pypl-edw.pp_scratch_risk.PPFM_donation_trend\n 927 | where pmt_start_date between '2024-08-01' and '2024-08-31'\n 928 | and l2_product_flow = 'PPFM'\n 929 | ;\n 930 | sql = \"\"\"\n 931 | select cnsr_seg\n 932 | , is_brm3_4_bad\n 933 | , case when cast(usd_amt as numeric) <= 50 then 1 else 0 end as is_low_asp\n 934 | , sum(1) as raw_cnt\n 935 | , sum(cast(unit_wgt as Numeric)) as unit_wgt\n 936 | , sum(cast(dol_wgt as Numeric)) as dol_wgt\n 937 | from pypl-edw.pp_scratch.cam24_il_strategy_20240905\n 938 | where cast(RMR_MODEL_SF_SL_2_model_score1 as numeric) > 818\n 939 | and product_flow_rollup in ('Branded Recurring')\n 940 | and cc_control = '1'\n 941 | group by 1,2,3\n 942 | order by 1,2,3\n 943 | ;\n 944 | \"\"\"\n 945 | # old_stats = %ppbq {sql}\n 946 | old_stats[old_stats.is_low_asp == 1].pivot(index='cnsr_seg', columns='is_brm3_4_bad', values=['raw_cnt','unit_wgt','dol_wgt']).fillna(0).reset_index()\n 947 | old_stats[old_stats.is_low_asp == 0].pivot(index='cnsr_seg', columns='is_brm3_4_bad', values=['raw_cnt','unit_wgt','dol_wgt']).reset_index()\n 948 | top_recur.groupby(['driver_cnsr_seg', 'driver_is_brm3_4_bad']).agg({\n 949 |     \"driver_unit_wgt\":\"sum\", \"driver_dol_wgt\":\"sum\"\n 950 | }).reset_index().pivot(index='driver_cnsr_seg', columns='driver_is_brm3_4_bad', values=['driver_unit_wgt','driver_dol_wgt']).reset_index()\n 951 | top_recur.query(\"driver_usd_amt <= 50 \").groupby(['driver_cnsr_seg', 'driver_is_brm3_4_bad']).agg({\n 952 |     \"driver_unit_wgt\":\"sum\", \"driver_dol_wgt\":\"sum\"\n 953 | }).reset_index().pivot(index='driver_cnsr_seg', columns='driver_is_brm3_4_bad', values=['driver_unit_wgt','driver_dol_wgt']).reset_index()\n 954 | fire_df = pd.read_parquet(\"/projects/gds-focus/data/catch/IL/CAM24/scoring/cam_il_0829_fire_scoring\")\n 955 | fire_df.columns\n 956 | fire_df['CAM24_DeepFM_new_transfer_202305_202403_cam24_deepfm_new_transfer_202305_202403_cls_src']\n 957 | fire_df = fire_df.rename(columns={\"CAM24_DeepFM_new_transfer_202305_202403_cam24_deepfm_new_transfer_202305_202403_cls_src\": \"CAM24_finetune\",\\\n 958 |                          \"CAM22_RMR_202405_sapout_CAM22_RMR_202405_sapout\": \"cam22_2405_rmr\",\\\n 959 |                          \"cam24_il_esm_20240905_sapout_CAM24_IL_EMS_ModelScore_sapout\": \"cam24_il_0905\"})\n 960 | fire_df.shape\n 961 | # ## Precision/Recall check\n 962 | df[df.mth_period == '0812-0911'].shape\n 963 | b1_df = df[df.mth_period == '0812-0911']\n 964 | df[(df.mth_period == '0812-0911')]['driver_is_brm3_4_bad'].value_counts(normalize=True)\n 965 | (0.993322/0.0015)/5\n 966 | 0.9/200\n 967 | df[(df.mth_period == '0812-0911')].groupby('driver_is_brm3_4_bad')['driver_dol_wgt'].sum()\n 968 | df[(df.mth_period == '0812-0911')]['driver_is_brm3_4_bad'].value_counts(normalize=True)\n 969 | b1_df = df[(df.mth_period == '0812-0911')&(df.exp1_1_cam_cls >= 0.5)]\n 970 | weighted_precision = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_dol_wgt'].sum()/b1_df['driver_dol_wgt'].sum()\n 971 | weighted_recall    = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_dol_wgt'].sum()/df[(df.mth_period == '0812-0911')&(df['driver_is_brm3_4_bad'] == 1.0)]['driver_dol_wgt'].sum()\n 972 | df['driver_RMR_MODEL_SF_2_model_score1'] = pd.to_numeric(df['driver_RMR_MODEL_SF_2_model_score1'])\n 973 | b1_df = df[(df.mth_period == '0812-0911')&(df.driver_RMR_MODEL_SF_2_model_score1 >= 750)]\n 974 | weighted_precision = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_dol_wgt'].sum()/b1_df['driver_dol_wgt'].sum()\n 975 | weighted_recall    = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_dol_wgt'].sum()/df[(df.mth_period == '0812-0911')&(df['driver_is_brm3_4_bad'] == 1.0)]['driver_dol_wgt'].sum()\n 976 | b1_df = df[(df.mth_period == '0812-0911')&(df.exp1_2_cam_cls >= 0.1)]\n 977 | weighted_precision = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_dol_wgt'].sum()/b1_df['driver_dol_wgt'].sum()\n 978 | weighted_recall    = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_dol_wgt'].sum()/df[(df.mth_period == '0812-0911')&(df['driver_is_brm3_4_bad'] == 1.0)]['driver_dol_wgt'].sum()\n 979 | b1_df = df[(df.mth_period == '0812-0911')&(df.exp1_1_cam_cls >= 0.5)]\n 980 | weighted_precision = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_dol_wgt'].sum()/b1_df['driver_dol_wgt'].sum()\n 981 | weighted_recall    = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_dol_wgt'].sum()/df[(df.mth_period == '0812-0911')&(df['driver_is_brm3_4_bad'] == 1.0)]['driver_dol_wgt'].sum()\n 982 | b1_df = df[(df.mth_period == '0812-0911')&(df.exp1_1_cam_cls >= 0.5)]\n 983 | precision = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_trans_id'].count()/(b1_df['driver_trans_id'].count()*0.2)\n 984 | weighted_precision = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_dol_wgt'].sum()/b1_df['driver_dol_wgt'].sum()\n 985 | weighted_recall    = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_dol_wgt'].sum()/df[(df.mth_period == '0812-0911')&(df['driver_is_brm3_4_bad'] == 1.0)]['driver_dol_wgt'].sum()\n 986 | b1_df = df[(df.mth_period == '0812-0911')&(df.exp1_1_cam_cls >= 0.1)]\n 987 | precision = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_trans_id'].count()/(b1_df['driver_trans_id'].count()*0.2)\n 988 | weighted_precision = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_dol_wgt'].sum()/b1_df['driver_dol_wgt'].sum()\n 989 | weighted_recall    = b1_df[b1_df['driver_is_brm3_4_bad'] == 1.0]['driver_dol_wgt'].sum()/df[(df.mth_period == '0812-0911')&(df['driver_is_brm3_4_bad'] == 1.0)]['driver_dol_wgt'].sum()\n 990 | b1_df['driver_dol_wgt'].sum()",
    "rmr_agent/repos/CAM_variable_research/analysis.py": "   1 | # %config PPMagics.domain=\"ccg24-hrzana-r-gds-gfs\"\n   2 | # %ppauth\n   3 | import json\n   4 | import warnings\n   5 | import os, sys\n   6 | from model_automation.utils.rmr.helper import run_cmd\n   7 | from py_dpu.utils import  save_pig,load_pig, rm_df_alias,load_parquet\n   8 | from pyspark.sql import functions as F\n   9 | from pyspark.sql.functions import col, max, min, when, lit\n  10 | import time\n  11 | from model_automation.gcp import dataproc_config\n  12 | import pandas as pd\n  13 | import pandas as pd\n  14 | import matplotlib.pyplot as plt\n  15 | import seaborn as sns\n  16 | pd.set_option('display.max_rows', 500)\n  17 | pd.set_option('display.max_columns', 500)\n  18 | pd.set_option('display.width', 1000)\n  19 | # visiualize\n  20 | from sklearn.decomposition import PCA\n  21 | from sklearn.manifold import TSNE\n  22 | from sklearn.cluster import KMeans\n  23 | import matplotlib.pyplot as plt\n  24 | import seaborn as sns\n  25 | # ## Embedding vis\n  26 | # ## read embedding\n  27 | # !ls -t /projects/gds-focus/data/catch/IL/CAM_variable/data/scoring/ | head\n  28 | # !du -h -s /projects/gds-focus/data/catch/IL/CAM/00_pipeline/scoring/offsim_embed_week1_9\n  29 | embed_path = \"/projects/gds-focus/data/catch/IL/CAM_variable/data/scoring/cam_il_model_scoring_0109\"\n  30 | df = pd.read_parquet(embed_path)\n  31 | df.shape\n  32 | len(df.columns)\n  33 | list(df.columns)\n  34 | df.columns = [c[7:] if c.startswith('driver_') else c for c in df.columns]\n  35 | df = df.rename(columns={\n  36 |     'madmen_is_cc_bad':'is_brm3_4_bad',\n  37 |     'bad3_4_nloss': 'nloss',\n  38 | })\n  39 | df = df.rename(columns={\n  40 |     'CAM24_EMS_V2_sapout_CAM24_EMS_SCORE_V2_sapout':'cam24_09_rmr',\n  41 |     'cam24_il_esm_20240912_sapout_CAM24_IL_EMS_ModelScore_sapout': 'cam_il_0912_sapout',\n  42 | })\n  43 | df['BRM_BAD_TAG_ASSIGNED'] = df['BRM_BAD_TAG_ASSIGNED'].astype(str)\n  44 | df['is_brm3_4_bad'] = df['is_brm3_4_bad'].astype(float)\n  45 | df['trans_id'] = df['trans_id'].astype(float).astype(int).astype(str)\n  46 | df['nloss'] = df['nloss'].astype(float)\n  47 | df['dol_wgt'] = df['dol_wgt'].astype(float)\n  48 | df['cnsr_seg'] = df['cnsr_seg'].astype(str)\n  49 | df['product_flow_rollup'] = df['product_flow_rollup'].astype(str)\n  50 | df['pmt_start_date'].min(), df['pmt_start_date'].max()\n  51 | # ## Driver analysis\n  52 | driver = pd.read_csv(\"/projects/gds-focus/data/catch/IL/CAM_variable/data/cam_0912_recent_meta.csv\", sep='\\x07')\n  53 | driver.shape\n  54 | driver['is_pos_nloss'] = (driver['driver_nloss'] > 0) | (driver['driver_brm_bad_tag_assigned'] == '3_STOLEN_CC_CB')\n  55 | t1 = driver.groupby(['driver_cnsr_seg','driver_brm_bad_tag_assigned'])\\\n  56 |       .agg({'driver_trans_id':'count'})\n  57 | t2 = t1.pivot_table(index='driver_brm_bad_tag_assigned',columns='driver_cnsr_seg',values='driver_trans_id')\n  58 | (t2/t1['driver_trans_id'].sum()).applymap(lambda x: f\"{x*100:.2f}%\")\n  59 | t1 = driver.groupby(['driver_cnsr_seg','driver_brm_bad_tag_assigned'])\\\n  60 |       .agg({'driver_trans_id':'count'})\n  61 | t2 = t1.pivot_table(index='driver_brm_bad_tag_assigned',columns='driver_cnsr_seg',values='driver_trans_id')\n  62 | (t2/t1['driver_trans_id'].sum()).applymap(lambda x: f\"{x*100:.2f}%\")\n  63 | t1 = driver.query(\"driver_cnsr_seg == 'Y' \").groupby(['driver_l2_product_flow','driver_brm_bad_tag_assigned'])\\\n  64 |       .agg({'driver_trans_id':'count'}).fillna(0)\n  65 | t2 = t1.pivot_table(index='driver_l2_product_flow',columns='driver_brm_bad_tag_assigned',values='driver_trans_id').fillna(0)\n  66 | (t2/t1['driver_trans_id'].sum()).applymap(lambda x: f\"{x*100:.2f}%\")\n  67 | t1 = driver.groupby(['driver_cnsr_seg','driver_brm_bad_tag_assigned'])\\\n  68 |       .agg({'driver_nloss':'sum'})\n  69 | t2 = t1.pivot_table(index='driver_brm_bad_tag_assigned',columns='driver_cnsr_seg',values='driver_nloss')\n  70 | (t2/t1['driver_nloss'].sum()).applymap(lambda x: f\"{x*100:.2f}%\")\n  71 | t2\n  72 | driver.groupby(['driver_brm_bad_tag_assigned']).agg({\"driver_trans_id\":\"count\",\"driver_dol_wgt\":\"sum\",\"driver_nloss\":\"sum\"})\n  73 | t1 = driver.query(\" driver_brm_bad_tag_assigned == '4_UNAUTH_RESTR' and driver_nloss <= 0 \")\\\n  74 |       .groupby(['driver_cnsr_seg','driver_l2_product_flow'])\\\n  75 |       .agg({'driver_trans_id':'count'})\n  76 | t2 = t1.pivot_table(index='driver_l2_product_flow',columns='driver_cnsr_seg',values='driver_trans_id').fillna(0)\n  77 | (t2/t1['driver_trans_id'].sum()).applymap(lambda x: f\"{x*100:.2f}%\")\n  78 | # t2\n  79 | t1 = driver.query(\" driver_brm_bad_tag_assigned in ('4_UNAUTH_RESTR', '11_FREE_UNAUTH_CBK', '3_STOLEN_CC_CB')  \")\\\n  80 |       .groupby(['driver_cnsr_seg','driver_l2_product_flow'])\\\n  81 |       .agg({'driver_nloss':'sum'})\n  82 | t2 = t1.pivot_table(index='driver_l2_product_flow',columns='driver_cnsr_seg',values='driver_nloss').fillna(0)\n  83 | (t2/t1['driver_nloss'].sum()).applymap(lambda x: f\"{x*100:.2f}%\")\n  84 | # t2\n  85 | t1 = driver.query(\" driver_brm_bad_tag_assigned == '4_UNAUTH_RESTR' and driver_nloss == 0.0 \")\\\n  86 |       .groupby(['driver_cnsr_seg','driver_l2_product_flow'])\\\n  87 |       .agg({'driver_trans_id':'count'})\n  88 | t2 = t1.pivot_table(index='driver_l2_product_flow',columns='driver_cnsr_seg',values='driver_trans_id').fillna(0)\n  89 | (t2/t1['driver_trans_id'].sum()).applymap(lambda x: f\"{x*100:.2f}%\")\n  90 | # t2\n  91 | t1 = driver.query(\" driver_is_brm3_4_bad == 1.0 \")\\\n  92 |       .groupby(['is_pos_nloss','driver_l2_product_flow'])\\\n  93 |       .agg({'driver_trans_id':'count'})\n  94 | t2 = t1.pivot_table(index='is_pos_nloss',columns='driver_l2_product_flow',values='driver_trans_id').fillna(0)\n  95 | # (t2/t1['driver_trans_id'].sum()).applymap(lambda x: f\"{x*100:.2f}%\")\n  96 | # t2\n  97 | t1 = driver.query(\" driver_is_brm3_4_bad == 1.0 \")\\\n  98 |       .groupby(['is_pos_nloss','driver_brm_bad_tag_assigned'])\\\n  99 |       .agg({'driver_trans_id':'count'})\n 100 | t2 = t1.pivot_table(index='is_pos_nloss',columns='driver_brm_bad_tag_assigned',values='driver_trans_id').fillna(0)\n 101 | # (t2/t1['driver_trans_id'].sum()).applymap(lambda x: f\"{x*100:.2f}%\")\n 102 | t2\n 103 | driver.query(\" driver_is_brm3_4_bad == 0.0 and driver_train_wgt_3 > 10\").sample()\n 104 | # ## fast perf get CO\n 105 | score_list = [\n 106 | 'cam24_il_esm_20240912_sapout_CAM24_IL_EMS_ModelScore_sapout',\n 107 |  'exp4_1_recur_cond_cam24_flow_score',\n 108 |  'exp4_2_xo_cond_cam24_flow_score',\n 109 |  'exp4_3_other_cond_cam24_flow_score',\n 110 |  'exp4_4_p2pff_cond_cam24_flow_score',\n 111 |  'exp4_5_p2pgs_cond_cam24_flow_score',\n 112 |  'exp4_6_ebaymor_cond_cam24_flow_score',\n 113 | ]\n 114 | from model_automation.model_evaluation import model_eval\n 115 | param = {\n 116 |     \"df\": df,\n 117 |     # \"dataPath\": \"/projects/cchen16/shared-lib/maglev/SF_SL_RMR/eval_data.csv\",\n 118 |     \"delimiter\": \"\\u0007\",\n 119 |     \"scoreList\": score_list,\n 120 |     \"filter\": \" cnsr_seg == 'G' or cnsr_seg == 'Y' \",\n 121 |     \"dimList\": [\n 122 |                 # 'pop_tag',\n 123 |                 # 'pop_tag*product_flow_rollup*mth_period',\n 124 |                 'product_flow_rollup*cnsr_seg',\n 125 |                 # 'product_flow_rollup*pop_tag',\n 126 |                 # 'cnsr_seg*pop_tag',\n 127 |                 # 'madmen_monthly*cnsr_seg',\n 128 |                 # 'madmen_monthly*product_flow_rollup',\n 129 |                 # 'eval_t11_to_t17_week_id*cnsr_seg',\n 130 |                 # 'eval_t11_to_t17_week_id*product_flow_rollup',\n 131 |                 # 'product_flow_rollup*pop_tag'\n 132 |                ],\n 133 |     \"OP\": 1000,\n 134 |     \"TopOP\": 100,\n 135 |     \"badList\": ['is_brm3_4_bad'],\n 136 |     \"xWeight\": ['dol_wgt', 'dol_wgt'],\n 137 |     \"yWeight\": ['dol_wgt', 'nloss'],\n 138 |     'weightAlias' : ['dol_wgt', 'dol_wgt_nloss']\n 139 |     # \"xWeight\": ['dol_wgt'],\n 140 |     # \"yWeight\": ['nloss'],\n 141 |     # 'weightAlias' : ['dol_wgt_nloss']\n 142 | }\n 143 | res2 = model_eval(\"evalkit\", param)\n 144 | res2 = res2.dropna(subset=['score_value'], axis=0)\n 145 | for xw, yw, aw in zip(param['xWeight'], param['yWeight'], param['weightAlias']):\n 146 |     res2['weight_alias']=res2['weight_alias'].replace({xw+'-'+yw: aw})\n 147 | res2['dimension_value'].unique()\n 148 | (df[df.is_brm3_4_bad == 1]['dol_wgt'].sum())\n 149 | grp_df = df[df.is_brm3_4_bad == 1].groupby(['product_flow_rollup','cnsr_seg']).agg({'nloss':'sum'}).reset_index()\n 150 | tmp = grp_df.pivot(index='product_flow_rollup', columns='cnsr_seg', values='nloss').fillna(0)\n 151 | (tmp/(df[df.is_brm3_4_bad == 1]['nloss'].sum())).applymap(lambda x: f\"{x*100:.2f}%\")\n 152 | flow = 'Other'\n 153 | pd.concat([\n 154 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('{flow}*G').values \")\\\n 155 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 156 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('{flow}*Y').values \")\\\n 157 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 158 |           ]\n 159 |     ).T.reset_index()\n 160 | flow = 'Other'\n 161 | pd.concat([\n 162 |            res2.query(f\"weight_alias == 'dol_wgt' and op == 30 and dimension_value.str.startswith('{flow}*G').values \")\\\n 163 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 164 |            res2.query(f\"weight_alias == 'dol_wgt' and op == 70 and dimension_value.str.startswith('{flow}*Y').values \")\\\n 165 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 166 |           ]\n 167 |     ).T.reset_index()\n 168 | flow = 'P2P-G&S'\n 169 | pd.concat([\n 170 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('{flow}*G').values \")\\\n 171 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 172 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('{flow}*Y').values \")\\\n 173 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 174 |           ]\n 175 |     ).T.reset_index()\n 176 | flow = 'P2P-F&F'\n 177 | pd.concat([\n 178 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('{flow}*G').values \")\\\n 179 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 180 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('{flow}*Y').values \")\\\n 181 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 182 |           ]\n 183 |     ).T.reset_index()\n 184 | flow = 'P2P-G&S'\n 185 | pd.concat([\n 186 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('{flow}*G').values \")\\\n 187 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 188 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('{flow}*Y').values \")\\\n 189 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 190 |           ]\n 191 |     ).T.reset_index()\n 192 | flow = 'eBay MOR'\n 193 | pd.concat([\n 194 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 30 and dimension_value.str.startswith('{flow}*G').values \")\\\n 195 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 196 |            res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('{flow}*Y').values \")\\\n 197 |     .pivot(index='score_name', columns='dimension_value', values='catch_rate').T,\n 198 |           ]\n 199 |     ).T.reset_index()\n 200 | # ### Check CO\n 201 | # flow = \"P2P-G&S\"\n 202 | # flow = 'eBay MOR'\n 203 | flow = 'Other'\n 204 | res2.query(f\"weight_alias == 'dol_wgt_nloss' and op == 70 and dimension_value.str.startswith('{flow}*Y').values \")\\\n 205 |     .pivot(index='score_name', columns='dimension_value', values=['score_value','catch_rate'])\n 206 | s1 = df.query(f\" product_flow_rollup == '{flow}' and cnsr_seg == 'Y' and cam24_il_esm_20240912_sapout_CAM24_IL_EMS_ModelScore_sapout >= 716.99\")\\\n 207 | s1.groupby('BRM_BAD_TAG_ASSIGNED')\\\n 208 |   .agg({\"trans_id\":\"count\",\"nloss\":\"sum\",\"dol_wgt\":\"sum\"})\n 209 | s2 = df.query(f\" product_flow_rollup == '{flow}' and cnsr_seg == 'Y' and exp3_2_tot_cam_tc_cls >= 0.05121 \")\n 210 | s2.groupby('BRM_BAD_TAG_ASSIGNED')\\\n 211 | .agg({\"trans_id\":\"count\",\"nloss\":\"sum\",\"dol_wgt\":\"sum\"})\n 212 | len(set(s1[s1.BRM_BAD_TAG_ASSIGNED == '3_STOLEN_CC_CB']['trans_id'])&set(s2[s2.BRM_BAD_TAG_ASSIGNED == '3_STOLEN_CC_CB']['trans_id']))\n 213 | len(set(s1[s1.BRM_BAD_TAG_ASSIGNED == '4_UNAUTH_RESTR']['trans_id'])&set(s2[s2.BRM_BAD_TAG_ASSIGNED == '4_UNAUTH_RESTR']['trans_id']))\n 214 | # ## Score dist\n 215 | df['BRM_BAD_TAG_ASSIGNED'].value_counts()\n 216 | sns.set_palette('husl')\n 217 | # grouped = df.query(\"is_brm3_4_bad == 1.0 and cnsr_seg in ('Y') \")\n 218 | grouped = df.query(\" BRM_BAD_TAG_ASSIGNED == '3_STOLEN_CC_CB' \")\n 219 | custom_palette = sns.color_palette(\"husl\", n_colors=2)\n 220 | for i, score_name in enumerate(['exp6_7_tot_cond_cam24_flow_score', 'exp7_1_all_flow_cond_cam24_flow_score']):\n 221 |     sns.histplot(grouped[score_name], bins=30, kde=True,alpha=0.3, label=score_name,color=custom_palette[i])\n 222 | plt.legend()\n 223 | plt.show()\n 224 | sns.set_palette('husl')\n 225 | # grouped = df.query(\"is_brm3_4_bad == 1.0 and cnsr_seg in ('Y') \")\n 226 | grouped = df.query(\" BRM_BAD_TAG_ASSIGNED == '4_UNAUTH_RESTR' \")\n 227 | custom_palette = sns.color_palette(\"husl\", n_colors=2)4_UNAUTH_RESTR\n 228 | for i, score_name in enumerate(['exp6_7_tot_cond_cam24_flow_score', 'exp7_1_all_flow_cond_cam24_flow_score']):\n 229 |     sns.histplot(grouped[score_name], bins=30, kde=True,alpha=0.3, label=score_name,color=custom_palette[i])\n 230 | plt.legend()\n 231 | plt.show()\n 232 | sns.set_palette('husl')\n 233 | grouped = df.query(\"is_brm3_4_bad == 1.0 and cnsr_seg in ('Y') and score_name in ('exp6_7_tot_cond_cam24_flow_score', 'exp7_1_all_flow_cond_cam24_flow_score')\").groupby('score_name')\n 234 | custom_palette = sns.color_palette(\"husl\", n_colors=len(df['score_name'].unique()))\n 235 | # Plotting the distributions\n 236 | fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n 237 | for i, (group_name, group_data) in enumerate(grouped):\n 238 |     sns.histplot(group_data['exp6_7_tot_cond_cam24_flow_score'], \n 239 |                  bins=30, kde=True, label=f'Group {group_name}', alpha=0.3, color=custom_palette[i],\n 240 |                  ax=axes[0])\n 241 | axes[0].set_title('200 steps score istribution by Group')\n 242 | axes[0].legend()\n 243 | for i, (group_name, group_data) in enumerate(grouped):\n 244 |     sns.histplot(group_data['exp7_1_all_flow_cond_cam24_flow_score'], \n 245 |                  bins=30, kde=True, label=f'Group {group_name}', alpha=0.3, color=custom_palette[i],\n 246 |                  ax=axes[1])\n 247 | axes[1].set_title('2000 steps score istribution by Group')\n 248 | axes[1].legend()\n 249 | for i, (group_name, group_data) in enumerate(grouped):\n 250 |     sns.histplot(group_data['cam_il_0912_sapout'], \n 251 |                  bins=30, kde=True, label=f'Group {group_name}', alpha=0.3, color=custom_palette[i],\n 252 |                  ax=axes[2])\n 253 | axes[2].set_title('rmr score istribution by Group')\n 254 | axes[2].legend()\n 255 | # plt.xlabel('Score')\n 256 | # plt.ylabel('Frequency')\n 257 | plt.legend()\n 258 | plt.grid(False)\n 259 | plt.show()\n 260 | # ### TSNE vis\n 261 | # * bad dist: check bad3/4 distribution in each segment\n 262 | # * high FP: \n 263 | df['is_brm3_4_bad'].value_counts()\n 264 | sample_df = pd.concat([df[df.is_brm3_4_bad == 0.0].sample(frac=0.02), df[df.is_brm3_4_bad == 1.0].sample(frac=1.0)])\n 265 | sample_df.shape\n 266 | sample_df.groupby(['BRM_BAD_TAG_ASSIGNED', 'cnsr_seg'])['trans_id'].agg(['count']).unstack().fillna(0)\n 267 | feature_list = [c for c in df.columns if c.startswith(\"cam24_09_embed\")]\n 268 | len(feature_list)\n 269 | x = sample_df[feature_list].to_numpy()\n 270 | tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n 271 | tsne_results = tsne.fit_transform(x)\n 272 | sample_df['tsne-2d-one'] = tsne_results[:,0]\n 273 | sample_df['tsne-2d-two'] = tsne_results[:,1]\n 274 | sample_df.shape\n 275 | sample_df['seg_bad'] = sample_df['cnsr_seg']+\"_\"+sample_df['BRM_BAD_TAG_ASSIGNED'].astype(str)\n 276 | sample_df['is_dcln_old'] = sample_df['exp7_36_week1_esm_cam_cls_src'].apply(lambda x:'high_36' if x >= 0.8 else'low_36')\n 277 | sample_df['is_dcln_new'] = sample_df['exp7_32_week1_esm_cam_cls_src'].apply(lambda x:'high_32' if x >= 0.8 else'low_32')\n 278 | sample_df['brm_bad_tag_assigned'].unique()\n 279 | sample_df.reset_index().to_feather('/projects/gds-focus/data/catch/IL/CAM/00_pipeline/scoring/oot_week1_9_exp7_week1_34_36_sampled.feather')\n 280 | sample_df = pd.read_feather('/projects/gds-focus/data/catch/IL/CAM/00_pipeline/scoring/oot_week1_9_exp7_week1_34_36_sampled.feather')\n 281 | # #### **bad dist: bad3/4 * segment**\n 282 | hue_col = 'BRM_BAD_TAG_ASSIGNED'\n 283 | filter_df = sample_df.query(\" is_brm3_4_bad == 1.0 and BRM_BAD_TAG_ASSIGNED in ('3_STOLEN_CC_CB', '4_UNAUTH_RESTR') \")#.sample(frac=0.1)\n 284 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n 285 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n 286 | hue_count = tmp[hue_col].nunique()\n 287 | sns.scatterplot(\n 288 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 289 |     hue=hue_col,\n 290 |     palette=sns.color_palette(\"hls\",hue_count),\n 291 |     data=tmp,\n 292 |     legend=\"brief\",\n 293 |     alpha=0.2,\n 294 |     ax=axes[0][0]\n 295 | )\n 296 | axes[0][0].set_title('segment Y')\n 297 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 298 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n 299 | hue_count = tmp[hue_col].nunique()\n 300 | sns.scatterplot(\n 301 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 302 |     hue=hue_col,\n 303 |     palette=sns.color_palette(\"hls\",hue_count),\n 304 |     data=tmp,\n 305 |     legend=\"brief\",\n 306 |     alpha=0.2,\n 307 |     ax=axes[0][1]\n 308 | )\n 309 | axes[0][1].set_title('segment G')\n 310 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 311 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n 312 | hue_count = tmp[hue_col].nunique()\n 313 | sns.scatterplot(\n 314 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 315 |     hue=hue_col,\n 316 |     palette=sns.color_palette(\"hls\",hue_count),\n 317 |     data=tmp,\n 318 |     legend=\"brief\",\n 319 |     alpha=0.2,\n 320 |     ax=axes[1][0]\n 321 | )\n 322 | axes[1][0].set_title('segment C')\n 323 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 324 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n 325 | hue_count = tmp[hue_col].nunique()\n 326 | sns.scatterplot(\n 327 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 328 |     hue=hue_col,\n 329 |     palette=sns.color_palette(\"hls\",hue_count),\n 330 |     data=tmp,\n 331 |     legend=\"brief\",\n 332 |     alpha=0.2,\n 333 |     ax=axes[1][1]\n 334 | )\n 335 | axes[1][1].set_title('segment T')\n 336 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 337 | # #### False positive case deep dive\n 338 | sample_df['brm_bad_tag_assigned'].unique()\n 339 | hue_col = 'BRM_BAD_TAG_ASSIGNED'\n 340 | filter_df = sample_df.query(\" cam_il_0912_sapout >= 727 and BRM_BAD_TAG_ASSIGNED in ('~', '3_STOLEN_CC_CB')\")#.sample(frac=0.1)\n 341 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n 342 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n 343 | hue_count = tmp[hue_col].nunique()\n 344 | sns.scatterplot(\n 345 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 346 |     hue=hue_col,\n 347 |     palette=sns.color_palette(\"hls\",hue_count),\n 348 |     data=tmp,\n 349 |     legend=\"brief\",\n 350 |     alpha=0.2,\n 351 |     ax=axes[0][0]\n 352 | )\n 353 | axes[0][0].set_title('segment Y')\n 354 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 355 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n 356 | hue_count = tmp[hue_col].nunique()\n 357 | sns.scatterplot(\n 358 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 359 |     hue=hue_col,\n 360 |     palette=sns.color_palette(\"hls\",hue_count),\n 361 |     data=tmp,\n 362 |     legend=\"brief\",\n 363 |     alpha=0.2,\n 364 |     ax=axes[0][1]\n 365 | )\n 366 | axes[0][1].set_title('segment G')\n 367 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 368 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n 369 | hue_count = tmp[hue_col].nunique()\n 370 | sns.scatterplot(\n 371 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 372 |     hue=hue_col,\n 373 |     palette=sns.color_palette(\"hls\",hue_count),\n 374 |     data=tmp,\n 375 |     legend=\"brief\",\n 376 |     alpha=0.2,\n 377 |     ax=axes[1][0]\n 378 | )\n 379 | axes[1][0].set_title('segment C')\n 380 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 381 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n 382 | hue_count = tmp[hue_col].nunique()\n 383 | sns.scatterplot(\n 384 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 385 |     hue=hue_col,\n 386 |     palette=sns.color_palette(\"hls\",hue_count),\n 387 |     data=tmp,\n 388 |     legend=\"brief\",\n 389 |     alpha=0.2,\n 390 |     ax=axes[1][1]\n 391 | )\n 392 | axes[1][1].set_title('segment T')\n 393 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 394 | hue_col = 'BRM_BAD_TAG_ASSIGNED'\n 395 | filter_df = sample_df.query(\" exp4_6_ebaymor_cond_cam24_flow_score >= 0.06 and BRM_BAD_TAG_ASSIGNED in ('~', '3_STOLEN_CC_CB')\")#.sample(frac=0.1)\n 396 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n 397 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n 398 | hue_count = tmp[hue_col].nunique()\n 399 | sns.scatterplot(\n 400 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 401 |     hue=hue_col,\n 402 |     palette=sns.color_palette(\"hls\",hue_count),\n 403 |     data=tmp,\n 404 |     legend=\"brief\",\n 405 |     alpha=0.2,\n 406 |     ax=axes[0][0]\n 407 | )\n 408 | axes[0][0].set_title('segment Y')\n 409 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 410 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n 411 | hue_count = tmp[hue_col].nunique()\n 412 | sns.scatterplot(\n 413 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 414 |     hue=hue_col,\n 415 |     palette=sns.color_palette(\"hls\",hue_count),\n 416 |     data=tmp,\n 417 |     legend=\"brief\",\n 418 |     alpha=0.2,\n 419 |     ax=axes[0][1]\n 420 | )\n 421 | axes[0][1].set_title('segment G')\n 422 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 423 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n 424 | hue_count = tmp[hue_col].nunique()\n 425 | sns.scatterplot(\n 426 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 427 |     hue=hue_col,\n 428 |     palette=sns.color_palette(\"hls\",hue_count),\n 429 |     data=tmp,\n 430 |     legend=\"brief\",\n 431 |     alpha=0.2,\n 432 |     ax=axes[1][0]\n 433 | )\n 434 | axes[1][0].set_title('segment C')\n 435 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 436 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n 437 | hue_count = tmp[hue_col].nunique()\n 438 | sns.scatterplot(\n 439 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 440 |     hue=hue_col,\n 441 |     palette=sns.color_palette(\"hls\",hue_count),\n 442 |     data=tmp,\n 443 |     legend=\"brief\",\n 444 |     alpha=0.2,\n 445 |     ax=axes[1][1]\n 446 | )\n 447 | axes[1][1].set_title('segment T')\n 448 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 449 | sample_df['product_flow_rollup'].unique()\n 450 | hue_col = 'brm_bad_tag_assigned'\n 451 | filter_df = sample_df.query(\" (is_high_exp32 == 'high_32' and is_high_exp36 == 'high_36') and product_flow_rollup in ('Branded XO')\")#.sample(frac=0.1)\n 452 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n 453 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n 454 | hue_count = tmp[hue_col].nunique()\n 455 | sns.scatterplot(\n 456 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 457 |     hue=hue_col,\n 458 |     palette=sns.color_palette(\"hls\",hue_count),\n 459 |     data=tmp,\n 460 |     legend=\"brief\",\n 461 |     alpha=0.2,\n 462 |     ax=axes[0][0]\n 463 | )\n 464 | axes[0][0].set_title('segment Y')\n 465 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 466 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n 467 | hue_count = tmp[hue_col].nunique()\n 468 | sns.scatterplot(\n 469 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 470 |     hue=hue_col,\n 471 |     palette=sns.color_palette(\"hls\",hue_count),\n 472 |     data=tmp,\n 473 |     legend=\"brief\",\n 474 |     alpha=0.2,\n 475 |     ax=axes[0][1]\n 476 | )\n 477 | axes[0][1].set_title('segment G')\n 478 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 479 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n 480 | hue_count = tmp[hue_col].nunique()\n 481 | sns.scatterplot(\n 482 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 483 |     hue=hue_col,\n 484 |     palette=sns.color_palette(\"hls\",hue_count),\n 485 |     data=tmp,\n 486 |     legend=\"brief\",\n 487 |     alpha=0.2,\n 488 |     ax=axes[1][0]\n 489 | )\n 490 | axes[1][0].set_title('segment C')\n 491 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 492 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n 493 | hue_count = tmp[hue_col].nunique()\n 494 | sns.scatterplot(\n 495 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 496 |     hue=hue_col,\n 497 |     palette=sns.color_palette(\"hls\",hue_count),\n 498 |     data=tmp,\n 499 |     legend=\"brief\",\n 500 |     alpha=0.2,\n 501 |     ax=axes[1][1]\n 502 | )\n 503 | axes[1][1].set_title('segment T')\n 504 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 505 | def swap_split(score1, score2):\n 506 |     if score1 >= 0.06 and score2 >= 0.06:\n 507 |         return 'both_high'\n 508 |     elif score1 >= 0.06 and score2 < 0.06:\n 509 |         return '200_high'\n 510 |     elif score1 < 0.06 and score2 >= 0.06:\n 511 |         return '2000_high'\n 512 |     else:\n 513 |         return 'both_low'\n 514 | sample_df['score_band'] = sample_df[['exp7_32_week1_esm_cam_cls_src',\n 515 |                                      'exp7_36_week1_esm_cam_cls_src']].apply(lambda x:swap_split(x[0],x[1]), axis=1)\n 516 | sample_df['score_band'].value_counts()\n 517 | hue_col = 'brm_bad_tag_assigned'\n 518 | base_df = sample_df.query(\" brm_bad_tag_assigned in ('~', '3_STOLEN_CC_CB', '4_UNAUTH_RESTR') and product_flow_rollup in ('P2P-G&S')\")#.sample(frac=0.1)\n 519 | for product in list(base_df['score_band'].unique()):\n 520 |     filter_df = base_df.query(f\" score_band in ('{product}') \")#.sample(frac=0.1)\n 521 |     fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n 522 |     tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n 523 |     hue_count = tmp[hue_col].nunique()\n 524 |     sns.scatterplot(\n 525 |         x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 526 |         hue=hue_col,\n 527 |         palette=sns.color_palette(\"hls\",hue_count),\n 528 |         data=tmp,\n 529 |         legend=\"brief\",\n 530 |         alpha=0.3,\n 531 |         ax=axes[0]\n 532 |     )\n 533 |     axes[0].set_title('segment Y')\n 534 |     tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n 535 |     hue_count = tmp[hue_col].nunique()\n 536 |     sns.scatterplot(\n 537 |         x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 538 |         hue=hue_col,\n 539 |         palette=sns.color_palette(\"hls\",hue_count),\n 540 |         data=tmp,\n 541 |         legend=\"brief\",\n 542 |         alpha=0.3,\n 543 |         ax=axes[1]\n 544 |     )\n 545 |     axes[1].set_title('segment C')\n 546 | # #### stability v.s. confidence\n 547 | sample_df['gap'] = sample_df['exp7_32_week1_esm_cam_cls_src'] - sample_df['exp7_36_week1_esm_cam_cls_src']\n 548 | hue_col = 'brm_bad_tag_assigned'\n 549 | filter_df = sample_df.query(\" abs(gap) > 0.01 and product_flow_rollup in ('Branded XO') and brm_bad_tag_assigned in ('~', '3_STOLEN_CC_CB', '4_UNAUTH_RESTR')\")#.sample(frac=0.1)\n 550 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n 551 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n 552 | hue_count = tmp[hue_col].nunique()\n 553 | sns.scatterplot(\n 554 |     x=\"gap\", y=\"CAM22_RMR_202401_sapout_CAM22_RMR_202401_sapout\",\n 555 |     hue=hue_col,\n 556 |     palette=sns.color_palette(\"hls\",hue_count),\n 557 |     data=tmp,\n 558 |     legend=\"brief\",\n 559 |     alpha=0.2,\n 560 |     ax=axes[0][0]\n 561 | )\n 562 | axes[0][0].set_title('segment Y')\n 563 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 564 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n 565 | hue_count = tmp[hue_col].nunique()\n 566 | sns.scatterplot(\n 567 |     x=\"gap\", y=\"CAM22_RMR_202401_sapout_CAM22_RMR_202401_sapout\",\n 568 |     hue=hue_col,\n 569 |     palette=sns.color_palette(\"hls\",hue_count),\n 570 |     data=tmp,\n 571 |     legend=\"brief\",\n 572 |     alpha=0.2,\n 573 |     ax=axes[0][1]\n 574 | )\n 575 | axes[0][1].set_title('segment G')\n 576 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 577 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n 578 | hue_count = tmp[hue_col].nunique()\n 579 | sns.scatterplot(\n 580 |     x=\"gap\", y=\"CAM22_RMR_202401_sapout_CAM22_RMR_202401_sapout\",\n 581 |     hue=hue_col,\n 582 |     palette=sns.color_palette(\"hls\",hue_count),\n 583 |     data=tmp,\n 584 |     legend=\"brief\",\n 585 |     alpha=0.2,\n 586 |     ax=axes[1][0]\n 587 | )\n 588 | axes[1][0].set_title('segment C')\n 589 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 590 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n 591 | hue_count = tmp[hue_col].nunique()\n 592 | sns.scatterplot(\n 593 |     x=\"gap\", y=\"CAM22_RMR_202401_sapout_CAM22_RMR_202401_sapout\",\n 594 |     hue=hue_col,\n 595 |     palette=sns.color_palette(\"hls\",hue_count),\n 596 |     data=tmp,\n 597 |     legend=\"brief\",\n 598 |     alpha=0.2,\n 599 |     ax=axes[1][1]\n 600 | )\n 601 | axes[1][1].set_title('segment T')\n 602 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 603 | hue_col = 'brm_bad_tag_assigned'\n 604 | filter_df = sample_df.query(\" abs(gap) > 0.01 and is_high_exp36 == 'high_36' and product_flow_rollup in ('Branded XO') and brm_bad_tag_assigned in ('~', '3_STOLEN_CC_CB', '4_UNAUTH_RESTR')\")#.sample(frac=0.1)\n 605 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n 606 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n 607 | hue_count = tmp[hue_col].nunique()\n 608 | sns.scatterplot(\n 609 |     x=\"gap\", y=\"CAM22_RMR_202401_sapout_CAM22_RMR_202401_sapout\",\n 610 |     hue=hue_col,\n 611 |     palette=sns.color_palette(\"hls\",hue_count),\n 612 |     data=tmp,\n 613 |     legend=\"brief\",\n 614 |     alpha=0.2,\n 615 |     ax=axes[0][0]\n 616 | )\n 617 | axes[0][0].set_title('segment Y')\n 618 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 619 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n 620 | hue_count = tmp[hue_col].nunique()\n 621 | sns.scatterplot(\n 622 |     x=\"gap\", y=\"CAM22_RMR_202401_sapout_CAM22_RMR_202401_sapout\",\n 623 |     hue=hue_col,\n 624 |     palette=sns.color_palette(\"hls\",hue_count),\n 625 |     data=tmp,\n 626 |     legend=\"brief\",\n 627 |     alpha=0.2,\n 628 |     ax=axes[0][1]\n 629 | )\n 630 | axes[0][1].set_title('segment G')\n 631 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 632 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n 633 | hue_count = tmp[hue_col].nunique()\n 634 | sns.scatterplot(\n 635 |     x=\"gap\", y=\"CAM22_RMR_202401_sapout_CAM22_RMR_202401_sapout\",\n 636 |     hue=hue_col,\n 637 |     palette=sns.color_palette(\"hls\",hue_count),\n 638 |     data=tmp,\n 639 |     legend=\"brief\",\n 640 |     alpha=0.2,\n 641 |     ax=axes[1][0]\n 642 | )\n 643 | axes[1][0].set_title('segment C')\n 644 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 645 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n 646 | hue_count = tmp[hue_col].nunique()\n 647 | sns.scatterplot(\n 648 |     x=\"gap\", y=\"CAM22_RMR_202401_sapout_CAM22_RMR_202401_sapout\",\n 649 |     hue=hue_col,\n 650 |     palette=sns.color_palette(\"hls\",hue_count),\n 651 |     data=tmp,\n 652 |     legend=\"brief\",\n 653 |     alpha=0.2,\n 654 |     ax=axes[1][1]\n 655 | )\n 656 | axes[1][1].set_title('segment T')\n 657 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 658 | # #### sample time period\n 659 | sample_df['madmen_monthly'].value_counts()\n 660 | hue_col = 'madmen_monthly'\n 661 | filter_df = sample_df.query(\" is_brm3_4_bad == 1.0 and brm_bad_tag_assigned in ('3_STOLEN_CC_CB', '4_UNAUTH_RESTR') \")#.sample(frac=0.1)\n 662 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n 663 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n 664 | hue_count = tmp[hue_col].nunique()\n 665 | sns.scatterplot(\n 666 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 667 |     hue=hue_col,\n 668 |     palette=sns.color_palette(\"hls\",hue_count),\n 669 |     data=tmp,\n 670 |     legend=\"brief\",\n 671 |     alpha=0.2,\n 672 |     ax=axes[0][0]\n 673 | )\n 674 | axes[0][0].set_title('segment Y')\n 675 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 676 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n 677 | hue_count = tmp[hue_col].nunique()\n 678 | sns.scatterplot(\n 679 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 680 |     hue=hue_col,\n 681 |     palette=sns.color_palette(\"hls\",hue_count),\n 682 |     data=tmp,\n 683 |     legend=\"brief\",\n 684 |     alpha=0.2,\n 685 |     ax=axes[0][1]\n 686 | )\n 687 | axes[0][1].set_title('segment G')\n 688 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 689 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n 690 | hue_count = tmp[hue_col].nunique()\n 691 | sns.scatterplot(\n 692 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 693 |     hue=hue_col,\n 694 |     palette=sns.color_palette(\"hls\",hue_count),\n 695 |     data=tmp,\n 696 |     legend=\"brief\",\n 697 |     alpha=0.2,\n 698 |     ax=axes[1][0]\n 699 | )\n 700 | axes[1][0].set_title('segment C')\n 701 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 702 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n 703 | hue_count = tmp[hue_col].nunique()\n 704 | sns.scatterplot(\n 705 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 706 |     hue=hue_col,\n 707 |     palette=sns.color_palette(\"hls\",hue_count),\n 708 |     data=tmp,\n 709 |     legend=\"brief\",\n 710 |     alpha=0.2,\n 711 |     ax=axes[1][1]\n 712 | )\n 713 | axes[1][1].set_title('segment T')\n 714 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 715 | #\n 716 | list(df.columns)\n 717 | df['is_high_exp36'] = df['exp7_36_week1_esm_cam_cls_src'].apply(lambda x:1 if x > 0.8 else 0)\n 718 | df['cam22_ems_score'] = df['cam22_ems_score'].fillna(0)\n 719 | df['cam22_ems_score'] = df['cam22_ems_score'].replace({-999:0.0})\n 720 | sns.set_palette('husl')\n 721 | grouped = df.query(\"is_high_exp36 == 1 and is_brm3_4_bad == 0.0 \").sample(frac=0.1).groupby('cnsr_seg')\n 722 | custom_palette = sns.color_palette(\"husl\", n_colors=len(df['cnsr_seg'].unique()))\n 723 | # Plotting the distributions\n 724 | fig, axes = plt.subplots(1, 4, figsize=(16, 6))\n 725 | for i, (group_name, group_data) in enumerate(grouped):\n 726 |     sns.histplot(group_data['exp7_32_week1_esm_cam_cls_src'], \n 727 |                  bins=30, kde=True, label=f'Group {group_name}', alpha=0.3, color=custom_palette[i],\n 728 |                  ax=axes[0])\n 729 | axes[0].set_title('200 steps score istribution by Group')\n 730 | axes[0].legend()\n 731 | for i, (group_name, group_data) in enumerate(grouped):\n 732 |     sns.histplot(group_data['exp7_36_week1_esm_cam_cls_src'], \n 733 |                  bins=30, kde=True, label=f'Group {group_name}', alpha=0.3, color=custom_palette[i],\n 734 |                  ax=axes[1])\n 735 | axes[1].set_title('2000 steps score istribution by Group')\n 736 | axes[1].legend()\n 737 | for i, (group_name, group_data) in enumerate(grouped):\n 738 |     sns.histplot(group_data['CAM22_RMR_202401_sapout_CAM22_RMR_202401_sapout'], \n 739 |                  bins=30, kde=True, label=f'Group {group_name}', alpha=0.3, color=custom_palette[i],\n 740 |                  ax=axes[2])\n 741 | axes[2].set_title('rmr score istribution by Group')\n 742 | axes[2].legend()\n 743 | for i, (group_name, group_data) in enumerate(grouped):\n 744 |     sns.histplot(group_data['cam22_ems_score'], \n 745 |                  bins=30, kde=True, label=f'Group {group_name}', alpha=0.3, color=custom_palette[i],\n 746 |                  ax=axes[3])\n 747 | axes[3].set_title('cam22 score istribution by Group')\n 748 | axes[3].legend()\n 749 | # plt.xlabel('Score')\n 750 | # plt.ylabel('Frequency')\n 751 | plt.legend()\n 752 | plt.grid(False)\n 753 | plt.show()\n 754 | df.query(\"is_high_exp36 == 1\").groupby(['cc_control',\n 755 |                                         'is_brm3_4_bad'])['trans_id'].agg(['count']).unstack()\n 756 | df['gap'] = df['exp7_32_week1_esm_cam_cls_src']-df['exp7_36_week1_esm_cam_cls_src']\n 757 | sns.set_palette('husl')\n 758 | grouped = df.query(\"is_high_exp36 == 1 and is_brm3_4_bad == 1.0 \").sample(frac=0.1).groupby('cnsr_seg')\n 759 | custom_palette = sns.color_palette(\"husl\", n_colors=len(df['cnsr_seg'].unique()))\n 760 | # Plotting the distributions\n 761 | fig, axes = plt.subplots(1,1,figsize=(8, 6))\n 762 | for i, (group_name, group_data) in enumerate(grouped):\n 763 |     sns.histplot(group_data['gap'], \n 764 |                  bins=30, kde=True, label=f'Group {group_name}', alpha=0.3, color=custom_palette[i],\n 765 |                 )\n 766 | # plt.xlabel('Score')\n 767 | # plt.ylabel('Frequency')\n 768 | plt.legend()\n 769 | plt.grid(False)\n 770 | plt.show()\n 771 | sns.set_palette('husl')\n 772 | grouped = df.query(\"is_high_exp36 == 1 and is_brm3_4_bad == 0.0 \").sample(frac=0.1).groupby('cnsr_seg')\n 773 | custom_palette = sns.color_palette(\"husl\", n_colors=len(df['cnsr_seg'].unique()))\n 774 | # Plotting the distributions\n 775 | fig, axes = plt.subplots(1,1,figsize=(8, 6))\n 776 | for i, (group_name, group_data) in enumerate(grouped):\n 777 |     sns.histplot(group_data['gap'], \n 778 |                  bins=30, kde=True, label=f'Group {group_name}', alpha=0.3, color=custom_palette[i],\n 779 |                 )\n 780 | # plt.xlabel('Score')\n 781 | # plt.ylabel('Frequency')\n 782 | plt.legend()\n 783 | plt.grid(False)\n 784 | plt.show()\n 785 | sns.set_palette('husl')\n 786 | hue_col = 'cnsr_seg'\n 787 | tmp = df.query(\" abs(gap) > 0.01 \").sample(frac=0.1)\n 788 | hue_count = tmp[hue_col].nunique()\n 789 | custom_palette = sns.color_palette(\"husl\", n_colors=hue_count)\n 790 | # Plotting the distributions\n 791 | fig, axes = plt.subplots(1,1,figsize=(8, 6))\n 792 | sns.scatterplot(\n 793 |         x=\"gap\", y=\"CAM22_RMR_202401_sapout_CAM22_RMR_202401_sapout\",\n 794 |         hue=hue_col,\n 795 |         palette=sns.color_palette(\"hls\",hue_count),\n 796 |         data=tmp,\n 797 |         legend=\"brief\",\n 798 |         alpha=0.3,\n 799 |     )\n 800 | # plt.xlabel('Score')\n 801 | # plt.ylabel('Frequency')\n 802 | plt.legend()\n 803 | plt.grid(False)\n 804 | plt.show()\n 805 | # #### training data vis\n 806 | # !du -h -s /projects/gds-focus/data/catch/IL/CAM/00_pipeline/scoring/offsim_embed_week1_9\n 807 | # !ls /projects/gds-focus/data/catch/IL/CAM/00_pipeline/scoring/offsim_embed_week1_9 | head -5\n 808 | score_df = pd.read_parquet('/projects/gds-focus/data/catch/IL/CAM/00_pipeline/scoring/offsim_embed_week1_9')\n 809 | score_df.columns[:50]\n 810 | score_df.columns[-80:]\n 811 | score_df.shape\n 812 | score_df = score_df.query(\" is_week1_need == '1' \")\n 813 | score_df['exp7_22_week1_esm_cam_cls_src'] = score_df['exp7_22_week1_esm_cam_cls_src'].astype(float)\n 814 | score_df['exp7_26_week1_esm_cam_cls_src'] = score_df['exp7_26_week1_esm_cam_cls_src'].astype(float)\n 815 | score_df.query(\" is_brm3_4_11_15_bad == 0.0 \")['exp7_22_week1_esm_cam_cls_src'].hist(legend=True)\n 816 | score_df.query(\" is_brm3_4_11_15_bad == 1.0 \")['exp7_22_week1_esm_cam_cls_src'].hist(legend=True)\n 817 | score_df.query(\" is_brm3_4_11_15_bad == 0.0 \")['exp7_26_week1_esm_cam_cls_src'].hist(legend=True)\n 818 | score_df['is_recent'] = score_df['pmt_start_date'].apply(lambda x:1 if x > '2023-10-30' else 0)\n 819 | score_df['is_recent'].value_counts()\n 820 | # #### TSNE vis\n 821 | sample_df = pd.concat([score_df[score_df.is_brm3_4_11_15_bad == 0.0].sample(frac=0.005), score_df[score_df.is_brm3_4_11_15_bad == 1.0].sample(frac=0.05)])\n 822 | sample_df.groupby(['is_recent','is_brm3_4_11_15_bad'])['trans_id'].agg(['count']).unstack()\n 823 | score_df.groupby(['is_recent','is_y_g','is_brm3_4_11_15_bad'])['trans_id'].agg(['count']).reset_index()\n 824 | sample_df.shape\n 825 | feature_list = ['cam_rmr_adapt_cls_embed_output_'+str(i) for i in range(128)]\n 826 | x = sample_df[feature_list].to_numpy()\n 827 | tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n 828 | tsne_results = tsne.fit_transform(x)\n 829 | sample_df['tsne-2d-one'] = tsne_results[:,0]\n 830 | sample_df['tsne-2d-two'] = tsne_results[:,1]\n 831 | # #### cg/ncg\n 832 | hue_col = 'brm_bad_tag_assigned'\n 833 | filter_df = sample_df.query(\" is_brm3_4_11_15_bad >= 0.0 and brm_bad_tag_assigned in ('~','3_STOLEN_CC_CB', '4_UNAUTH_RESTR') \")#.sample(frac=0.1)\n 834 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n 835 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n 836 | hue_count = tmp[hue_col].nunique()\n 837 | sns.scatterplot(\n 838 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 839 |     hue=hue_col,\n 840 |     palette=sns.color_palette(\"hls\",hue_count),\n 841 |     data=tmp,\n 842 |     legend=\"brief\",\n 843 |     alpha=0.2,\n 844 |     ax=axes[0][0]\n 845 | )\n 846 | axes[0][0].set_title('segment Y')\n 847 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 848 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n 849 | hue_count = tmp[hue_col].nunique()\n 850 | sns.scatterplot(\n 851 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 852 |     hue=hue_col,\n 853 |     palette=sns.color_palette(\"hls\",hue_count),\n 854 |     data=tmp,\n 855 |     legend=\"brief\",\n 856 |     alpha=0.2,\n 857 |     ax=axes[0][1]\n 858 | )\n 859 | axes[0][1].set_title('segment G')\n 860 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 861 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n 862 | hue_count = tmp[hue_col].nunique()\n 863 | sns.scatterplot(\n 864 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 865 |     hue=hue_col,\n 866 |     palette=sns.color_palette(\"hls\",hue_count),\n 867 |     data=tmp,\n 868 |     legend=\"brief\",\n 869 |     alpha=0.2,\n 870 |     ax=axes[1][0]\n 871 | )\n 872 | axes[1][0].set_title('segment C')\n 873 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 874 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n 875 | hue_count = tmp[hue_col].nunique()\n 876 | sns.scatterplot(\n 877 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 878 |     hue=hue_col,\n 879 |     palette=sns.color_palette(\"hls\",hue_count),\n 880 |     data=tmp,\n 881 |     legend=\"brief\",\n 882 |     alpha=0.2,\n 883 |     ax=axes[1][1]\n 884 | )\n 885 | axes[1][1].set_title('segment T')\n 886 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n 887 | hue_col = 'cc_control'\n 888 | filter_df = sample_df.query(\" is_brm3_4_11_15_bad == 1.0 and brm_bad_tag_assigned in ('3_STOLEN_CC_CB') \")#.sample(frac=0.1)\n 889 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n 890 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n 891 | hue_count = tmp[hue_col].nunique()\n 892 | sns.scatterplot(\n 893 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 894 |     hue=hue_col,\n 895 |     palette=sns.color_palette(\"hls\",hue_count),\n 896 |     data=tmp,\n 897 |     legend=\"brief\",\n 898 |     alpha=0.2,\n 899 |     ax=axes[0][0]\n 900 | )\n 901 | axes[0][0].set_title('segment Y')\n 902 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n 903 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n 904 | hue_count = tmp[hue_col].nunique()\n 905 | sns.scatterplot(\n 906 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 907 |     hue=hue_col,\n 908 |     palette=sns.color_palette(\"hls\",hue_count),\n 909 |     data=tmp,\n 910 |     legend=\"brief\",\n 911 |     alpha=0.2,\n 912 |     ax=axes[0][1]\n 913 | )\n 914 | axes[0][1].set_title('segment G')\n 915 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n 916 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n 917 | hue_count = tmp[hue_col].nunique()\n 918 | sns.scatterplot(\n 919 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 920 |     hue=hue_col,\n 921 |     palette=sns.color_palette(\"hls\",hue_count),\n 922 |     data=tmp,\n 923 |     legend=\"brief\",\n 924 |     alpha=0.2,\n 925 |     ax=axes[1][0]\n 926 | )\n 927 | axes[1][0].set_title('segment C')\n 928 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n 929 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n 930 | hue_count = tmp[hue_col].nunique()\n 931 | sns.scatterplot(\n 932 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 933 |     hue=hue_col,\n 934 |     palette=sns.color_palette(\"hls\",hue_count),\n 935 |     data=tmp,\n 936 |     legend=\"brief\",\n 937 |     alpha=0.2,\n 938 |     ax=axes[1][1]\n 939 | )\n 940 | axes[1][1].set_title('segment T')\n 941 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n 942 | hue_col = 'cc_control'\n 943 | filter_df = sample_df.query(\" is_brm3_4_11_15_bad == 0.0 and brm_bad_tag_assigned in ('~') \")#.sample(frac=0.1)\n 944 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n 945 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n 946 | hue_count = tmp[hue_col].nunique()\n 947 | sns.scatterplot(\n 948 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 949 |     hue=hue_col,\n 950 |     palette=sns.color_palette(\"hls\",hue_count),\n 951 |     data=tmp,\n 952 |     legend=\"brief\",\n 953 |     alpha=0.2,\n 954 |     ax=axes[0][0]\n 955 | )\n 956 | axes[0][0].set_title('segment Y')\n 957 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n 958 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n 959 | hue_count = tmp[hue_col].nunique()\n 960 | sns.scatterplot(\n 961 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 962 |     hue=hue_col,\n 963 |     palette=sns.color_palette(\"hls\",hue_count),\n 964 |     data=tmp,\n 965 |     legend=\"brief\",\n 966 |     alpha=0.2,\n 967 |     ax=axes[0][1]\n 968 | )\n 969 | axes[0][1].set_title('segment G')\n 970 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n 971 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n 972 | hue_count = tmp[hue_col].nunique()\n 973 | sns.scatterplot(\n 974 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 975 |     hue=hue_col,\n 976 |     palette=sns.color_palette(\"hls\",hue_count),\n 977 |     data=tmp,\n 978 |     legend=\"brief\",\n 979 |     alpha=0.2,\n 980 |     ax=axes[1][0]\n 981 | )\n 982 | axes[1][0].set_title('segment C')\n 983 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n 984 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n 985 | hue_count = tmp[hue_col].nunique()\n 986 | sns.scatterplot(\n 987 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n 988 |     hue=hue_col,\n 989 |     palette=sns.color_palette(\"hls\",hue_count),\n 990 |     data=tmp,\n 991 |     legend=\"brief\",\n 992 |     alpha=0.2,\n 993 |     ax=axes[1][1]\n 994 | )\n 995 | axes[1][1].set_title('segment T')\n 996 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n 997 | # #### data replay/recent data\n 998 | hue_col = 'is_recent'\n 999 | filter_df = sample_df.query(\" is_brm3_4_11_15_bad == 1.0 and brm_bad_tag_assigned in ('3_STOLEN_CC_CB') \")#.sample(frac=0.1)\n1000 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n1001 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n1002 | hue_count = tmp[hue_col].nunique()\n1003 | sns.scatterplot(\n1004 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1005 |     hue=hue_col,\n1006 |     palette=sns.color_palette(\"hls\",hue_count),\n1007 |     data=tmp,\n1008 |     legend=\"brief\",\n1009 |     alpha=0.2,\n1010 |     ax=axes[0][0]\n1011 | )\n1012 | axes[0][0].set_title('segment Y')\n1013 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n1014 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n1015 | hue_count = tmp[hue_col].nunique()\n1016 | sns.scatterplot(\n1017 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1018 |     hue=hue_col,\n1019 |     palette=sns.color_palette(\"hls\",hue_count),\n1020 |     data=tmp,\n1021 |     legend=\"brief\",\n1022 |     alpha=0.2,\n1023 |     ax=axes[0][1]\n1024 | )\n1025 | axes[0][1].set_title('segment G')\n1026 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n1027 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n1028 | hue_count = tmp[hue_col].nunique()\n1029 | sns.scatterplot(\n1030 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1031 |     hue=hue_col,\n1032 |     palette=sns.color_palette(\"hls\",hue_count),\n1033 |     data=tmp,\n1034 |     legend=\"brief\",\n1035 |     alpha=0.2,\n1036 |     ax=axes[1][0]\n1037 | )\n1038 | axes[1][0].set_title('segment C')\n1039 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n1040 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n1041 | hue_count = tmp[hue_col].nunique()\n1042 | sns.scatterplot(\n1043 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1044 |     hue=hue_col,\n1045 |     palette=sns.color_palette(\"hls\",hue_count),\n1046 |     data=tmp,\n1047 |     legend=\"brief\",\n1048 |     alpha=0.2,\n1049 |     ax=axes[1][1]\n1050 | )\n1051 | axes[1][1].set_title('segment T')\n1052 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n1053 | hue_col = 'is_recent'\n1054 | filter_df = sample_df.query(\" is_brm3_4_11_15_bad == 0.0 and brm_bad_tag_assigned in ('~') \")#.sample(frac=0.1)\n1055 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n1056 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n1057 | hue_count = tmp[hue_col].nunique()\n1058 | sns.scatterplot(\n1059 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1060 |     hue=hue_col,\n1061 |     palette=sns.color_palette(\"hls\",hue_count),\n1062 |     data=tmp,\n1063 |     legend=\"brief\",\n1064 |     alpha=0.2,\n1065 |     ax=axes[0][0]\n1066 | )\n1067 | axes[0][0].set_title('segment Y')\n1068 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n1069 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n1070 | hue_count = tmp[hue_col].nunique()\n1071 | sns.scatterplot(\n1072 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1073 |     hue=hue_col,\n1074 |     palette=sns.color_palette(\"hls\",hue_count),\n1075 |     data=tmp,\n1076 |     legend=\"brief\",\n1077 |     alpha=0.2,\n1078 |     ax=axes[0][1]\n1079 | )\n1080 | axes[0][1].set_title('segment G')\n1081 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n1082 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n1083 | hue_count = tmp[hue_col].nunique()\n1084 | sns.scatterplot(\n1085 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1086 |     hue=hue_col,\n1087 |     palette=sns.color_palette(\"hls\",hue_count),\n1088 |     data=tmp,\n1089 |     legend=\"brief\",\n1090 |     alpha=0.2,\n1091 |     ax=axes[1][0]\n1092 | )\n1093 | axes[1][0].set_title('segment C')\n1094 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n1095 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n1096 | hue_count = tmp[hue_col].nunique()\n1097 | sns.scatterplot(\n1098 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1099 |     hue=hue_col,\n1100 |     palette=sns.color_palette(\"hls\",hue_count),\n1101 |     data=tmp,\n1102 |     legend=\"brief\",\n1103 |     alpha=0.2,\n1104 |     ax=axes[1][1]\n1105 | )\n1106 | axes[1][1].set_title('segment T')\n1107 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n1108 | list(sample_df.columns[-100:])\n1109 | hue_col = 'sndr_region'\n1110 | filter_df = sample_df.query(\" is_brm3_4_11_15_bad == 1.0 and brm_bad_tag_assigned in ('3_STOLEN_CC_CB')\")#.sample(frac=0.1)\n1111 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n1112 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n1113 | hue_count = tmp[hue_col].nunique()\n1114 | sns.scatterplot(\n1115 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1116 |     hue=hue_col,\n1117 |     palette=sns.color_palette(\"hls\",hue_count),\n1118 |     data=tmp,\n1119 |     legend=\"brief\",\n1120 |     alpha=0.2,\n1121 |     ax=axes[0][0]\n1122 | )\n1123 | axes[0][0].set_title('segment Y')\n1124 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n1125 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n1126 | hue_count = tmp[hue_col].nunique()\n1127 | sns.scatterplot(\n1128 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1129 |     hue=hue_col,\n1130 |     palette=sns.color_palette(\"hls\",hue_count),\n1131 |     data=tmp,\n1132 |     legend=\"brief\",\n1133 |     alpha=0.2,\n1134 |     ax=axes[0][1]\n1135 | )\n1136 | axes[0][1].set_title('segment G')\n1137 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n1138 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n1139 | hue_count = tmp[hue_col].nunique()\n1140 | sns.scatterplot(\n1141 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1142 |     hue=hue_col,\n1143 |     palette=sns.color_palette(\"hls\",hue_count),\n1144 |     data=tmp,\n1145 |     legend=\"brief\",\n1146 |     alpha=0.2,\n1147 |     ax=axes[1][0]\n1148 | )\n1149 | axes[1][0].set_title('segment C')\n1150 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n1151 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n1152 | hue_count = tmp[hue_col].nunique()\n1153 | sns.scatterplot(\n1154 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1155 |     hue=hue_col,\n1156 |     palette=sns.color_palette(\"hls\",hue_count),\n1157 |     data=tmp,\n1158 |     legend=\"brief\",\n1159 |     alpha=0.2,\n1160 |     ax=axes[1][1]\n1161 | )\n1162 | axes[1][1].set_title('segment T')\n1163 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n1164 | sample_df['l2_product_flow'].unique()\n1165 | hue_col = 'sndr_region'\n1166 | base_df = sample_df.query(\" is_brm3_4_11_15_bad == 1.0 and brm_bad_tag_assigned in ('3_STOLEN_CC_CB') \")#.sample(frac=0.1)\n1167 | for product in list(base_df['sndr_region'].unique()):\n1168 |     filter_df = base_df.query(f\" sndr_region in ('{product}') \")#.sample(frac=0.1)\n1169 |     fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n1170 |     tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n1171 |     hue_count = tmp[hue_col].nunique()\n1172 |     sns.scatterplot(\n1173 |         x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1174 |         hue=hue_col,\n1175 |         palette=sns.color_palette(\"hls\",hue_count),\n1176 |         data=tmp,\n1177 |         legend=\"brief\",\n1178 |         alpha=0.3,\n1179 |         ax=axes[0]\n1180 |     )\n1181 |     axes[0].set_title('segment Y')\n1182 |     tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n1183 |     hue_count = tmp[hue_col].nunique()\n1184 |     sns.scatterplot(\n1185 |         x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1186 |         hue=hue_col,\n1187 |         palette=sns.color_palette(\"hls\",hue_count),\n1188 |         data=tmp,\n1189 |         legend=\"brief\",\n1190 |         alpha=0.3,\n1191 |         ax=axes[1]\n1192 |     )\n1193 |     axes[1].set_title('segment C')\n1194 | # #### join with cam24 subtagging\n1195 | sample_df['pmt_start_date'].min(), sample_df['pmt_start_date'].max()\n1196 | sql = \"\"\"\n1197 | select a.trans_id,\n1198 | a.cam24_hierarchy_tagging,\n1199 | a.cam24_tagging,\n1200 | a.dormant_flag,\n1201 | a.dirty_bad4_flag,\n1202 | a.bad_tagging,\n1203 | a.bad_hierarchy_tagging,\n1204 | a.bad_tagging_hasato,\n1205 | from pypl-edw.pp_scratch.0719_ok_cc_driver_final_tmp_oct a\n1206 | inner join (select * from pypl-edw.pp_scratch_risk.il_offsim_all_final_240201_week1\n1207 |             where is_brm3_4_11_15_bad = 1.0\n1208 |             and pmt_start_date between '2023-12-01' and '2024-01-30'\n1209 |             ) as b\n1210 | on a.trans_id = b.trans_id\n1211 | where a.is_brm3_4_11_15_bad = 1\n1212 | and a.cam24_tagging in ('PureSF','Merchant','Abuse','ATO','Grey')\n1213 | \"\"\"\n1214 | # tag_df = %ppbq {sql}\n1215 | tag_df.shape\n1216 | tag_df.dtypes\n1217 | tag_df['cam24_tagging'].value_counts()\n1218 | sample_df.shape\n1219 | score_df['trans_id'] = score_df['trans_id'].astype(str)\n1220 | tag_df['trans_id'] = tag_df['trans_id'].astype(str)\n1221 | merge_df = pd.merge(score_df[score_df.is_brm3_4_11_15_bad == 1.0], tag_df, on='trans_id', how='inner')\n1222 | merge_df['bad_tagging'].value_counts()\n1223 | merge_df.shape\n1224 | x = merge_df[feature_list].to_numpy()\n1225 | tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n1226 | tsne_results = tsne.fit_transform(x)\n1227 | merge_df['tsne-2d-one'] = tsne_results[:,0]\n1228 | merge_df['tsne-2d-two'] = tsne_results[:,1]\n1229 | hue_col = 'cam24_tagging'\n1230 | filter_df = merge_df.query(\" is_brm3_4_11_15_bad >= 0.0 and brm_bad_tag_assigned in ('3_STOLEN_CC_CB', '4_UNAUTH_RESTR') \")#.sample(frac=0.1)\n1231 | fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n1232 | tmp = filter_df.query(\" cnsr_seg == 'Y' \").sort_values(by=hue_col)\n1233 | hue_count = tmp[hue_col].nunique()\n1234 | sns.scatterplot(\n1235 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1236 |     hue=hue_col,\n1237 |     palette=sns.color_palette(\"hls\",hue_count),\n1238 |     data=tmp,\n1239 |     legend=\"brief\",\n1240 |     alpha=0.2,\n1241 |     ax=axes[0][0]\n1242 | )\n1243 | axes[0][0].set_title('segment Y')\n1244 | axes[0][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n1245 | tmp = filter_df.query(\" cnsr_seg == 'G' \").sort_values(by=hue_col)\n1246 | hue_count = tmp[hue_col].nunique()\n1247 | sns.scatterplot(\n1248 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1249 |     hue=hue_col,\n1250 |     palette=sns.color_palette(\"hls\",hue_count),\n1251 |     data=tmp,\n1252 |     legend=\"brief\",\n1253 |     alpha=0.2,\n1254 |     ax=axes[0][1]\n1255 | )\n1256 | axes[0][1].set_title('segment G')\n1257 | axes[0][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n1258 | tmp = filter_df.query(\" cnsr_seg == 'C' \").sort_values(by=hue_col)\n1259 | hue_count = tmp[hue_col].nunique()\n1260 | sns.scatterplot(\n1261 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1262 |     hue=hue_col,\n1263 |     palette=sns.color_palette(\"hls\",hue_count),\n1264 |     data=tmp,\n1265 |     legend=\"brief\",\n1266 |     alpha=0.2,\n1267 |     ax=axes[1][0]\n1268 | )\n1269 | axes[1][0].set_title('segment C')\n1270 | axes[1][0].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n1271 | tmp = filter_df.query(\" cnsr_seg == 'T' \").sort_values(by=hue_col)\n1272 | hue_count = tmp[hue_col].nunique()\n1273 | sns.scatterplot(\n1274 |     x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n1275 |     hue=hue_col,\n1276 |     palette=sns.color_palette(\"hls\",hue_count),\n1277 |     data=tmp,\n1278 |     legend=\"brief\",\n1279 |     alpha=0.2,\n1280 |     ax=axes[1][1]\n1281 | )\n1282 | axes[1][1].set_title('segment T')\n1283 | axes[1][1].legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n1284 | # #### Balance check\n1285 | sql = \"\"\"\n1286 | select \n1287 | cnsr_seg,\n1288 | is_brm3_4_11_15_bad,\n1289 | case when l2_product_flow IN ('MS EC Non Recurring','WPS Non Recurring') then 'Branded XO'\n1290 |         when l2_product_flow IN ('MS EC Recurring','WPS Recurring', 'Branded_Recurring') then 'Branded Recurring'\n1291 |         when l2_product_flow IN ('P2P-F&F','P2P-G&S','eBay MOR') then l2_product_flow\n1292 | else 'Other' end as product_flow_rollup,\n1293 | count(*) as n,\n1294 | sum((case when is_brm3_4_11_15_bad = 1 then 1 else 0 end) * 1) as cc_bad_txn,\n1295 | sum(nloss) as nloss,\n1296 | sum((case when is_brm3_4_11_15_bad = 1 then 1 else 0 end) * tpv) as gloss,\n1297 | sum(tpv) as tpv\n1298 | from pypl-edw.pp_scratch.il_offsim_all_combine_20240501_matured\n1299 | group by 1, 2, 3\n1300 | \"\"\"\n1301 | # stats_df = %ppbq {sql}\n1302 | stats_df.query(\" is_brm3_4_11_15_bad == 1.0 and cnsr_seg == 'C'  \").pivot(index='is_large_merchant', columns='product_flow_rollup', values='cnt')\n1303 | stats_df.query(\" is_brm3_4_11_15_bad == 0.0 and cnsr_seg == 'C'  \").pivot(index='is_large_merchant', columns='product_flow_rollup', values='cnt')\n1304 | adapt embedding\n1305 | adp embed * rob embed deepfm\n1306 | feature list segment \n1307 | P2P\n1308 | other flow large\n1309 | other flow other seg\n1310 | stats_df.query(\"product_flow_rollup in (eBay MOR)\")\n1311 | bad_pt = stats_df.query(\" is_brm3_4_11_15_bad == 1.0 \").pivot(index='cnsr_seg', columns='product_flow_rollup', values='cnt')\n1312 | bad_pt\n1313 | good_pt = stats_df.query(\" is_brm3_4_11_15_bad == 0.0 \").pivot(index='cnsr_seg', columns='product_flow_rollup', values='n')\n1314 | bad_pt = stats_df.query(\" is_brm3_4_11_15_bad == 1.0 \").pivot(index='cnsr_seg', columns='product_flow_rollup', values='n')\n1315 | (bad_pt/(good_pt+bad_pt)).applymap(lambda x: f\"{x*100:.2f}%\" if not pd.isna(x) else 'NaN')\n1316 | # product bad/good\n1317 | good_seg = stats_df.query(\" is_brm3_4_11_15_bad == 0.0 \").groupby(['cnsr_seg'])['n'].agg('sum')\n1318 | bad_seg = stats_df.query(\" is_brm3_4_11_15_bad == 1.0 \").groupby(['cnsr_seg'])['n'].agg('sum')\n1319 | # (bad_seg/(good_seg+bad_seg)).apply(lambda x: f\"{x*100:.2f}%\" if not pd.isna(x) else 'NaN')\n1320 | good_seg,bad_seg\n1321 | bad_seg\n1322 | # segment bad/good\n1323 | good_prd = stats_df.query(\" is_brm3_4_11_15_bad == 0.0 \").groupby(['product_flow_rollup'])['tpv'].agg('sum')\n1324 | bad_prd = stats_df.query(\" is_brm3_4_11_15_bad == 1.0 \").groupby(['product_flow_rollup'])['nloss'].agg('sum')\n1325 | (bad_prd/(good_prd+bad_prd)).apply(lambda x: f\"{x*100:.2f}%\" if not pd.isna(x) else 'NaN')\n1326 | # segment bad/good\n1327 | good_prd = stats_df.query(\" is_brm3_4_11_15_bad == 0.0 \").groupby(['product_flow_rollup'])['tpv'].agg('sum')\n1328 | bad_prd = stats_df.query(\" is_brm3_4_11_15_bad == 1.0 \").groupby(['product_flow_rollup'])['nloss'].agg('sum')\n1329 | (bad_prd/(good_prd+bad_prd)).apply(lambda x: f\"{x*100:.2f}%\" if not pd.isna(x) else 'NaN')\n1330 | good_prd\n1331 | import numpy as np\n1332 | def decayed_learning_rate(step):\n1333 |     return initial_learning_rate * (1 + decay_rate * step / decay_step)\n1334 | df = pd.DataFrame()\n1335 | df['pmt_start_date'] = pd.date_range(start=\"2024-01-01\", end=\"2024-03-31\", freq='D')\n1336 | df.shape\n1337 | df['steps'] = np.arange(0,91)\n1338 | 'float' and 'int'\n1339 | initial_learning_rate * decay_rate ** (1 / decay_step)\n1340 | decay_rate, int(1 / decay_step)\n1341 | 1.25^0\n1342 | initial_learning_rate = 0.0001\n1343 | decay_step = 5\n1344 | decay_rate = 1.12\n1345 | # def decayed_learning_rate(step):\n1346 | #     return initial_learning_rate / (1 - decay_rate * step / decay_step)\n1347 | def decayed_learning_rate(step):\n1348 |     return initial_learning_rate * decay_rate ** int(step / decay_step)\n1349 | df = pd.DataFrame()\n1350 | df['steps'] = np.arange(0,200)\n1351 | df['lr'] = df['steps'].apply(lambda x:decayed_learning_rate(x))\n1352 | sns.lineplot(x=df['steps'], y=df['lr'])",
    "rmr_agent/repos/CAM_variable_research/model_redefine.py": "   1 | # %ppauth\n   2 | import tensorflow as tf\n   3 | display(tf.__version__)\n   4 | import sys\n   5 | import aml.cloud_v1 as cloud\n   6 | import sys\n   7 | from pyScoring import UMEModel\n   8 | client = cloud.DataProcClient(\n   9 |      gcp_project='ccg24-hrzana-gds-focus'  \n  10 | )\n  11 | from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Flatten, Concatenate, Lambda, Add, Embedding\n  12 | # # model redefine locally and upload to gcp\n  13 | import sys\n  14 | import sys    \n  15 | import tensorflow as tf\n  16 | from tensorflow.keras.layers import Dense, Dropout\n  17 | sub_model_paths=['gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/tf_models/TagFineTune_best', \\\n  18 |                  'gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/tf_models/robust_large_bottom_super_mmoe_id_0_transfer_learning_10expert_2task_best']\n  19 | sub_models=[]\n  20 | for m in sub_model_paths:\n  21 |     sub_models.append(tf.keras.models.load_model(m))\n  22 | adaptive,robust = sub_models\n  23 | adaptive.summary()\n  24 | robust.summary()\n  25 | # ## Extract adaptive/robust/adp+rbt\n  26 | adaptive.input\n  27 | #adaptive\n  28 | adaptive_input = adaptive.input\n  29 | adp_output_cls = adaptive.get_layer('batch_normalization_2').output\n  30 | adaptive_cls_new = tf.keras.models.Model(adaptive_input,adp_output_cls)\n  31 | inputs = [tf.keras.layers.Input(adaptive_cls_new.input.shape[1],name='adaptive_input')]\n  32 | outputs = [adaptive_cls_new(inputs[0])]\n  33 | new_model = tf.keras.models.Model(inputs,outputs)\n  34 | new_model.trainable=True\n  35 | for layer in new_model.layers:\n  36 |     layer.trainable=True\n  37 | task_layer_cls = tf.keras.models.Sequential([tf.keras.layers.Dense(64,'relu'), tf.keras.layers.Dense(1,'sigmoid')], name='adaptive_cls')\n  38 | inputs = new_model.inputs\n  39 | mid = new_model(inputs)\n  40 | outputs = [task_layer_cls(mid)]\n  41 | final_model = tf.keras.models.Model(inputs, outputs)\n  42 | final_model.summary()\n  43 | final_model.save('/projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_adp_cls_tf')\n  44 | #adaptive\n  45 | adaptive_input = adaptive.input\n  46 | adp_output_cls = adaptive.get_layer('batch_normalization_2').output\n  47 | adaptive_cls_new = tf.keras.models.Model(adaptive_input,adp_output_cls)\n  48 | inputs = [tf.keras.layers.Input(adaptive_cls_new.input.shape[1],name='adaptive_input')]\n  49 | outputs = [adaptive_cls_new(inputs[0])]\n  50 | new_model = tf.keras.models.Model(inputs,outputs)\n  51 | new_model.trainable=True\n  52 | for layer in new_model.layers:\n  53 |     layer.trainable=True\n  54 | task_layer_cls = tf.keras.models.Model([tf.keras.layers.Dense(64,'relu'), tf.keras.layers.Dense(1,'sigmoid')], name='adaptive_cls')\n  55 | inputs = new_model.inputs\n  56 | mid = new_model(inputs)\n  57 | outputs = [task_layer_cls(mid)]\n  58 | final_model = tf.keras.models.Model(inputs, outputs, '')\n  59 | final_model.summary()\n  60 | final_model.save('/projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_adp_cls_tf')\n  61 | # robust\n  62 | robust_input = robust.input\n  63 | rbt_output_cls = robust.get_layer('batch_normalization_2').output\n  64 | robust_cls_new = tf.keras.models.Model(robust_input,rbt_output_cls)\n  65 | inputs = [tf.keras.layers.Input(robust_cls_new.input.shape[1],name='robust_input')]\n  66 | outputs = [robust_cls_new(inputs[0])]\n  67 | new_model = tf.keras.models.Model(inputs,outputs)\n  68 | new_model.trainable=True\n  69 | for layer in new_model.layers:\n  70 |     layer.trainable=True\n  71 | task_layer_cls = tf.keras.models.Sequential([tf.keras.layers.Dense(64,'relu'), tf.keras.layers.Dense(1,'sigmoid')], name='robust_cls')\n  72 | inputs = new_model.inputs\n  73 | mid = new_model(inputs)\n  74 | outputs = [task_layer_cls(mid)]\n  75 | final_model = tf.keras.models.Model(inputs, outputs)\n  76 | final_model.summary()\n  77 | final_model.save('/projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_rbt_cls_tf')\n  78 | # robust\n  79 | robust_input = robust.input\n  80 | rbt_output_cls = robust.get_layer('batch_normalization_2').output\n  81 | robust_cls_new = tf.keras.models.Model(robust_input,rbt_output_cls)\n  82 | inputs = [tf.keras.layers.Input(robust_cls_new.input.shape[1],name='robust_input')]\n  83 | outputs = [robust_cls_new(inputs[0])]\n  84 | new_model = tf.keras.models.Model(inputs,outputs)\n  85 | new_model.trainable=True\n  86 | for layer in new_model.layers:\n  87 |     layer.trainable=True\n  88 | task_layer_cls = tf.keras.models.Sequential([tf.keras.layers.Dense(64,'relu'), tf.keras.layers.Dense(1,'sigmoid')], name='robust_cls')\n  89 | inputs = new_model.inputs\n  90 | mid = new_model(inputs)\n  91 | outputs = [task_layer_cls(mid)]\n  92 | final_model = tf.keras.models.Model(inputs, outputs)\n  93 | final_model.summary()\n  94 | final_model.save('/projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_rbt_cls_tf')\n  95 | robust_input = robust.input\n  96 | rob_output = robust.get_layer('batch_normalization_2').output\n  97 | robust_new = tf.keras.models.Model(robust_input,rob_output)\n  98 | adaptive_input = adaptive.input\n  99 | adp_output = adaptive.get_layer('batch_normalization_2').output\n 100 | adaptive_new = tf.keras.models.Model(adaptive_input,adp_output)\n 101 | inputs = [tf.keras.layers.Input(robust_new.input.shape[1],name='robust_input'),tf.keras.layers.Input(adaptive_new.input.shape[1],name='adaptive_input')]\n 102 | outputs = [robust_new(inputs[0]),adaptive_new(inputs[1])]\n 103 | ensemble_embedding = tf.keras.layers.Concatenate(axis=-1,name='ensemble_embedding')(outputs)\n 104 | ensemble_model = tf.keras.models.Model(inputs,ensemble_embedding)\n 105 | ensemble_model.trainable=False\n 106 | for layer in ensemble_model.layers:\n 107 |     layer.trainable=True\n 108 | task_layer = tf.keras.models.Sequential(\n 109 |             [tf.keras.layers.Dense(64,'relu'),tf.keras.layers.Dense(1,'sigmoid')]\n 110 |         )\n 111 | inputs = ensemble_model.inputs\n 112 | mid = ensemble_model(inputs)\n 113 | outputs = task_layer(mid)\n 114 | final_model = tf.keras.models.Model(inputs,outputs)\n 115 | final_model.summary()\n 116 | final_model.save('/projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_adp_rbt_cls_tf')\n 117 | final_model.summary()\n 118 | final_model.inputs\n 119 | # !ls -tl /projects/gds-focus/data/catch/IL/CAM24/tf_model/\n 120 | # !gsutil cp -r /projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_adp_cls_tf gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/tf_models/\n 121 | # !gsutil cp -r /projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_rbt_cls_tf gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/tf_models/\n 122 | # !gsutil cp -r /projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_adp_rbt_cls_tf gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/tf_models/\n 123 | # ## CLS+REG finetune\n 124 | # ### 2.1 concat+two layers\n 125 | #adaptive\n 126 | adaptive_input = adaptive.input\n 127 | adp_output_cls = adaptive.get_layer('dropout_14').output # last emb layer\n 128 | adp_output_reg = adaptive.get_layer('dropout_15').output # last emb layer\n 129 | adaptive_cls_new = tf.keras.models.Model(adaptive_input,adp_output_cls)\n 130 | adaptive_reg_new = tf.keras.models.Model(adaptive_input,adp_output_reg)\n 131 | inputs = [tf.keras.layers.Input(adaptive_cls_new.input.shape[1],name='adaptive_input')]\n 132 | outputs = [adaptive_cls_new(inputs[0]),adaptive_reg_new(inputs[0])]\n 133 | ensemble_model = tf.keras.models.Model(inputs,outputs)\n 134 | ensemble_model.trainable=True\n 135 | for layer in ensemble_model.layers:\n 136 |     layer.trainable=True\n 137 | task_layer_cls = tf.keras.models.Sequential([tf.keras.layers.Dense(64,'relu'),tf.keras.layers.Dense(32,'relu'),tf.keras.layers.Dense(1,'sigmoid')], name='sf_cls')\n 138 | task_layer_reg = tf.keras.models.Sequential([tf.keras.layers.Dense(64,'relu'),tf.keras.layers.Dense(32,'relu'),tf.keras.layers.Dense(1,'linear')], name='sf_reg')\n 139 | # # !mkdir /projects/gds-focus/data/catch/IL/CAM/training_codes/mk_training/extract_ume\n 140 | inputs = ensemble_model.inputs\n 141 | mid = ensemble_model(inputs)\n 142 | outputs = [task_layer_cls(mid[0]),task_layer_reg(mid[1])]\n 143 | final_model = tf.keras.models.Model(inputs, outputs)\n 144 | final_model.summary()\n 145 | final_model.save('/projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_adp_cls_reg_embed_concat')\n 146 | # ### 2.2 DeepFM layers\n 147 | def share_mlp_for3(shared_layers_length,activate_func,drop_out,kernel_initializer,regular=None,share_name='Shared_mlp'):\n 148 |     shared_mlp = tf.keras.models.Sequential(name=share_name)\n 149 |     for nb in shared_layers_length:\n 150 |         shared_mlp.add(Dense(nb,activation=activate_func, kernel_initializer=kernel_initializer, kernel_regularizer=regular))\n 151 |         shared_mlp.add(BatchNormalization())\n 152 |         shared_mlp.add(Dropout(rate = drop_out))\n 153 |     return shared_mlp\n 154 | def reduce_sum(input_tensor,\n 155 |                axis=None,\n 156 |                keep_dims=False,\n 157 |                name=None,\n 158 |                reduction_indices=None):\n 159 |     try:\n 160 |         return tf.reduce_sum(input_tensor,\n 161 |                              axis=axis,\n 162 |                              keep_dims=keep_dims,\n 163 |                              name=name,\n 164 |                              reduction_indices=reduction_indices)\n 165 |     except TypeError:\n 166 |         return tf.reduce_sum(input_tensor,\n 167 |                              axis=axis,\n 168 |                              keepdims=keep_dims,\n 169 |                              name=name)\n 170 | adp_input = adaptive.input\n 171 | rob_input = robust.input\n 172 | adp_tf_model_new = tf.keras.models.Model(adp_input, adaptive.get_layer('dropout_14').output, name='new_adp_part') \n 173 | rob_tf_model_new = tf.keras.models.Model(rob_input, robust.get_layer('dropout_14').output, name='new_rob_part') \n 174 | inputs = [tf.keras.layers.Input(adp_tf_model_new.input.shape[1],name='adaptive_input'), \\\n 175 |           tf.keras.layers.Input(rob_tf_model_new.input.shape[1],name='robust_input')]\n 176 | outputs = [adp_tf_model_new(inputs[0]), rob_tf_model_new(inputs[1])]\n 177 | ensemble_model = tf.keras.models.Model(inputs, outputs, name='adp_rob_ems')\n 178 | for layer in ensemble_model.layers:\n 179 |     if (layer.name == 'new_adp_part' or layer.name == 'new_rob_part'):\n 180 |         for t in layer.layers:\n 181 |             if (t.name == 'MMoE_Layer'):\n 182 |                 t.trainable = True\n 183 | inputs = ensemble_model.input\n 184 | mid = ensemble_model(inputs)\n 185 | task_names = ['cls', 'reg']\n 186 | output_activate = ['sigmoid', 'linear']\n 187 | task_names = ['cls']\n 188 | output_activate = ['sigmoid']\n 189 | # FM part\n 190 | stack_ = tf.stack(mid, axis=1)\n 191 | square_of_sum = tf.square(reduce_sum(\n 192 |         stack_, axis=1, keep_dims=True))\n 193 | sum_of_square = reduce_sum(\n 194 |         stack_ * stack_, axis=1, keep_dims=True)\n 195 | FM_out = square_of_sum - sum_of_square\n 196 | FM_out = 0.5 * reduce_sum(FM_out, axis=2, keep_dims=False)\n 197 | concat_ = tf.keras.layers.Concatenate(axis=-1,name='ensemble_embedding')(mid)\n 198 | # Linear part\n 199 | linear_out = tf.keras.layers.Dense(units=1,\n 200 |                                     name= 'linear_part',\n 201 |                                     activation= None,\n 202 |                                     kernel_initializer=tf.keras.initializers.VarianceScaling())(concat_)\n 203 | # DeepFM\n 204 | final_outputs = []\n 205 | kernel_initializer = tf.keras.initializers.glorot_normal(seed=1024)\n 206 | for index, task_name in enumerate(task_names):\n 207 |     mlp_out = share_mlp_for3([64, 16], tf.nn.relu, drop_out=0.5, \n 208 |                          kernel_initializer=kernel_initializer, regular=None, share_name='mlp_for_task_{}'.format(task_name))(concat_)\n 209 |     nn_out = tf.keras.layers.Dense(units=1,\n 210 |                                     name= 'nn_pred_' + task_name,\n 211 |                                     activation= None,\n 212 |                                     kernel_initializer=tf.keras.initializers.VarianceScaling())(mlp_out)\n 213 |     task_logit = Add(name='add_logit_' + task_name)([nn_out, linear_out, FM_out])\n 214 |     if ('cls' in task_name):\n 215 |         final_outputs.append(tf.sigmoid(task_logit))\n 216 |     else:\n 217 |         final_outputs.append(task_logit)\n 218 | model = tf.keras.Model(inputs, \n 219 |                        outputs = final_outputs,\n 220 |                        # outputs = {task_names[0]: final_outputs[0], task_names[1]: final_outputs[1]},\n 221 |                        name='CAM24_DeepFM_transfer_new_1task')\n 222 | model.summary()\n 223 | # model.save('/projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_ensemble_finetune')\n 224 | model.save('/projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_ensemble_all_new')\n 225 | # ### 2.3 Finetune ensemble layer\n 226 | cam24_path = \"/projects/gds-focus/data/makang/CAM24/source_code/ems/model/DeepFM_transfer_new_1task_202305_202403/final_model/DeepFM_transfer_new_1task_202305_202403_best\"\n 227 | cam24 = tf.keras.models.load_model(cam24_path)\n 228 | cam24.summary()\n 229 | cam24.save('/projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_fix_mmoe_finetune')\n 230 | cam24.summary()\n 231 | # !ls /projects/gds-focus/data/catch/IL/CAM24/tf_model\n 232 | # !gsutil cp -r /projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_fix_mmoe_finetune gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/tf_models/\n 233 | # ### 2.4 change to two outputs\n 234 | cam24_path = \"/projects/gds-focus/data/makang/CAM24/source_code/ems/model/DeepFM_transfer_new_1task_202305_202403/final_model/DeepFM_transfer_new_1task_202305_202403_best\"\n 235 | cam24 = tf.keras.models.load_model(cam24_path)\n 236 | cam24.summary()\n 237 | cam24.input[1].shape[1]\n 238 | tf_model_new\n 239 | inputs = [tf.keras.layers.Input(cam24.input[0].shape[1],name='adaptive_input'),\n 240 |           tf.keras.layers.Input(cam24.input[1].shape[1],name='robust_input')]\n 241 | tf_model_new = tf.keras.models.Model(cam24.input,\n 242 |                                      cam24.get_layer('add_logit_cls').output, name='add_logit_cls'\n 243 |                                     )\n 244 | mid = tf_model_new(inputs)\n 245 | # outputs = [tf.keras.layers.Dense(1,name='task_0',activation='sigmoid')(mid), \n 246 | #            tf.keras.layers.Dense(1,name='task_1',activation='sigmoid')(mid)]\n 247 | outputs = [tf.sigmoid(mid, name='task_0'), tf.sigmoid(mid, name='task_1')]\n 248 | final_model = tf.keras.models.Model(inputs, outputs)\n 249 | final_model.summary()\n 250 | final_model.save('/projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_two_outputs_all_finetune')\n 251 | # !gsutil cp -r /projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_two_outputs_all_finetune gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/tf_models/\n 252 | # ## Extract UME\n 253 | sub_model_paths=['gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/CAM22_RMR/20240101/model/adaptive/EarlyTagging/final_model/adaptive_mmoe_id_13_early_fine_tune_8expert_6task_best', \\\n 254 |                 'gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/CAM22_RMR/20240101/model/robust/EarlyTagging/final_model/robust_mmoe_id_5_earlytagging_2expert_2task_best']\n 255 | sub_models=[]\n 256 | for m in sub_model_paths:\n 257 |     sub_mode\n 258 |     ls.append(tf.keras.models.load_model(m))\n 259 | adaptive,robust = sub_models\n 260 | adaptive.summary()\n 261 | from pyScoring.model import UMEModel\n 262 | import tensorflow as tf\n 263 | from pyScoring.onnx.support.tf2.tf2_to_onnx import tf_model_to_onnx_as_spec\n 264 | import pandas as pd\n 265 | sys.path.append(\"/projects/gds-focus/data/catch/IL/CAM/anatomy_util/\")\n 266 | from model_anatomy import *\n 267 | def extract_layer_from_tf_model_and_package_woe(model_path, layer_name, tensor_dim, tf_model_input, woe_model_path, output_name, new_model_name):\n 268 |     '''\n 269 |     model_path: tf keras model path\n 270 |     output_name: \n 271 |     new_model_name:\n 272 |     woe_model_path:\n 273 |     tf_model_input:\n 274 |     tensor_dim: dimension like 128\n 275 |     '''\n 276 |     tf_model = tf.keras.models.load_model(model_path)\n 277 |     extractor = tf.keras.Model(inputs=tf_model.inputs,\n 278 |                                 outputs=tf_model.get_layer(layer_name).output)\n 279 |     woe = UMEModel(woe_model_path)\n 280 |     on = []\n 281 |     for i in range(tensor_dim):\n 282 |         string = str(i)\n 283 |         string2 = 'output_' + string\n 284 |         on.append(string2)\n 285 |     tf_ume_model = tf_model_to_onnx_as_spec(\n 286 |             tf_model=extractor, output_mappings={layer_name+\"/Relu\": on})\n 287 |     return package_woe_and_tf(woe, tf_ume_model, tf_model_input, new_model_name)\n 288 | woe_model_path = \"/projects/gds-focus/data/catch/IL/CAM/00_pipeline/umes/base/adaptive_woe.m\"\n 289 | model_path = \"gs://pypl-bkt-rsh-row-std-gds-art/user/xiaopzhang/CAM22_RMR/20240101/model/adaptive/EarlyTagging/final_model/adaptive_mmoe_id_13_early_fine_tune_8expert_6task_best\"\n 290 | tf_model_input_path = '/projects/gds-focus/data/catch/IL/CAM/00_pipeline/columns/adaptive_tf_input.txt'\n 291 | with open (tf_model_input_path, 'r') as f:\n 292 |     adaptive_tf_vars = [c.strip() for c in f.read().split('|')]\n 293 | new_model_name = \"cam_rmr_adapt_cls_embed\"\n 294 | tf_model = tf.keras.models.load_model(model_path)\n 295 | extractor = tf.keras.Model(inputs=tf_model.inputs,\n 296 |                             outputs=tf_model.get_layer(\"batch_normalization_1\").output)\n 297 | woe = UMEModel(woe_model_path)\n 298 | on = []\n 299 | tensor_dim = 128\n 300 | for i in range(tensor_dim):\n 301 |     string = str(i)\n 302 |     string2 = 'output_' + string\n 303 |     on.append(string2)\n 304 | tf_ume_model = tf_model_to_onnx_as_spec(\n 305 |         tf_model=extractor, output_mappings={\"batch_normalization_1/batchnorm/add_1\": on})\n 306 | embed = package_woe_and_tf(woe, tf_ume_model, adaptive_tf_vars, new_model_name)\n 307 | embed.predict({})\n 308 | embed.save(\"/projects/gds-focus/data/catch/IL/CAM/00_pipeline/umes\")\n 309 | # !ls -tl /projects/gds-focus/data/catch/IL/CAM/training_codes/mk_training\n 310 | # !gsutil cp -r /projects/gds-focus/data/catch/IL/CAM/training_codes/mk_training/extract_ume/adp_cls_reg_embed_concat gs://pypl-bkt-rsh-row-std-gds-focus/user/makang/IL_models/extract_ume/\n 311 | # !gsutil ls gs://pypl-bkt-rsh-row-std-gds-focus/user/makang/IL_models/extract_ume\n 312 | import pandas as pd\n 313 | candidate_vars = [list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM/anatomy_util/adaptive_tf_input.txt',sep='|').columns), \\\n 314 |                   list(pd.read_csv('/projects/gds-focus/data/catch/IL/CAM/anatomy_util/robust_tf_input.txt',sep='|').columns)]\n 315 | candidate_vars[0] = ['adaptive_woe_'+item for item in candidate_vars[0]]\n 316 | candidate_vars[1] = ['robust_woe_'+item for item in candidate_vars[1]]\n 317 | candidate_vars\n 318 | # ## MMOE+PPNet\n 319 | inputs\n 320 | import tensorflow as tf\n 321 | from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Multiply, Lambda\n 322 | num_circle_flow = 30\n 323 | num_seller_seg = 30\n 324 | num_cc_level = 30\n 325 | num_s_dof = 30\n 326 | embedding_dim = 10\n 327 | scale = 2\n 328 | def scale_layer(x):\n 329 |     return x * scale\n 330 | # base mmoe\n 331 | adp_input = adaptive.input\n 332 | rob_input = robust.input\n 333 | adp_tf_model_new = tf.keras.models.Model(adp_input, adaptive.get_layer('dropout_14').output, name='new_adp_part') \n 334 | rob_tf_model_new = tf.keras.models.Model(rob_input, robust.get_layer('dropout_14').output, name='new_rob_part') \n 335 | circle_flow_input = Input(shape=(1,), name='circle_flow')\n 336 | seller_seg_input = Input(shape=(1,), name='seller_seg')\n 337 | cc_level_input = Input(shape=(1,), name='cc_level')\n 338 | s_dof_input = Input(shape=(1,), name='s_dof')\n 339 | inputs = [\n 340 |           tf.keras.layers.Input(adp_tf_model_new.input.shape[1],name='adaptive_input'),\n 341 |           tf.keras.layers.Input(rob_tf_model_new.input.shape[1],name='robust_input'),\\\n 342 |           circle_flow_input,\n 343 |           seller_seg_input,\n 344 |           cc_level_input,\n 345 |           s_dof_input,\n 346 |          ]\n 347 | circle_flow_embedding = Embedding(input_dim=num_circle_flow, output_dim=embedding_dim)(inputs[2])\n 348 | seller_seg_embedding = Embedding(input_dim=num_seller_seg, output_dim=embedding_dim)(inputs[3])\n 349 | cc_level_embedding = Embedding(input_dim=num_cc_level, output_dim=embedding_dim)(inputs[4])\n 350 | s_dof_embedding = Embedding(input_dim=num_s_dof, output_dim=embedding_dim)(inputs[5])\n 351 | # circle_flow_embedding = CategoryEncoding(num_classes=num_circle_flow)\n 352 | # seller_seg_embedding = CategoryEncoding(num_classes=num_seller_seg)\n 353 | # cc_level_embedding = CategoryEncoding(num_classes=num_cc_level)\n 354 | # s_dof_embedding = CategoryEncoding(num_classes=num_s_dof)\n 355 | circle_flow_embedding_flat = Flatten()(circle_flow_embedding)\n 356 | seller_seg_embedding_flat = Flatten()(seller_seg_embedding)\n 357 | cc_level_embedding_flat = Flatten()(cc_level_embedding)\n 358 | s_dof_embedding_flat = Flatten()(s_dof_embedding)\n 359 | concat_embedding = Concatenate()([\n 360 |     circle_flow_embedding_flat,\n 361 |     seller_seg_embedding_flat,\n 362 |     cc_level_embedding_flat,\n 363 |     s_dof_embedding_flat,\n 364 |     ])\n 365 | adp_embedding = adp_tf_model_new(inputs[0])\n 366 | rob_embedding = rob_tf_model_new(inputs[1])\n 367 | for num_unit in [128, 64]:\n 368 |     adp_embedding = Dense(num_unit, activation='relu')(adp_embedding)\n 369 |     rob_embedding = Dense(num_unit, activation='relu')(rob_embedding)\n 370 |     # gate block\n 371 |     concat_embedding = Dense(num_unit, activation='relu')(concat_embedding)\n 372 |     concat_scale = Dense(num_unit, activation='sigmoid')(concat_embedding)\n 373 |     # interaction part\n 374 |     adp_embedding = Multiply()([adp_embedding, concat_scale])\n 375 |     adp_embedding = Lambda(scale_layer)(adp_embedding)\n 376 |     # adp_embedding = Multiply()([adp_embedding, tf.constant(2, dtype=tf.float32)])\n 377 |     rob_embedding = Multiply()([rob_embedding, concat_scale])\n 378 |     # rob_embedding = Multiply()([rob_embedding, tf.constant(2, dtype=tf.float32)])\n 379 |     rob_embedding = Lambda(scale_layer)(rob_embedding)\n 380 | adp_rob_concat = Concatenate()([adp_embedding, rob_embedding])\n 381 | outputs = tf.keras.layers.Dense(1,name='cls',activation='sigmoid')(adp_rob_concat)\n 382 | # outputs = [tf.keras.layers.Dense(1,name='task_yg',activation='sigmoid')(adp_rob_concat), \n 383 | #            tf.keras.layers.Dense(1,name='task_ct',activation='sigmoid')(adp_rob_concat)]\n 384 | final_model = tf.keras.models.Model(inputs, outputs)\n 385 | final_model.summary()\n 386 | final_model.save('/projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_fix_mmoe_lhuc_all_task')\n 387 | # final_model.save('/projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_fix_mmoe_lhuc_multi_task')\n 388 | # # !rm -r /projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_fix_mmoe_lhuc_multi_task\n 389 | # !gsutil cp -r /projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_fix_mmoe_lhuc_all_task gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/tf_models/\n 390 | # !gsutil cp -r /projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_fix_mmoe_lhuc_multi_task gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/tf_models/\n 391 | # # !gsutil -m rm -r gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/tf_models/CAM24_fix_mmoe_lhuc_multi_task\n 392 | # ## MMOE+CT/YG tower\n 393 | import tensorflow as tf\n 394 | from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Multiply, Lambda, BatchNormalization\n 395 | # base mmoe\n 396 | adp_input = adaptive.input\n 397 | rob_input = robust.input\n 398 | adp_tf_model_new = tf.keras.models.Model(adp_input, adaptive.get_layer('dropout_14').output, name='new_adp_part') \n 399 | rob_tf_model_new = tf.keras.models.Model(rob_input, robust.get_layer('dropout_14').output, name='new_rob_part') \n 400 | ct_input = Input(shape=(641,), name='ct_adp_vars')\n 401 | yg_input = Input(shape=(583,), name='yg_adp_vars')\n 402 | inputs = [\n 403 |           tf.keras.layers.Input(adp_tf_model_new.input.shape[1],name='adaptive_input'),\n 404 |           tf.keras.layers.Input(rob_tf_model_new.input.shape[1],name='robust_input'),\n 405 |           tf.keras.layers.Input(shape=(641,),name='ct_adp_input'),\n 406 |           tf.keras.layers.Input(shape=(583,),name='yg_adp_input'),\n 407 |          ]\n 408 | adp_embedding = adp_tf_model_new(inputs[0])\n 409 | rob_embedding = rob_tf_model_new(inputs[1])\n 410 | ct_input = inputs[2]\n 411 | yg_input = inputs[3]\n 412 | outputs = []\n 413 | task_name = ['task_yg', 'task_ct']\n 414 | for idx, ip in enumerate([yg_input, ct_input]):\n 415 |     for num_unit in [256, 128]:\n 416 |         ip = Dense(num_unit, activation='relu')(ip)\n 417 |         ip = BatchNormalization()(ip)\n 418 |         ip = Dropout(rate = 0.5)(ip)\n 419 |     concat_embedding = Concatenate()([adp_embedding,ip,rob_embedding])\n 420 |     concat_embedding = Dense(64, activation='relu')(concat_embedding)\n 421 |     concat_embedding = BatchNormalization()(concat_embedding)\n 422 |     outputs.append(tf.keras.layers.Dense(1,name=task_name[idx], \n 423 |                                          activation='sigmoid')(concat_embedding))\n 424 | final_model = tf.keras.models.Model(inputs, outputs)\n 425 | final_model.summary()\n 426 | final_model.save('/projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_fix_mmoe_extra_tower_multi_task')\n 427 | # !gsutil cp -r /projects/gds-focus/data/catch/IL/CAM24/tf_model/CAM24_fix_mmoe_extra_tower_multi_task gs://pypl-bkt-rsh-row-std-gds-focus/user/cchen16/IL_CAM24/tf_models/"
  }
}