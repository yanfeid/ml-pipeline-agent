{
  "summaries": {
    "rmr_agent/repos/toy_research_code/notebooks/pipeline.py": "**[Import necessary libraries] (Lines 1-6):**\n- Imports essential libraries for data manipulation, model training, evaluation, and saving objects.\n\n**[Load and preprocess raw data] (Lines 8-10):**\n- Loads raw customer data from a CSV file.\n- Removes duplicate records from the dataset.\n\n**[Create interaction features] (Lines 12-14):**\n- Creates a new feature 'price_per_unit' by dividing 'total_price' by 'quantity'.\n- Saves the processed data to a Parquet file.\n\n**[Feature selection and train-test split] (Lines 16-20):**\n- Loads the processed data from a Parquet file.\n- Selects specific features and the label for the model.\n- Splits the data into training and testing sets.\n\n**[Save train and test data] (Lines 22-24):**\n- Saves the training data to a Parquet file.\n- Saves the testing data to a Parquet file.\n\n**[Normalize training data] (Lines 26-29):**\n- Loads the training data from a Parquet file.\n- Normalizes the selected features using StandardScaler.\n\n**[Save normalized data and scaler] (Lines 31-33):**\n- Saves the normalized training data to a Parquet file.\n- Saves the scaler object for later use.\n\n**[Train the model] (Lines 35-38):**\n- Loads the normalized training data from a Parquet file.\n- Trains a RandomForestClassifier using the selected features and labels.\n\n**[Save the trained model] (Line 40):**\n- Saves the trained RandomForest model to a file.\n\n**[Model evaluation] (Lines 42-55):**\n- Loads the trained model and scaler.\n- Loads the test data from a Parquet file.\n- Normalizes the test data using the previously saved scaler.\n- Makes predictions on the test data.\n- Calculates the accuracy of the model.\n- Saves the evaluation results to a text file."
  },
  "cleaned_code": {
    "rmr_agent/repos/toy_research_code/notebooks/pipeline.py": "   1 | import pandas as pd\n   2 | from sklearn.model_selection import train_test_split\n   3 | from sklearn.preprocessing import StandardScaler\n   4 | from sklearn.ensemble import RandomForestClassifier\n   5 | from sklearn.metrics import accuracy_score\n   6 | import joblib\n   7 | # Load raw data \n   8 | df = pd.read_csv(\"gs://my-bucket/data/customer_data.csv\")\n   9 | # Remove duplicate records\n  10 | df = df.drop_duplicates()\n  11 | # Create interaction features\n  12 | df['price_per_unit'] = df['total_price'] / df['quantity']\n  13 | # Save driver set\n  14 | df.to_parquet(\"gs://my-bucket/data/processed/customer_data_processed.parquet\", index=False)\n  15 | # Feature selection \n  16 | df = pd.read_parquet(\"gs://my-bucket/data/processed/customer_data_processed.parquet\")\n  17 | feature_list = [\"age\", \"income\", \"credit_score\", \"price_per_unit\"]\n  18 | df = df[feature_list + [\"label\"]]\n  19 | # Train-test split\n  20 | train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n  21 | # Save train data\n  22 | train_df.to_parquet(\"gs://my-bucket/data/processed/train_data.parquet\", index=False)\n  23 | # Save test data\n  24 | test_df.to_parquet(\"gs://my-bucket/data/processed/test_data.parquet\", index=False)\n  25 | # Normalize data\n  26 | train_df = pd.read_parquet(\"gs://my-bucket/data/processed/train_data.parquet\")\n  27 | feature_list = [\"age\", \"income\", \"credit_score\", \"price_per_unit\"]\n  28 | scaler = StandardScaler()\n  29 | train_df[feature_list] = scaler.fit_transform(train_df[feature_list])\n  30 | # Save normalized data\n  31 | train_df.to_parquet(\"gs://my-bucket/data/processed/normalized_train_data.parquet\", index=False)\n  32 | # Save the scaler for later use\n  33 | joblib.dump(scaler, \"/home/user/models/scaler.pkl\")\n  34 | # Train model \n  35 | train_df = pd.read_parquet(\"gs://my-bucket/data/processed/normalized_train_data.parquet\")\n  36 | feature_list = [\"age\", \"income\", \"credit_score\", \"price_per_unit\"]\n  37 | model = RandomForestClassifier(n_estimators=100, random_state=42)\n  38 | model.fit(train_df[feature_list], train_df['label'])\n  39 | # Save the trained model\n  40 | joblib.dump(model, \"/home/user/models/random_forest.pkl\")\n  41 | # Model evaluation \n  42 | # Load the trained model and scaler\n  43 | model = joblib.load(\"/home/user/models/random_forest.pkl\")\n  44 | scaler = joblib.load(\"/home/user/models/scaler.pkl\")\n  45 | # load test data\n  46 | test_df = pd.read_parquet(\"gs://my-bucket/data/processed/test_data.parquet\")\n  47 | feature_list = [\"age\", \"income\", \"credit_score\", \"price_per_unit\"]\n  48 | # normalize test data\n  49 | test_features = scaler.transform(test_df[feature_list])\n  50 | # score the model\n  51 | predictions = model.predict(test_features)\n  52 | accuracy = accuracy_score(test_df['label'], predictions)\n  53 | # Save evaluation results\n  54 | with open(\"/home/user/models/eval_results.txt\", \"w\") as f:\n  55 |     f.write(f\"Accuracy: {accuracy:.2f}\\n\")"
  }
}