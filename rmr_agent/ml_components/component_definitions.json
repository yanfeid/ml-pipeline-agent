{
    "Driver Creation": "Typically the first step in the entire pipeline, it is the initial data loading and extraction (ETL) via SQL (often BigQuery) to create the “driver” dataset. This dataset contains the basic rows and metadata (e.g. class labels (0, 1), class weights, split category (train, val, test/OOT)). It can also sometimes include feature engineering and transformation in the SQL, but it still results in the final driverset at the end. The final driver dataset will typically be saved to GCS (e.g. parquet, PIG, CSV).",
    "Feature Engineering": "Creating new predictive features, typically using SQL (most often in BigQuery). This involves applying transformations, aggregations, and derivations to raw data or data in the driver set. The engineered features are intended to later be joined with the driver dataset to enrich the final feature set.",
    "Data Pulling": "Enriching the driver dataset with additional data/features from a feature store/variable mart (typically with a proprietary API/SDK such as Data Fetcher, PyOFS, QPull, MadMen, PyDPU, mdlc.dataset).",
    "Feature Consolidation": "Merging multiple datasets into a unified feature set for modeling (distinct from driver creation joins).",
    "Data Preprocessing": "Cleaning/transforming data (e.g., imputation, handling outliers, encoding categorical variables, transforming to WOE, or normalization/scaling of features). Can include saving the preprocessors/transformers used (that is not a separate component). A commonly used proprietary library for preprocessing is Shifu.",
    "Feature Selection": "Selecting key features for modeling. Common proprietary libraries include Shifu and model_automation. Jobs may be submitting for execution on GCP Dataproc. Final variable list is saved. ",
    "TFRecord Conversion": "Converting data into TFRecord format. Typically a tfrecord convert function is submitted for execution on GCP Dataproc.",
    "Model Training": "Fitting models on training data. Libraries commonly used are TensorFlow, LightGBM, and PyTorch. Training may happen on local resources, or a train function may be submitted for execution on GCP Dataproc or VertexAI. The model will likely be evaluated on the validation dataset. The model can be saved using library-specific formats (e.g., .h5 for TensorFlow, .model for LightGBM, .pt for PyTorch).",
    "Hyperparameter Tuning": "Tuning model hyperparameters to improve performance. Libraries commonly used include Optuna, Ray, aml.automl.experiment, Hyperopt, or Scikit-learn’s GridSearchCV and RandomizedSearchCV. Optimization may be executed locally or submitted as a job to GCP Dataproc or VertexAI for distributed processing. The process typically involves defining a search space, evaluating model performance on a validation dataset, and selecting the best hyperparameter set.",
    "Model Ensembling": "Combining multiple independently trained models to improve performance by aggregating their predictions through methods like averaging, voting, stacking, or blending. Code related to this typically involves loading multiple trained models, merging their outputs, or using a meta-model for final predictions",
    "Model Packaging": "Saving trained models into deployment-ready formats (e.g. ONNX, UME, SavedModel, PMML, txt, etc.). This step may include bundling preprocessing/normalization logic (often from Shifu) with the model.",
    "Model Scoring": "Inferencing the trained model on the unseen test/OOT dataset. It must be the test/OOT dataset, not the validation dataset to classify as Model Scoring. The commonly used proprietary tool is PyScoring.",
    "Model Evaluation": "Calculating performance metrics on the unseen test/OOT dataset. To classify as Model Evaluation, you must be sure the evaluation is occuring on the test/OOT dataset and not the validation dataset.",
    "Model Deployment": "Deploying/serving models for online or offline inference."
}